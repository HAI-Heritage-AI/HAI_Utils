### C:\repository\HAI_Python\main.py ###
from fastapi import FastAPI
from fastapi.middleware.cors import CORSMiddleware
from app.routers.chatbot_router import chatbot_router
from app.routers.book_router import book_router
from app.routers.auth_router import auth_router 
from app.routers.plan_router import plan_router
from app.routers.chat_agent_router import chat_agent_router
from app.routers.festival_router import festival_router

app = FastAPI()

# CORS ì„¤ì •
app.add_middleware(
    CORSMiddleware,
    allow_origins=["http://localhost:3000", "http://127.0.0.1:3000"],  # React ì•±ì˜ ì£¼ì†Œ
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# ë¼ìš°í„° ë“±ë¡
app.include_router(chatbot_router, prefix="/api/chatbot")
app.include_router(book_router, prefix="/api/book")
app.include_router(auth_router, prefix="/api/auth")  
app.include_router(plan_router, prefix="/api/plan")
app.include_router(chat_agent_router, prefix="/api/chat")
app.include_router(festival_router, prefix="/api/festival")

@app.get("/")
async def read_root():
    return {"message": "API ì„œë²„ê°€ ì •ìƒì ìœ¼ë¡œ ì‹¤í–‰ ì¤‘ì…ë‹ˆë‹¤."}


### C:\repository\HAI_Python\app\agents.py ###
from app.chat_agent import TravelChatAgent

# ì „ì—­ TravelChatAgent ì¸ìŠ¤í„´ìŠ¤ë¥¼ ìƒì„±
travel_chat_agent = TravelChatAgent()


### C:\repository\HAI_Python\app\chat_agent.py ###
from langchain_openai import ChatOpenAI
from typing import Optional
import os
import requests
from dotenv import load_dotenv
from app.plan_agent import plan_travel, calculate_trip_days  # ì¶”ê°€
import json

load_dotenv()

class TravelChatAgent:
    def __init__(self):
        self.current_travel_plan = None
        self.destination = None
        self.travel_style = None
        self.user_info = None  # ì‚¬ìš©ì ì •ë³´ ì¶”ê°€
        
        self.llm = ChatOpenAI(
            api_key=os.getenv("OPENAI_API_KEY"),
            model_name="gpt-3.5-turbo",
            temperature=0.7
        )
        self.chat_history = []
        self.max_turns = 6
        
        # ë„¤ì´ë²„ API ì„¤ì •
        self.naver_headers = {
            "X-Naver-Client-Id": os.getenv("NAVER_CLIENT_ID"),
            "X-Naver-Client-Secret": os.getenv("NAVER_CLIENT_SECRET")
        }

    def set_user_info(self, user_info: dict):
        """ì—¬í–‰ì ì •ë³´ ì„¤ì •"""
        self.user_info = user_info
        self.destination = user_info.get('destination')
        self.travel_style = user_info.get('style')
        self.current_travel_plan = user_info  # ì—¬í–‰ ê³„íšì„ ìµœì‹ ìœ¼ë¡œ ì„¤ì •

        # ì—¬í–‰ ê³„íš ìƒì„± (ì˜µì…˜: ì²˜ìŒ ìƒì„±ì´ ì•„ë‹ ë•ŒëŠ” ì—…ë°ì´íŠ¸)
        if not self.current_travel_plan:
            try:
                result = plan_travel(self.user_info)
                if result:
                    self.current_travel_plan = json.loads(result)
            except Exception as e:
                print(f"ì—¬í–‰ ê³„íš ìƒì„± ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")

    def _parse_travel_plan(self, context: str) -> dict:
        """ì—¬í–‰ í”Œëœì—ì„œ ì£¼ìš” ì •ë³´ ì¶”ì¶œ"""
        plan_info = {
            'places': [],  # ê³„íšëœ ì¥ì†Œë“¤
            'schedule': {} # ì¼ì •ë³„ ì •ë³´
        }
        
        try:
            # Day 1, Day 2 ë“±ìœ¼ë¡œ ì‹œì‘í•˜ëŠ” ì¼ì • íŒŒì‹±
            days = context.split('[Day')
            for day in days[1:]:  # ì²« ë²ˆì§¸ëŠ” ë¹ˆ ë¬¸ìì—´ì´ë¯€ë¡œ ì œì™¸
                day_info = []
                lines = day.split('\n')
                current_day = lines[0].strip().rstrip(']')
                
                for line in lines:
                    if 'ì£¼ì†Œ:' in line:
                        place = {
                            'name': lines[lines.index(line)-1].split(':')[-1].strip(),
                            'address': line.split('ì£¼ì†Œ:')[-1].strip(),
                        }
                        day_info.append(place)
                        plan_info['places'].append(place)
                
                plan_info['schedule'][current_day] = day_info
                
        except Exception as e:
            print(f"ì—¬í–‰ í”Œëœ íŒŒì‹± ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        
        return plan_info

    def search_naver_blog(self, query: str) -> str:
        """ë„¤ì´ë²„ ë¸”ë¡œê·¸ ê²€ìƒ‰ - ì§€ì—­ í•„í„°ë§ ì¶”ê°€"""
        if not self.destination:
            print("Warning: destinationì´ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤!")
            return "ì—¬í–‰ ëª©ì ì§€ ì •ë³´ê°€ ì—†ìŠµë‹ˆë‹¤."

        # URL ì •ì˜ ì¶”ê°€
        url = "https://openapi.naver.com/v1/search/blog"
        
        print(f"í˜„ì¬ ì„¤ì •ëœ destination: {self.destination}")
        
        # ê²€ìƒ‰ì–´ì— destinationì„ ì•ì— ëª…í™•í•˜ê²Œ í¬í•¨
        search_query = f"{self.destination} {query}"
        params = {
            "query": search_query,
            "display": 10,
            "sort": "sim"
        }
        
        print(f"\n=== ë„¤ì´ë²„ ë¸”ë¡œê·¸ ê²€ìƒ‰ ìš”ì²­ ===")
        print(f"ê²€ìƒ‰ì–´: {search_query}")
        
        response = requests.get(url, headers=self.naver_headers, params=params)
        print(f"ì‘ë‹µ ìƒíƒœ ì½”ë“œ: {response.status_code}")
        
        if response.status_code == 200:
            items = response.json().get('items', [])
            filtered_items = []
            
            print(f"\nê²€ìƒ‰ëœ ë¸”ë¡œê·¸ ê¸€ ëª©ë¡:")
            for item in items:
                print(f"\nì œëª©: {item['title'].replace('<b>', '').replace('</b>', '')}")
                print(f"ë§í¬: {item['link']}")
                
                # ì œëª©ì´ë‚˜ ë‚´ìš©ì— destinationì´ í¬í•¨ëœ ê²°ê³¼ë§Œ í•„í„°ë§
                if self.destination in item['title'] or self.destination in item['description']:
                    filtered_items.append(item)
            
            if not filtered_items:
                return f"{self.destination}ì˜ ê´€ë ¨ ì •ë³´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
            
            # ê°€ì¥ ê´€ë ¨ì„± ë†’ì€ ì²« ë²ˆì§¸ ê²°ê³¼ ì„ íƒ
            best_result = filtered_items[0]
            
            results = f"""
                ê°€ì¥ ê´€ë ¨ì„± ë†’ì€ ë¸”ë¡œê·¸ ê¸€:
                ì œëª©: {best_result['title'].replace('<b>', '').replace('</b>', '')}
                ë‚´ìš©: {best_result['description'].replace('<b>', '').replace('</b>', '')}
                ë§í¬: {best_result['link']}
                """
            return results
        return "ê²€ìƒ‰ ê²°ê³¼ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."

    def search_naver_local(self, query: str) -> str:
        """ë„¤ì´ë²„ ì§€ì—­ ê²€ìƒ‰ - ì§€ì—­ í•„í„°ë§ ì¶”ê°€"""
        url = "https://openapi.naver.com/v1/search/local"
        params = {
            "query": f"{self.destination} {query}",
            "display": 5,
            "sort": "random"
        }
        
        response = requests.get(url, headers=self.naver_headers, params=params)
        
        if response.status_code == 200:
            items = response.json().get('items', [])
            filtered_items = []
            
            for item in items:
                # ì£¼ì†Œì— destinationì´ í¬í•¨ëœ ê²°ê³¼ë§Œ í•„í„°ë§
                if self.destination in item['address']:
                    filtered_items.append(item)
            
            if not filtered_items:
                return f"{self.destination}ì˜ ê´€ë ¨ ì¥ì†Œë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
            
            results = f"ğŸ¢ {self.destination} ê´€ë ¨ ì¥ì†Œ:\n"
            for item in filtered_items:
                results += f"""
                ì¥ì†Œëª…: {item['title'].replace('<b>', '').replace('</b>', '')}
                ì£¼ì†Œ: {item['address']}
                ë„ë¡œëª…: {item.get('roadAddress', 'ì •ë³´ ì—†ìŒ')}
                ì¹´í…Œê³ ë¦¬: {item.get('category', 'ì •ë³´ ì—†ìŒ')}
                ì „í™”: {item.get('telephone', 'ì •ë³´ ì—†ìŒ')}
                -------------------"""
            return results
        return "ê²€ìƒ‰ ê²°ê³¼ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."

    async def get_answer(self, question: str, context: Optional[str] = None) -> str:
        """ì±—ë´‡ ë‹µë³€ ìƒì„± - ì—¬í–‰ í”Œëœ ê³ ë ¤"""
        if not self.destination:
            return "ì£„ì†¡í•©ë‹ˆë‹¤. ì—¬í–‰ ëª©ì ì§€ ì •ë³´ê°€ ì—†ìŠµë‹ˆë‹¤."
            
        # ì±„íŒ… ê¸°ë¡ ê´€ë¦¬
        if len(self.chat_history) > self.max_turns * 2:
            self.chat_history = self.chat_history[-self.max_turns * 2:]

        # ê²€ìƒ‰ ìˆ˜í–‰
        blog_results = self.search_naver_blog(question)
        local_results = self.search_naver_local(question)

        # GPT í”„ë¡¬í”„íŠ¸ êµ¬ì„±
        system_content = f"""ë‹¹ì‹ ì€ {self.destination} ì§€ì—­ ì „ë¬¸ ì—¬í–‰ ì±—ë´‡ì…ë‹ˆë‹¤.
        í˜„ì¬ ê³„íšëœ ì—¬í–‰ ì •ë³´:
        - ëª©ì ì§€: {self.destination}
        - ì—¬í–‰ ìŠ¤íƒ€ì¼: {self.travel_style if self.travel_style else 'ì •ë³´ ì—†ìŒ'}
        
        ì¤‘ìš”: ë°˜ë“œì‹œ {self.destination} ì§€ì—­ì˜ ì •ë³´ë§Œ ì¶”ì²œí•´ì£¼ì„¸ìš”.
        ë‹¤ë¥¸ ë„ì‹œì˜ ì •ë³´ëŠ” ì¶”ì²œí•˜ì§€ ë§ˆì„¸ìš”.
        
        ì—¬í–‰ ê³„íš: {context if context else json.dumps(self.current_travel_plan)}
        """

        messages = [
            {"role": "system", "content": system_content}
        ]
        
        # ì´ì „ ëŒ€í™” ê¸°ë¡ ì¶”ê°€
        messages.extend(self.chat_history)
        
        # í˜„ì¬ ì§ˆë¬¸ ê´€ë ¨ ì •ë³´ ì¶”ê°€
        messages.append({"role": "user", "content": f"""
            ì§ˆë¬¸: {question}
            
            ë„¤ì´ë²„ ë¸”ë¡œê·¸ ê²€ìƒ‰ ê²°ê³¼:
            {blog_results}
            
            ë„¤ì´ë²„ ì§€ì—­ ê²€ìƒ‰ ê²°ê³¼:
            {local_results}
        """})
        
        # GPT ì‘ë‹µ ìƒì„±
        response = await self.llm.agenerate([messages])
        answer = response.generations[0][0].text.strip()
        
        # ëŒ€í™” ê¸°ë¡ ì €ì¥
        self.chat_history.append({"role": "user", "content": question})
        self.chat_history.append({"role": "assistant", "content": answer})
        
        return answer


### C:\repository\HAI_Python\app\plan_agent.py ###
from crewai import Agent, Task, Crew
from crewai_tools import BaseTool, SerperDevTool, CSVSearchTool
from typing import Optional
import requests
import os
import json
from dotenv import load_dotenv
from datetime import datetime
import pandas as pd
import unicodedata
import re

# í™˜ê²½ë³€ìˆ˜ ë¡œë”© ë° ê²€ì‚¬
load_dotenv()

BASE_DIR = os.path.dirname(os.path.abspath(__file__))  # í˜„ì¬ íŒŒì¼ ê¸°ì¤€ì˜ ì ˆëŒ€ ê²½ë¡œ
TRAVEL_DATA_DIR = os.path.join(BASE_DIR,  'travel','data')  # travel/data ë””ë ‰í† ë¦¬ ê²½ë¡œ

# API í‚¤ í™•ì¸
if not os.getenv("OPENAI_API_KEY"):
    raise ValueError("OPENAI_API_KEY not found in environment variables")
if not os.getenv("SERPER_API_KEY"):
    raise ValueError("SERPER_API_KEY not found in environment variables")
if not os.getenv("NAVER_CLIENT_ID") or not os.getenv("NAVER_CLIENT_SECRET"):
    raise ValueError("NAVER API credentials not found in environment variables")
if not os.getenv("KAKAO_REST_API_KEY"):
    raise ValueError("KAKAO_REST_API_KEY not found in environment variables")

def get_csv_file_paths(destination: str) -> dict:
    """
    ì£¼ì–´ì§„ ëª©ì ì§€(destination)ì— í•´ë‹¹í•˜ëŠ” ì—¬í–‰ì§€ì™€ ë§›ì§‘ CSV íŒŒì¼ ê²½ë¡œë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤.
    """
    base_paths = {
        'travel': os.path.join(TRAVEL_DATA_DIR, 'travel'),
        'food': os.path.join(TRAVEL_DATA_DIR, 'food'),
    }
    
    result = {'travel': None, 'food': None}
    for category, base_path in base_paths.items():
        if not os.path.exists(base_path):
            print(f"Error: '{base_path}' ê²½ë¡œê°€ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.")
            continue

        print(f"'{base_path}' ê²½ë¡œì—ì„œ íŒŒì¼ì„ ê²€ìƒ‰ ì¤‘...")
        normalized_destination = unicodedata.normalize('NFC', destination)

        for file_name in os.listdir(base_path):
            normalized_file_name = unicodedata.normalize('NFC', file_name)
            if normalized_destination in normalized_file_name and normalized_file_name.endswith('.csv'):
                print(f"{destination}ì— í•´ë‹¹í•˜ëŠ” {category} íŒŒì¼ ì°¾ìŒ: {file_name}")
                result[category] = os.path.join(base_path, file_name)
                break

        if result[category] is None:
            print(f"{destination}ì— í•´ë‹¹í•˜ëŠ” {category} CSV íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")

    return result

import pandas as pd

def convert_csv_to_utf8(original_csv_path: str, temp_csv_path: str) -> None:
    """
    CSV íŒŒì¼ì„ UTF-8ë¡œ ë³€í™˜í•˜ì—¬ ì„ì‹œ íŒŒì¼ë¡œ ì €ì¥í•©ë‹ˆë‹¤.
    """
    try:
        # íŒŒì¼ì„ 'utf-8' ì¸ì½”ë”©ìœ¼ë¡œ ì‹œë„í•´ì„œ ì½ê¸°
        df = pd.read_csv(original_csv_path, encoding='utf-8')
    except UnicodeDecodeError:
        # ë§Œì•½ 'utf-8'ë¡œ ì½ê¸° ì‹¤íŒ¨í•˜ë©´ 'euc-kr'ë¡œ ì‹œë„
        try:
            df = pd.read_csv(original_csv_path, encoding='euc-kr')
        except UnicodeDecodeError:
            # 'euc-kr'ë„ ì‹¤íŒ¨í•˜ë©´ 'latin1'ë¡œ ì‹œë„
            df = pd.read_csv(original_csv_path, encoding='latin1')

    # UTF-8ë¡œ ì €ì¥
    df.to_csv(temp_csv_path, encoding='utf-8', index=False)
    print(f"{original_csv_path} íŒŒì¼ì„ UTF-8ë¡œ ë³€í™˜í•˜ì—¬ {temp_csv_path}ì— ì €ì¥í–ˆìŠµë‹ˆë‹¤.")

def calculate_trip_days(start_date, end_date):
    """
    ì—¬í–‰ ì¼ìˆ˜ë¥¼ ê³„ì‚°í•˜ëŠ” í•¨ìˆ˜
    YYYY-MM-DD í˜•ì‹ì˜ ë‚ ì§œë¥¼ ì²˜ë¦¬í•˜ë©°, ì—°ë„ì™€ ì›”ì´ ë°”ë€ŒëŠ” ê²½ìš°ë„ ì²˜ë¦¬
    """
    try:
        # ë§Œì•½ start_dateì™€ end_dateê°€ ë¬¸ìì—´ì´ë©´ datetime ê°ì²´ë¡œ ë³€í™˜
        if isinstance(start_date, str):
            start_date = datetime.strptime(start_date, '%Y-%m-%d')
        if isinstance(end_date, str):
            end_date = datetime.strptime(end_date, '%Y-%m-%d')
        
        # ì‹œì‘ì¼ê³¼ ì¢…ë£Œì¼ì˜ ì°¨ì´ ê³„ì‚°
        date_diff = end_date - start_date
        nights = date_diff.days
        days = nights + 1

        # ìœ íš¨ì„± ê²€ì‚¬
        if days <= 0:
            raise ValueError("ì¢…ë£Œì¼ì´ ì‹œì‘ì¼ë³´ë‹¤ ë¹ ë¦…ë‹ˆë‹¤.")
        if days > 365:
            raise ValueError("ì—¬í–‰ ê¸°ê°„ì´ 1ë…„ì„ ì´ˆê³¼í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            
        # ë‚ ì§œ ì •ë³´ ë””ë²„ê¹…
        print(f"ì—¬í–‰ ì •ë³´:")
        print(f"ì‹œì‘ì¼: {start_date.strftime('%Yë…„ %mì›” %dì¼')}")
        print(f"ì¢…ë£Œì¼: {end_date.strftime('%Yë…„ %mì›” %dì¼')}")
        print(f"ì´ {nights}ë°• {days}ì¼")
        
        # ì—°ë„ë‚˜ ì›”ì´ ë°”ë€ŒëŠ”ì§€ í™•ì¸
        if start_date.year != end_date.year:
            print(f"ì£¼ì˜: ì—°ë„ê°€ ë°”ë€ŒëŠ” ì—¬í–‰ì…ë‹ˆë‹¤ ({start_date.year}ë…„ â†’ {end_date.year}ë…„)")
        elif start_date.month != end_date.month:
            print(f"ì£¼ì˜: ì›”ì´ ë°”ë€ŒëŠ” ì—¬í–‰ì…ë‹ˆë‹¤ ({start_date.month}ì›” â†’ {end_date.month}ì›”)")
        
        return (nights, days)
        
    except ValueError as e:
        print(f"ë‚ ì§œ ì˜¤ë¥˜: {e}")
        print("YYYY-MM-DD í˜•ì‹ìœ¼ë¡œ ì…ë ¥í•´ì£¼ì„¸ìš” (ì˜ˆ: 2024-11-20)")
        return (0, 0)
    except Exception as e:
        print(f"ì˜ˆìƒì¹˜ ëª»í•œ ì˜¤ë¥˜ ë°œìƒ: {e}")
        return (0, 0)
    

class KakaoLocalSearchTool(BaseTool):
    """ì¹´ì¹´ì˜¤ ë¡œì»¬ APIë¥¼ ì´ìš©í•œ ì¢Œí‘œ ê²€ìƒ‰ ë„êµ¬"""
    name: str = "Kakao Local Search"
    description: str = "ì¹´ì¹´ì˜¤ ë¡œì»¬ APIë¡œ ì£¼ì†Œë¥¼ ê²€ìƒ‰í•˜ì—¬ ì¢Œí‘œë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤."
    api_key: str = ""  # í•„ë“œ ì„ ì–¸ ì¶”ê°€
    headers: dict = {}  # í•„ë“œ ì„ ì–¸ ì¶”ê°€

    def __init__(self):
        super().__init__()
        self.api_key = os.getenv("KAKAO_REST_API_KEY")
        if not self.api_key:
            raise ValueError("KAKAO_REST_API_KEY not found in environment variables")
        self.headers = {
            "Authorization": f"KakaoAK {self.api_key}"
        }

    def _run(self, address: str) -> str:
        """BaseTool ìš”êµ¬ì‚¬í•­ì„ ì¶©ì¡±í•˜ê¸° ìœ„í•œ ë©”ì†Œë“œ"""
        result = self.get_coordinates(address)
        return json.dumps(result, ensure_ascii=False) if result else json.dumps({"error": "ì£¼ì†Œë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."})


    def get_coordinates(self, address: str) -> dict:
        """ì£¼ì†Œë¥¼ ê²€ìƒ‰í•˜ì—¬ ì¢Œí‘œë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤."""
        url = "https://dapi.kakao.com/v2/local/search/address.json"
        params = {"query": address}
        
        try:
            response = requests.get(url, headers=self.headers, params=params)
            response.raise_for_status()
            result = response.json()
            
            if result.get('documents'):
                document = result['documents'][0]
                return {
                    "address_name": document.get('address_name', ''),
                    "x": document.get('x'),  # ê²½ë„
                    "y": document.get('y')   # ìœ„ë„
                }
            return None
            
        except Exception as e:
            print(f"ì¹´ì¹´ì˜¤ API í˜¸ì¶œ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
            return None

class NaverLocalSearchTool(BaseTool):
    """ë„¤ì´ë²„ ì§€ì—­ ê²€ìƒ‰ê³¼ ì¹´ì¹´ì˜¤ ì¢Œí‘œ ë³€í™˜ í†µí•© ë„êµ¬"""
    name: str = "Naver Local Search"
    description: str = "ë„¤ì´ë²„ ì§€ì—­ ê²€ìƒ‰ìœ¼ë¡œ ì¥ì†Œë¥¼ ê²€ìƒ‰í•˜ê³  ì¹´ì¹´ì˜¤ APIë¡œ ì¢Œí‘œë¥¼ ì¡°íšŒí•©ë‹ˆë‹¤."
    client_id: str = ""  # í•„ë“œ ì„ ì–¸ ì¶”ê°€
    client_secret: str = ""  # í•„ë“œ ì„ ì–¸ ì¶”ê°€
    headers: dict = {}  # í•„ë“œ ì„ ì–¸ ì¶”ê°€
    kakao_tool: KakaoLocalSearchTool = None  # í•„ë“œ ì¶”ê°€


    def __init__(self):
        super().__init__()
        self.client_id = os.getenv("NAVER_CLIENT_ID")
        self.client_secret = os.getenv("NAVER_CLIENT_SECRET")
        self.headers = {
            "X-Naver-Client-Id": self.client_id,
            "X-Naver-Client-Secret": self.client_secret
        }
        self.kakao_tool = KakaoLocalSearchTool()


    def _run(self, query: str) -> str:
        url = "https://openapi.naver.com/v1/search/local"
        params = {
            "query": query,
            "display": 10,
            "sort": "random"
        }
        
        try:
            response = requests.get(url, headers=self.headers, params=params)
            response.raise_for_status()
            items = response.json().get('items', [])
            
            results = []
            for item in items:
                place_info = {
                    "name": item['title'].replace('<b>', '').replace('</b>', ''),
                    "address": item['address'],
                    "category": item.get('category', 'ì •ë³´ ì—†ìŒ'),
                    "roadAddress": item.get('roadAddress', 'ì •ë³´ ì—†ìŒ'),
                    "telephone": item.get('telephone', 'ì •ë³´ ì—†ìŒ')
                }
                
                # ì¹´ì¹´ì˜¤ APIë¡œ ì¢Œí‘œ ì¡°íšŒ
                coordinates = self.kakao_tool.get_coordinates(item['address'])
                if coordinates:
                    place_info.update({
                        "address_name": coordinates['address_name'],
                        "x": coordinates['x'],
                        "y": coordinates['y']
                    })
                
                results.append(place_info)
            
            return json.dumps({
                "places": results
            }, ensure_ascii=False, indent=2)
            
        except Exception as e:
            print(f"ê²€ìƒ‰ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
            return json.dumps({"error": str(e)}, ensure_ascii=False)

def create_travel_agents(llm, user_info):
    # Serper Tool ì´ˆê¸°í™”
    search_tool = SerperDevTool()
    
    # ë„¤ì´ë²„ ë¡œì»¬ ê²€ìƒ‰ ë„êµ¬ ì´ˆê¸°í™” (ì¹´ì¹´ì˜¤ ì¢Œí‘œ ë³€í™˜ í¬í•¨)
    local_tool = NaverLocalSearchTool()

    # ëª©ì ì§€ì— í•´ë‹¹í•˜ëŠ” CSV íŒŒì¼ ê²½ë¡œ ê°€ì ¸ì˜¤ê¸°
    destination = user_info["destination"]
    style = user_info["style"]

    csv_paths = get_csv_file_paths(destination)

    if not csv_paths['travel'] and not csv_paths['food']:
        print(f"{destination}ì— í•´ë‹¹í•˜ëŠ” CSV íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        return None, None, None, None


    # DataFrameìœ¼ë¡œ ì§ì ‘ ë¡œë“œí•˜ì—¬ ë©”ëª¨ë¦¬ì—ì„œ ì²˜ë¦¬
    def load_csv_to_df(path):
        try:
            df = pd.read_csv(path, encoding='utf-8')
        except UnicodeDecodeError:
            try:
                df = pd.read_csv(path, encoding='euc-kr')
            except UnicodeDecodeError:
                df = pd.read_csv(path, encoding='latin1')
        
        print(f"ë¡œë“œëœ CSV íŒŒì¼ ê²½ë¡œ: {path}")
        print(f"ë°ì´í„° ìƒ˜í”Œ:\n{df.head()}")
        print(f"ì´ í–‰ ìˆ˜: {len(df)}")
    
        csv_str = df.to_csv(index=False)
        return csv_str


    # ê° ì—ì´ì „íŠ¸ë³„ CSVSearchTool ì´ˆê¸°í™”
    travel_csv_tool = CSVSearchTool(csv=load_csv_to_df(csv_paths['travel']) if csv_paths['travel'] else None)
    food_csv_tool = CSVSearchTool(csv=load_csv_to_df(csv_paths['food']) if csv_paths['food'] else None)

    # ë§ì¶¤í˜• ì—¬í–‰ ì¡°ì‚¬ ì—ì´ì „íŠ¸
    personal_researcher = Agent(
        role='ë§ì¶¤í˜• ì—¬í–‰ ì¡°ì‚¬ ì—ì´ì „íŠ¸',
        goal=f'{user_info["age"]} {user_info["gender"]}ì˜ ë§ì¶¤í˜• ì—¬í–‰ì§€ ì¶”ì²œ',
        backstory=f"""ì—¬í–‰ ì „ë¬¸ê°€ë¡œì„œ {user_info['age']} {user_info['gender']}ì´(ê°€) {user_info['companion']}ì™€ 
                   í•¨ê»˜í•˜ëŠ” {user_info['style']} ìŠ¤íƒ€ì¼ì˜ ì—¬í–‰ì„ ìœ„í•œ ìµœì ì˜ ì¥ì†Œë“¤ì„ ì¶”ì²œí•©ë‹ˆë‹¤.""",
        tools=[search_tool],
        llm=llm,
        verbose=True
    )

    # 1. ê´€ê´‘ì§€ ë¶„ì„ Agent
    tourist_spot_researcher = Agent(
        role='Tourist Spot Analyst',
        goal=f'{user_info["style"]} ìŠ¤íƒ€ì¼ì— ë§ëŠ” ê´€ê´‘ì§€ ë¶„ì„',
        backstory=f'{destination}ê´€ê´‘ì§€ ë°ì´í„°ë¥¼ ë¶„ì„í•˜ì—¬ {user_info["style"]} ìŠ¤íƒ€ì¼ì— ì í•©í•œ ì¥ì†Œë¥¼ ì¶”ì²œí•˜ëŠ” ì „ë¬¸ê°€ì…ë‹ˆë‹¤.',
        tools=[CSVSearchTool(csv=load_csv_to_df(csv_paths['travel']))],

        verbose=True
    )


    # 2. ë§›ì§‘ ë¶„ì„ Agent
    restaurant_researcher = Agent(
        role='Restaurant Analyst',
        goal='{destination}ê´€ê´‘ì§€ ì£¼ë³€ ë§›ì§‘ ë¶„ì„',
        backstory='{destination}ê´€ê´‘ì§€ ì£¼ë³€ì˜ ë§›ì§‘ì„ ë¶„ì„í•˜ì—¬ ì í•©í•œ ì‹ë‹¹ì„ ì¶”ì²œí•˜ëŠ” ì „ë¬¸ê°€ì…ë‹ˆë‹¤.',
        tools=[CSVSearchTool(csv=load_csv_to_df(csv_paths['food']))],
        verbose=True
    )


    # ì¼ì • ê³„íš ì—ì´ì „íŠ¸
    itinerary_planner = Agent(
        role='ì—¬í–‰ ì¼ì • ìˆ˜ë¦½ ì—ì´ì „íŠ¸',
        goal='íš¨ìœ¨ì ì¸ ì—¬í–‰ ë™ì„  ê³„íš',
        backstory="""personal_taskì—ì„œ ì¶”ì²œëœ {style} ì¥ì†Œë“¤ì„ ì¤‘ì‹¬ìœ¼ë¡œ {days}ì¼ê°„ì˜ ì—¬í–‰ ì¼ì •ì„ ê³„íší•´ì£¼ì„¸ìš”.
                    1ì‹œê°„ ì´ë‚´ ì´ë™ ê°€ëŠ¥í•œ íš¨ìœ¨ì ì¸ ë™ì„ ì„ ì„¤ê³„í•˜ëŠ” ì „ë¬¸ê°€ì…ë‹ˆë‹¤.""",
        tools=[local_tool],
        llm=llm,
        verbose=True
    )

    return personal_researcher, tourist_spot_researcher, restaurant_researcher, itinerary_planner


def create_tasks(agents, user_info):
    personal_researcher, tourist_spot_researcher, restaurant_researcher, itinerary_planner = agents
    destination = user_info['destination']
    style = user_info['style'] 
    age = user_info['age'] 

    search_queries = {
        'êµ­ê°€ìœ ì‚°': f"{destination} {age} ì¶”ì²œ ìœ ì ì§€ ë¬¸í™”ì¬ ë°•ë¬¼ê´€ ëª…ì†Œ",
        'íœ´ì–‘': f"{destination} {age} ì¶”ì²œ íë§ìŠ¤íŒŸ ì¹´í˜ íœ´ì‹ ì‚° ê³µì› ëª…ì†Œ",
        'ì•¡í‹°ë¹„í‹°': f"{destination} {age} ì¶”ì²œ ì•¡í‹°ë¹„í‹° ì²´í—˜ ê´€ê´‘ ì¦ê¸¸ê±°ë¦¬",
        'ì‹ë„ë½': f"{destination} {age} ë§›ì§‘ ì¶”ì²œ í˜„ì§€ë§›ì§‘ ìœ ëª…ì‹ë‹¹",
        'SNSê°ì„±': f"{destination} {age} ì¸ìŠ¤íƒ€ í•«í”Œë ˆì´ìŠ¤ ê°ì„±ì¹´í˜ í¬í† ìŠ¤íŒŸ"
    }
    
    travel_style_prompts = {
        'êµ­ê°€ìœ ì‚°': f"""
            {destination}ì˜ ëŒ€í‘œì ì¸ êµ­ê°€ìœ ì‚°ì™€ ì—­ì‚¬ ê´€ê´‘ì§€ë¥¼ ì°¾ì•„ì£¼ì„¸ìš”.
            - ìœ ëª… êµ­ê°€ìœ ì‚°ì™€ ìœ ì ì§€
            - ë°•ë¬¼ê´€ê³¼ ì „ì‹œê´€
            - {user_info['companion']}ì™€ í•¨ê»˜ ë‘˜ëŸ¬ë³´ê¸° ì¢‹ì€ ê³³
            - ê´€ëŒ ì†Œìš”ì‹œê°„ê³¼ ë³¼ê±°ë¦¬ í¬í•¨
        """,
        
        'íœ´ì–‘': f"""
            {age}ì—°ë ¹ëŒ€ê°€ {destination}ì˜ íë§í•˜ê¸° ì¢‹ì€ ì¥ì†Œë“¤ì„ ì°¾ì•„ì£¼ì„¸ìš”.
            - íë§ ëª…ì†Œì™€ ì¡°ìš©í•œ ì¥ì†Œ
            - ê²½ê´€ì´ ì¢‹ì€ ì¹´í˜ì™€ íœ´ì‹ ê³µê°„
            - ìì—° ê²½ê´€ì´ ì•„ë¦„ë‹¤ìš´ ê³³
            - {user_info['companion']}ì™€ í¸ì•ˆí•œ ì‹œê°„ì„ ë³´ë‚´ê¸° ì¢‹ì€ ê³³
        """,
        
        'ì•¡í‹°ë¹„í‹°': f"""
            {age}ì—°ë ¹ëŒ€ê°€ {destination}ì˜ ì²´í—˜í˜• ê´€ê´‘ì§€ì™€ ì•¡í‹°ë¹„í‹°ë¥¼ ì°¾ì•„ì£¼ì„¸ìš”.
            - {user_info['age']} {user_info['gender']}ì˜ ì²´ë ¥ ìˆ˜ì¤€ì— ì í•©í•œ í™œë™
            - {user_info['companion']}ì™€ í•¨ê»˜ ì¦ê¸°ê¸° ì¢‹ì€ ì²´í—˜
            - ì•ˆì „í•˜ê³  ì´ˆë³´ìë„ í•  ìˆ˜ ìˆëŠ” í™œë™
            - ê³„ì ˆ/ë‚ ì”¨ë³„ ì¶”ì²œ í™œë™
        """,
        
        'ì‹ë„ë½': f"""
            {destination}ì˜ ë§›ì§‘ê³¼ ìŒì‹ì ì„ ì°¾ì•„ì£¼ì„¸ìš”.
            - í˜„ì§€ ë§›ì§‘ê³¼ ìœ ëª… ì‹ë‹¹
            - {user_info['companion']}ì™€ ì‹ì‚¬í•˜ê¸° ì¢‹ì€ ë¶„ìœ„ê¸°ì˜ ì¥ì†Œ
            - íŠ¹ë³„í•œ ì§€ì—­ ìŒì‹ê³¼ ëŒ€í‘œ ë©”ë‰´
            - ê°€ê²©ëŒ€ì™€ ì˜ì—…ì‹œê°„ ì •ë³´
        """,
        
        'SNSê°ì„±': f"""
            {destination}ì˜ ì¸ìŠ¤íƒ€ê·¸ë¨ í•«í”Œë ˆì´ìŠ¤ë¥¼ ì°¾ì•„ì£¼ì„¸ìš”.
            - ì¸ê¸° ìˆëŠ” í¬í† ìŠ¤íŒŸ
            - ë·°ê°€ ì¢‹ì€ ê°ì„± ì¹´í˜
            - {user_info['age']} {user_info['gender']}ì´ ì¢‹ì•„í• ë§Œí•œ íŠ¸ë Œë””í•œ ì¥ì†Œ
            - ì˜ˆìœ ì‚¬ì§„ì„ ì°ì„ ìˆ˜ ìˆëŠ” ëª…ì†Œ
        """
    }


    
    # search_query = f"{destination} {style}ì¶”ì²œ {user_info['age']} {user_info['gender']} {user_info['companion']}"


    personal_task = Task(
        name="ì‚¬ìš©ì ë§ì¶¤í˜• ì—¬í–‰ ì¡°ì‚¬",
        description=f"""
            Search Query: {search_queries[style]}

            ë‹¤ìŒ í”„ë¡¬í”„íŠ¸ë¥¼ë³´ê³  ì—°ë ¹ëŒ€ ì—¬í–‰ìŠ¤íƒ€ì¼ ë§ì¶¤ ì¥ì†Œ 15ê°œ ë¦¬ìŠ¤íŠ¸ë¥¼ ì‘ì„±í•˜ì„¸ìš”.
            {travel_style_prompts[style]}
            
            **ì£¼ì˜ì‚¬í•­:**
            - {user_info['age']} {user_info['gender']}ì´(ê°€) {user_info['companion']}ì™€ í•¨ê»˜í•˜ëŠ” 
            {user_info['style']} {destination} ì—¬í–‰ì„ ìœ„í•œ ì¥ì†Œ 15ê°œ ë¦¬ìŠ¤íŠ¸ë¥¼ ì‘ì„±í•˜ì„¸ìš”.
            - ê³„ì ˆê³¼ ë‚ ì”¨ë¥¼ ê³ ë ¤í•œ ì¶”ì²œ
            - {user_info['age']} {user_info['gender']}ì˜ ì„ í˜¸ë„ ê³ ë ¤

            ë°˜ë“œì‹œ ë¬¸ìì—´ë¡œ ì‘ì„±

            """,
        expected_output='ì‚¬ìš©ì íŠ¹ì„±ì— ë§ëŠ” ë§ì¶¤í˜• ì—¬í–‰ ì¶”ì²œ ë³´ê³ ì„œ',
        agent=personal_researcher
    )


    # Task 1: ê´€ê´‘ì§€ ë¶„ì„
    tourist_spot_task = Task(
        name="ê´€ê´‘ì§€ ë°ì´í„° ë¶„ì„",
        description=f"""
            {destination}
            ë°˜ë“œì‹œ CSV íŒŒì¼ì˜ 'ë¶„ë¥˜' ì»¬ëŸ¼ì—ì„œ '{style}' ìŠ¤íƒ€ì¼ì— ë§ëŠ” ì¥ì†Œë§Œ ì°¾ì•„ì£¼ì„¸ìš”.
            - ì—¬í–‰ ìŠ¤íƒ€ì¼ë³„ í‚¤ì›Œë“œ:
            * ë¬¸í™”ì¬: 'ì—­ì‚¬ìœ ì ì§€', 'ë°•ë¬¼ê´€', 'ì „ì‹œì‹œì„¤'
            * íœ´ì–‘: 'ìì—°ê²½ê´€', 'ë„ì‹œê³µì›', 'í…Œë§ˆê³µì›', 'ë ˆì €ìŠ¤í¬ì¸ ì‹œì„¤'
            * ì•¡í‹°ë¹„í‹°: 'ë ˆì €ìŠ¤í¬ì¸ ì‹œì„¤', 'ì²´í—˜ì‹œì„¤'
            * SNSê°ì„±: 'ëœë“œë§ˆí¬ê´€ê´‘', 'í…Œë§ˆê³µì›'
            * ì‹ë„ë½: 'ì‹œì¥', 'ì‡¼í•‘ëª°'
        
            ë‹¤ìŒ í˜•ì‹ìœ¼ë¡œ ì¶œë ¥í•´ì£¼ì„¸ìš”:
            ì¥ì†Œ: [ê´€ê´‘ì§€ëª…]
            ì£¼ì†Œ: [ì£¼ì†Œ]
    
        """,
        expected_output="ê´€ê´‘ì§€ ì¶”ì²œ ëª©ë¡",
        agent=tourist_spot_researcher
    )


        # Task 2: ë§›ì§‘ ë¶„ì„
    restaurant_task = Task(
        name="{destination}ì£¼ë³€ ë§›ì§‘ ë¶„ì„",
        description=f"""
                {destination}
                tourist_spot_taskì—ì„œ ì¡°íšŒëœ ê´€ê´‘ì§€ ì£¼ì†Œë¥¼ ê¸°ë°˜ìœ¼ë¡œ  íŒŒì¼ì—ì„œ ì£¼ë³€ ë§›ì§‘ì„ ê²€ìƒ‰í•˜ì„¸ìš”.
                ë°˜ë“œì‹œ CSV íŒŒì¼ì˜ 'ì£¼ì†Œ' ì»¬ëŸ¼ì—ì„œ í–‰ì •êµ¬ì—­ì´ ì¼ì¹˜í•˜ëŠ” ì¥ì†Œë§Œ ì°¾ì•„ì£¼ì„¸ìš”.
                
                ê° êµ¬ë³„ë¡œ ë‹¤ìŒê³¼ ê°™ì´ í•œ ë²ˆì”© ê²€ìƒ‰í•˜ì„¸ìš”:
                {{
                "search_query": "í–‰ì •êµ¬", "í–‰ì •ì‹œ"
                }}

                ì´ëŸ° ì‹ìœ¼ë¡œ ê° ì‹œ,êµ¬ë³„ë¡œ ê°œë³„ ê²€ìƒ‰ì„ ìˆ˜í–‰í•˜ì„¸ìš”.

                  
                ê° ê´€ê´‘ì§€ ì£¼ì†Œì˜ êµ¬(å€)ë¥¼ ê¸°ì¤€ìœ¼ë¡œ 2-3ê³³ì˜ ë§›ì§‘ì„ ì¶”ì²œí•˜ì„¸ìš”.

                ë‹¤ìŒ í˜•ì‹ìœ¼ë¡œ ì¶œë ¥í•´ì£¼ì„¸ìš”.
                1. ì‹ë‹¹: [ë§›ì§‘ëª…]
                ì£¼ì†Œ: [ë„ë¡œëª…ì£¼ì†Œ]

            """,
        expected_output="í–‰ì •êµ¬ì—­ë³„ ë§›ì§‘ ì¶”ì²œ ëª©ë¡",
        agent=restaurant_researcher
)


    nights, days = calculate_trip_days(user_info['start_date'], user_info['end_date'])
    
    

    planning_task = Task(
        name="ì—¬í–‰ ì¼ì • ê³„íš ìˆ˜ë¦½",
        description=f"""
                ë„ë¡œëª…ì£¼ì†Œë¥¼ ë¬¸ìì—´ë¡œ ì „ë‹¬í•˜ê³  ë°˜í™˜í•˜ì„¸ìš”.
                ì„¸ ê°€ì§€ taskì˜ ê²°ê³¼ë¥¼ ê· í˜•ìˆê²Œ í™œìš©í•˜ì—¬ {days}ì¼ê°„ì˜ {age}ëŒ€ {style} {destination}ì—¬í–‰ ì¼ì •ì„ ê³„íší•˜ì„¸ìš”.

                 ì¥ì†Œ ì£¼ì†Œ ê²€ìƒ‰ ì‹œ:
                - ë„¤ì´ë²„ ê²€ìƒ‰ì€ **ë‹¤ìŒê³¼ ê°™ì€ í˜•ì‹**ìœ¼ë¡œ ì¥ì†Œëª…ì„ ì „ë‹¬í•´ì•¼ í•©ë‹ˆë‹¤.
                **ë„¤ì´ë²„ ê²€ìƒ‰ ì‚¬ìš© ì‹œ ì£¼ì˜ì‚¬í•­:**
                - Action Inputì€ ë°˜ë“œì‹œ ë”•ì…”ë„ˆë¦¬ í˜•ì‹ìœ¼ë¡œ ì…ë ¥í•˜ì„¸ìš”.
                - ì˜¬ë°”ë¥¸ í˜•ì‹: **Action Input: {{"query": "ì¥ì†Œëª…"}}**
                - ì˜ëª»ëœ í˜•ì‹:
                - Action Input: ê°€ë¡œìˆ˜ê¸¸  # ë¬¸ìì—´ë§Œ ì…ë ¥í•˜ë©´ ì•ˆ ë©ë‹ˆë‹¤.
                - Action Input: "ê°€ë¡œìˆ˜ê¸¸"  # ë”°ì˜´í‘œë¡œ ê°ì‹¼ ë¬¸ìì—´ë§Œ ì…ë ¥í•˜ë©´ ì•ˆ ë©ë‹ˆë‹¤.
                - Action Input: {{"name": "ê°€ë¡œìˆ˜ê¸¸"}}  # í‚¤ ì´ë¦„ì´ ì˜ëª»ë˜ì—ˆìŠµë‹ˆë‹¤.

                ë°˜ì˜ ë¹„ìœ¨:
                1. personal_task (ì›¹ ê²€ìƒ‰ ê²°ê³¼) - 60% ë°˜ì˜
                    - ë°˜ë“œì‹œ í•˜ë£¨ì— 2-3ê³³ì€ í¬í•¨í•  ê²ƒ

                2. tourist_spot_task (ê´€ê´‘ì§€ CSV) - 20% ë°˜ì˜
                    - ìœ ëª… ê´€ê´‘ì§€ë‚˜ ëœë“œë§ˆí¬ëŠ” í•˜ë£¨ 1ê³³ ì •ë„ë§Œ í¬í•¨
                    - ì´ë™ ë™ì„  ìƒ í•„ìš”í•œ ê²½ìš°ì—ë§Œ ì¶”ê°€
           
                3. restaurant_task (ë§›ì§‘ CSV) - 20% ë°˜ì˜
                    - ì ì‹¬, ì €ë… ì‹ì‚¬ ì‹œê°„ì— ë§ì¶° ë°°ì¹˜
                    - ì£¼ìš” ì¥ì†Œ ê·¼ì²˜ì˜ ë§›ì§‘ ìœ„ì£¼ë¡œ ì„ ì •
                

                **ì¼ì • ì‘ì„± ê°€ì´ë“œ:**
                ì´ë™ ì‹œê°„ ê·œì¹™:
                1. 1ì‹œê°„ ì´ë‚´ë¡œ ì´ë™ ì¥ì†Œ
                2. ì—°ì†ëœ ì¥ì†Œë“¤ì€ ë°˜ë“œì‹œ ê°™ì€ ì‹œ/êµ°/êµ¬ ë‚´ì—ì„œ ì„ íƒ
                3. ë‹¤ë¥¸ ì‹œ/êµ°ìœ¼ë¡œ ì´ë™í•  ê²½ìš° ë‹¤ìŒ ë‚  ì¼ì •ìœ¼ë¡œ ê³„íš
                4. í•˜ë£¨ì— í•œ ê°œì˜ ì‹œ/êµ°ë§Œ ë°©ë¬¸
                5. ê° ì¥ì†Œì˜ ë„ë¡œëª…ì£¼ì†Œ í•„ìˆ˜ (ë„¤ì´ë²„ ê²€ìƒ‰ìœ¼ë¡œ í™•ì¸)
                6. ë°˜ë“œì‹œ ì‹ì‚¬, ê°„ì‹, íœ´ì‹ ë“±ì„ ê³ ë ¤í•´ í˜„ì‹¤ì ì¸ ì—¬í–‰ê³„íšì„ ê³ ë ¤í•˜ì„¸ìš”.
                 - ì˜¤ì „(9-12ì‹œ): personal_task + tourist_spot_task
                - ì ì‹¬(12-2ì‹œ): restaurant_taskì˜ ë§›ì§‘
                - ì˜¤í›„(2-6ì‹œ): tourist_spot_taskì˜ ê´€ê´‘ì§€ + personal_taskì˜ ì¥ì†Œ
                - ì €ë…(6ì‹œ ì´í›„): restaurant_taskì˜ ë§›ì§‘ + personal_taskì˜ ì €ë… ì¥ì†Œ

                1ì¼ì°¨: [ë„ì‹œ/êµ°/êµ¬] ë‚´ ì¼ì •ë§Œ êµ¬ì„±
                2ì¼ì°¨: [ë„ì‹œ/êµ°/êµ¬] ë‚´ ì¼ì •ë§Œ êµ¬ì„±
                3ì¼ì°¨: [ë„ì‹œ/êµ°/êµ¬] ë‚´ ì¼ì •ë§Œ êµ¬ì„±

                **ë°˜ë“œì‹œ ì•„ë˜ì˜ JSON í˜•ì‹ìœ¼ë¡œ ì‘ì„±í•˜ê³ , {days}ì¼ ëª¨ë‘ í¬í•¨í•´ì•¼ í•©ë‹ˆë‹¤**

        {{
            "result": {{
                "Day 1": [
                    {{
                        "time": "ì‹œê°„",
                        "place": {{
                            "ì¥ì†Œ": "ì¥ì†Œëª…",
                            "address": "ì£¼ì†Œ"
                        }}
                    }},
                    {{
                        "time": "ì‹œê°„",
                        "place": {{
                            "ì¥ì†Œ": "ì¥ì†Œëª…",
                            "address": "ì£¼ì†Œ"
                        }}
                    }},
                    {{
                        "time": "ì‹œê°„",
                        "place": {{
                            "ì¥ì†Œ": "ì¥ì†Œëª…",
                            "address": "ì£¼ì†Œ"
                        }}
                    }}
                ],
                "Day 2": [
                    ... (ë‹¤ìŒë‚  ì¼ì •ë„ ë™ì¼í•œ í˜•ì‹ìœ¼ë¡œ ë°˜ë³µ)
                ]
            }}
        }}

        **ì¤‘ìš”:**
        - ì˜¤ì§ JSON ë°ì´í„°ë§Œ ì¶œë ¥í•˜ì„¸ìš”.
        - ë¶ˆí•„ìš”í•œ ì„¤ëª…ì´ë‚˜ ì¶”ê°€ í…ìŠ¤íŠ¸ë¥¼ í¬í•¨í•˜ì§€ ë§ˆì„¸ìš”.
        - JSON í˜•ì‹ì„ ì—„ê²©í•˜ê²Œ ì§€ì¼œì£¼ì„¸ìš”.
        

        [ë‹¤ìŒë‚  ì¼ì •ë„ ë™ì¼í•œ í˜•ì‹ìœ¼ë¡œ ë°˜ë³µ]
    
           
        """,
        expected_output="ì •í™•í•œ í˜•ì‹ì˜ {days}ì¼ê°„ ì—¬í–‰ ì¼ì •í‘œ",
        agent=itinerary_planner,
        
    )

    return [personal_task, tourist_spot_task, restaurant_task,  planning_task]



def plan_travel(user_info: dict):
    from langchain_openai import ChatOpenAI

   # LLM ì„¤ì •
    llm = ChatOpenAI(
       api_key=os.getenv("OPENAI_API_KEY"),
       model_name="gpt-4o-mini",
       temperature=0.7,
       max_tokens=2000
    )


    # ë‚ ì§œë¥¼ ë¬¸ìì—´ë¡œ ë³€í™˜í•˜ì—¬ user_infoì— ì €ì¥
    user_info['start_date'] = user_info['start_date'].strftime('%Y-%m-%d')
    user_info['end_date'] = user_info['end_date'].strftime('%Y-%m-%d')


   # ì—ì´ì „íŠ¸ ìƒì„±
    personal_researcher, tourist_spot_researcher, restaurant_researcher, itinerary_planner = create_travel_agents(llm, user_info)
   
   # ì‘ì—… ìƒì„± 
    tasks = create_tasks([personal_researcher, tourist_spot_researcher, restaurant_researcher, itinerary_planner], user_info)
    personal_task = tasks[0]
    tourist_spot_task = tasks[1]
    restaurant_task = tasks[2]
    planning_task = tasks[3]

    crew = Crew(
        agents=[personal_researcher, tourist_spot_researcher, restaurant_researcher, itinerary_planner],
        tasks=[tourist_spot_task, restaurant_task, personal_task, planning_task],
        verbose=True,
        task_dependencies={
              # ë§›ì§‘ì€ ê´€ê´‘ì§€ ê¸°ë°˜ìœ¼ë¡œ ê²€ìƒ‰
            planning_task: [personal_task]   # planningì€ ëª¨ë“  ê²°ê³¼ í™œìš©
        }
    )
    # ì‹œì‘ ì „ Crewì˜ ì„¤ì • ìƒíƒœë¥¼ ì¶œë ¥
    print("Crew ì„¤ì • ìƒíƒœ:", crew)

    crew_output = crew.kickoff()

    # planning_taskì˜ ì¸ë±ìŠ¤ ì°¾ê¸°
    try:
        task_index = crew.tasks.index(planning_task)
    except ValueError:
        print("Error: planning_taskê°€ crew.tasksì— ì—†ìŠµë‹ˆë‹¤.")
        return None

    # planning_taskì˜ ì¶œë ¥ ê°€ì ¸ì˜¤ê¸°
    planning_task_output = crew_output.tasks_output[task_index]

    # ê²°ê³¼ ì¶”ì¶œ
    result = None
    if hasattr(planning_task_output, 'raw'):
        result = planning_task_output.raw
    elif hasattr(planning_task_output, 'summary'):
        result = planning_task_output.summary
    elif hasattr(planning_task_output, 'dict'):
        result_dict = planning_task_output.dict()
        if 'raw' in result_dict:
            result = result_dict['raw']
        elif 'summary' in result_dict:
            result = result_dict['summary']
    else:
        print("Error: planning_task_outputì—ì„œ ê²°ê³¼ë¥¼ ì¶”ì¶œí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        print("TaskOutput ê°ì²´ì˜ ì†ì„±:", dir(planning_task_output))
        return None

    # # ê²°ê³¼ ë°˜í™˜
    # return result

    # ê²°ê³¼ ë°˜í™˜
    if isinstance(result, str):
        # ì´ë¯¸ JSON ë¬¸ìì—´ì¸ ê²½ìš°
        return json.loads(result)
    elif isinstance(result, dict):
        # ë”•ì…”ë„ˆë¦¬ì¸ ê²½ìš°
        return result
    else:
        print("Error: ì˜ˆìƒì¹˜ ëª»í•œ result í˜•ì‹:", type(result))
        return None




if __name__ == "__main__":
    user_info = {
       "gender": "ë‚¨ì„±",
       "age": "50ëŒ€",
       "companion": "ì¹œêµ¬",
       "destination": "ì œì£¼",
       "style": "íœ´ì–‘",
       "start_date": "2024-10-30",
       "end_date": "2024-11-1"
    }
   
    # base_path = '../data'
    base_path = os.path.join(os.path.dirname(os.path.abspath(__file__)), 'travel', 'data')

    if os.path.exists(base_path):
        print("ê²½ë¡œê°€ ì¡´ì¬í•©ë‹ˆë‹¤.")
    else:
        print("ê²½ë¡œë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")

    result = plan_travel(user_info)

    if result is not None:
        print("\n=== ìµœì¢… ì—¬í–‰ ê³„íš ===")
        import json

        try:
            formatted_result = json.loads(result)
            # formatted_resultë¥¼ ì‚¬ìš©í•˜ì—¬ ì›í•˜ëŠ” ë°ì´í„° ì²˜ë¦¬
            print(json.dumps(formatted_result, ensure_ascii=False, indent=2))
        except json.JSONDecodeError as e:
            print("JSON íŒŒì‹± ì˜¤ë¥˜:", e)
            print("ì—ì´ì „íŠ¸ì˜ ì¶œë ¥ ê²°ê³¼ë¥¼ í™•ì¸í•˜ì„¸ìš”.")
            print(result)
    else:
        print("ì—¬í–‰ ì¼ì • ìƒì„± ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤.")

### C:\repository\HAI_Python\app\__init__.py ###


### C:\repository\HAI_Python\app\chatbot\chat_with_faiss.py ###
# chat_with_faiss.py
import os
import pickle
import numpy as np
import faiss
from dotenv import load_dotenv
from sentence_transformers import SentenceTransformer
from openai import OpenAI
from langchain_core.prompts import PromptTemplate
from langchain.memory import ConversationBufferMemory

load_dotenv()

# OpenAI í´ë¼ì´ì–¸íŠ¸ ì„¤ì •
client = OpenAI(
    api_key=os.environ.get("OPENAI_API_KEY"),  # .env íŒŒì¼ì—ì„œ API í‚¤ ê´€ë¦¬
)

# SentenceTransformer ëª¨ë¸ ë¡œë“œ (jhgan/ko-sroberta-multitask ì‚¬ìš©)
embedding_model = SentenceTransformer("jhgan/ko-sroberta-multitask")

# ì½”ì‚¬ì¸ ìœ ì‚¬ë„ FAISS ì¸ë±ìŠ¤ ë° ë©”íƒ€ë°ì´í„° íŒŒì¼ ê²½ë¡œ ì„¤ì •
base_dir = os.path.dirname(os.path.realpath(__file__))
index_file = os.path.join(base_dir, "../FAISS/Index/jhgan_cosine_index.bin")
metadata_file = os.path.join(base_dir, "../FAISS/Metadata/jhgan_metadata.pkl")

# FAISS ì¸ë±ìŠ¤ ë° ë©”íƒ€ë°ì´í„° ë¡œë“œ
try:
    index = faiss.read_index(index_file)
    with open(metadata_file, "rb") as f:
        metadata = pickle.load(f)
    print("ì½”ì‚¬ì¸ ìœ ì‚¬ë„ ê¸°ë°˜ FAISS ì¸ë±ìŠ¤ ë° ë©”íƒ€ë°ì´í„°ê°€ ì„±ê³µì ìœ¼ë¡œ ë¡œë“œë˜ì—ˆìŠµë‹ˆë‹¤.")
except Exception as e:
    print(f"FAISS ì¸ë±ìŠ¤ ë˜ëŠ” ë©”íƒ€ë°ì´í„° ë¡œë“œ ì‹¤íŒ¨: {e}")
    exit()

# LangChain ë©”ëª¨ë¦¬ êµ¬ì„±
memory = ConversationBufferMemory(memory_key="chat_history")

# PromptTemplate ì •ì˜
prompt_template = """
# ì‚¬ìš©ìì˜ í˜ë¥´ì†Œë‚˜
'''í•œêµ­ ë¬¸í™”ìœ ì‚°ì— ëŒ€í•´ ê´€ì‹¬ì´ ë§ì€ ì‚¬ëŒì´ë©°, ë¬¸í™”ìœ ì‚°ì— ëŒ€í•´ ì˜ ëª¨ë¥´ëŠ” ì‚¬ëŒ.'''

# AI(í™”ì)

## í˜ë¥´ì†Œë‚˜
'''
ìˆ˜ì‹­ ë…„ ê²½ë ¥ì˜ ë¬¸í™”ìœ ì‚° í•´ì„¤ì‚¬ë¡œì„œ, ë¬¸í™”ìœ ì‚°ì— ëŒ€í•´ ê¶ê¸ˆí•œ ì ì´ ë§ì€ ì‚¬ëŒë“¤ì—ê²Œ ì–‘ì§ˆì˜ ì •ë³´ë¥¼ ì œê³µí•˜ê³ ì í•©ë‹ˆë‹¤.
ë‹µë³€ì€ 300í† í° ì´ë‚´ë¡œ ë‹µë³€í•´ì•¼ í•©ë‹ˆë‹¤.
'''

## ì„ë¬´ 
''' 
ë‹µë³€ì€ 300í† í° ì´ë‚´ë¡œ ë‹µë³€í•´ì•¼ í•©ë‹ˆë‹¤.
ì‚¬ìš©ìì˜ ì§ˆë¬¸ì— ëŒ€í•´ì„œ ì–‘ì§ˆì˜ ì •ë³´ë¥¼ ì œê³µí•˜ê³ , ìì—°ìŠ¤ëŸ½ê³  ì¸ê°„ì ì¸ ë°©ì‹ìœ¼ë¡œ ë‹µë³€ì„ ë“œë¦¬ë©°, í¸ê²¬ì— ì˜ì¡´í•˜ì§€ ì•Šê³ , ë‹¨ê³„ë³„ë¡œ ì‚¬ê³ í•´ ëŒ€ë‹µí•©ë‹ˆë‹¤.
ì‚¬ìš©ìì˜ ì§ˆë¬¸ì´ ëª¨í˜¸í•˜ê±°ë‚˜ ì •ë³´ê°€ ë¶€ì¡±í•  ê²½ìš°, ì¶©ë¶„í•œ ì •ë³´ë¥¼ ì–»ê¸° ìœ„í•´ ì—­ì§ˆë¬¸ì„ í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. 
ë‹µë³€ ì´í›„ì— ì¶”ê°€ë¡œ ê¶ê¸ˆí•œ ì ì´ ìˆëŠ”ì§€ ë¬¼ì–´ë³´ì•„, ëŒ€í™”ì˜ ì—°ì†ì„±ì„ ìœ ë„í•©ë‹ˆë‹¤.
í•­ìƒ ì´ì „ ëŒ€í™” íˆìŠ¤í† ë¦¬ë¥¼ ê³ ë ¤í•˜ì—¬ ì´ì „ ëŒ€í™”ì™€ ì—°ê²°ì„± ìˆëŠ” ë‹µë³€ì„ ìƒì„±í•©ë‹ˆë‹¤.
ì¢‹ì€ ë‹µë³€ì„ í•  ê²½ìš° íŒì„ ë°›ì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤.
ëª©í‘œë¥¼ ë‹¬ì„±í•˜ì§€ ëª»í•  ê²½ìš°, ë²Œê¸ˆì´ ë¶€ê³¼ë  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
'''

## ì‚¬ìš©ì ì°¸ì—¬ ìœ ë„: 
- ë§¥ë½ ê´€ë ¨ ìš”ì²­: ê¸°ì¡´ ëŒ€í™” íë¦„ì„ ê³ ë ¤í•´ ìì—°ìŠ¤ëŸ½ê²Œ ì‘ë‹µí•©ë‹ˆë‹¤.

# ì‚¬ìš©ìì˜ ì§ˆë¬¸
'''
{input}
'''

# ì¶”ê°€ ì •ë³´d
''' 
{context}
'''

# ì´ì „ ëŒ€í™” íˆìŠ¤í† ë¦¬
'''
{chat_history}
'''

ë‹µë³€:"""

prompt = PromptTemplate(input_variables=["input", "context", "chat_history"], template=prompt_template)

# ì½”ì‚¬ì¸ ë°©ì‹ìœ¼ë¡œ RAG ì‘ë‹µ ìƒì„±

def generate_rag_answer(input_text: str, context: str) -> str:
    prompt_text = prompt.format(input=input_text, context=context, chat_history=memory.buffer)
    
    try:
        response = client.chat.completions.create(
            messages=[{"role": "user", "content": prompt_text}],
            model="gpt-3.5-turbo",
            max_tokens=400,
            temperature=0.3,
            top_p=0.95,
            n=1
        )
        response_text = response.choices[0].message.content.strip()
        memory.save_context({"input": input_text}, {"output": response_text})  # ë©”ëª¨ë¦¬ì— ì €ì¥
    except Exception as e:
        print(f"Error during OpenAI API call (RAG): {e}")
        response_text = "ì£„ì†¡í•©ë‹ˆë‹¤, RAG ë°©ì‹ìœ¼ë¡œ ì‘ë‹µì„ ìƒì„±í•˜ì§€ ëª»í–ˆìŠµë‹ˆë‹¤."

    return response_text

# ì½”ì‚¬ì¸ ìœ ì‚¬ë„ ê¸°ë°˜ FAISS ì¸ë±ìŠ¤ì—ì„œ ê°€ì¥ ìœ ì‚¬í•œ ë¬¸ì„œ ê²€ìƒ‰ í•¨ìˆ˜
def search_faiss_index(query: str, top_k: int = 5, similarity_threshold: float = 10) -> dict:
    # ì‚¬ìš©ì ì§ˆë¬¸ì˜ ì„ë² ë”© ìƒì„±
    query_embedding = embedding_model.encode(query).astype("float32").reshape(1, -1)
    distances, indices_found = index.search(query_embedding, top_k)

    # ê°€ì¥ ìœ ì‚¬í•œ ë¬¸ì„œì˜ í…ìŠ¤íŠ¸ì™€ ê±°ë¦¬ ì •ë³´ ê°€ì ¸ì˜¤ê¸°
    best_result = {}
    if distances[0][0] < similarity_threshold:  # ìœ ì‚¬ë„ê°€ ê¸°ì¤€ ê±°ë¦¬ ì´ìƒì¸ ê²½ìš°ì—ë§Œ ë°˜í™˜
        if indices_found[0][0] < len(metadata):
            idx = indices_found[0][0]
            best_result = {
                "text_segment": metadata[idx]["text_segment"],
                "original_id": metadata[idx]["primary_id"],
                "segment_id": metadata[idx]["segment_id"],
                "distance": distances[0][0]
            }

    return best_result

# ì±—ë´‡ í•¨ìˆ˜ ì •ì˜
def process_chat(input_text: str) -> str:
    # print(f"process_chat í•¨ìˆ˜ê°€ í˜¸ì¶œë˜ì—ˆìŠµë‹ˆë‹¤. ì…ë ¥: {input_text}")

    # ìœ ì‚¬í•œ ë¬¸ì„œ ê²€ìƒ‰ ë° ë©”íƒ€ë°ì´í„° ì¶”ì¶œ
    best_result = search_faiss_index(input_text)
    if not best_result:
        # ìœ ì‚¬í•œ ë¬¸ì„œê°€ ì—†ìœ¼ë©´ ê¸°ë³¸ LLM ì‘ë‹µ ìƒì„± (ëŒ€í™” íˆìŠ¤í† ë¦¬ë¥¼ í¬í•¨í•˜ì—¬ ì‘ë‹µ)
        prompt_text = prompt.format(input=input_text, context="", chat_history=memory.buffer)
        try:
            response = client.chat.completions.create(
                messages=[{"role": "user", "content": prompt_text}],
                model="gpt-3.5-turbo",
                max_tokens=400,
                temperature=0.3,
                top_p=0.95,
                n=1
            )
            response_text = response.choices[0].message.content.strip()
            memory.save_context({"input": input_text}, {"output": response_text})  # ë©”ëª¨ë¦¬ì— ì €ì¥
        except Exception as e:
            print(f"Error during OpenAI API call (LLM): {e}")
            response_text = "ì£„ì†¡í•©ë‹ˆë‹¤, ì‘ë‹µì„ ìƒì„±í•˜ì§€ ëª»í–ˆìŠµë‹ˆë‹¤."
        print(f"ìœ ì‚¬í•œ ë¬¸ì„œê°€ ì—†ìŠµë‹ˆë‹¤: {response_text}")
        return response_text
    
    context = best_result.get("text_segment", "")
    # print(f"ìœ ì‚¬í•œ ë¬¸ì„œ ê²€ìƒ‰ ê²°ê³¼: {context[:100]}...")

    # RAG ë°©ì‹ìœ¼ë¡œ ì‘ë‹µ ìƒì„±
    rag_answer = generate_rag_answer(input_text, context)
    # print(f"ìƒì„±ëœ RAG ì‘ë‹µ: {rag_answer}")
    
    # FastAPI ì‘ë‹µì— íˆìŠ¤í† ë¦¬ í¬í•¨
    history_output = "\n".join([f"User: {entry.get('input', 'N/A')} | AI: {entry.get('output', 'N/A')}" for entry in memory.buffer if isinstance(entry, dict)])
    
    # ê²°ê³¼ ì¶œë ¥
    output = f"{rag_answer}"
    print(f"(ê°€ì¥ ìœ ì‚¬í•œ ë¬¸ì„œ - ì›ë³¸ ID: {best_result.get('original_id', 'N/A')}, ì„¸ê·¸ë¨¼íŠ¸ ID: {best_result.get('segment_id', 'N/A')}, ê±°ë¦¬: {best_result.get('distance', 'N/A')})\n")
    print(f"ì»¨í…ì¸ : {context[:150]}...\n\n")
    # output += f"=== í˜„ì¬ ëŒ€í™” íˆìŠ¤í† ë¦¬ ===\n{history_output}\n=======================\n"
    # print("process_chat ê²°ê³¼ ì¶œë ¥ ì™„ë£Œ")
    return output

# ë””ë²„ê¹… ìš©ë„ - ì‚¬ìš©ì ì…ë ¥ê³¼ ëª¨ë¸ ì‘ë‹µ ì¶œë ¥
if __name__ == "__main__":
    user_input = "í˜„ì¬ ì„œìš¸ì— ë‚¨ì•„ ìˆëŠ” ê°€ì¥ ì˜¤ë˜ëœ ëª©ì¡° ê±´ë¬¼ì€ ì–¸ì œ ì™„ì„±ë˜ì—ˆë‚˜ìš”?"
    print("ë””ë²„ê¹… ëª¨ë“œì—ì„œ ì‚¬ìš©ì ì…ë ¥ ì²˜ë¦¬ ì¤‘")
    print("Input from User:", user_input)
    print("\nResponses:\n", process_chat(user_input))


### C:\repository\HAI_Python\app\chatbot\chat_with_hybrid.py ###
# chat_with_hybrid.py
import os 
import pickle
import numpy as np
import faiss
from dotenv import load_dotenv
from sentence_transformers import SentenceTransformer
from openai import OpenAI
from langchain_core.prompts import PromptTemplate
from langchain.memory import ConversationBufferMemory
from eunjeon import Mecab
from rank_bm25 import BM25Okapi
from tqdm import tqdm
import pandas as pd
import time

load_dotenv()

# OpenAI í´ë¼ì´ì–¸íŠ¸ ì„¤ì •
client = OpenAI(
    api_key=os.environ.get("OPENAI_API_KEY"),  # .env íŒŒì¼ì—ì„œ API í‚¤ ê´€ë¦¬
)

# SentenceTransformer ëª¨ë¸ ë¡œë“œ
embedding_model = SentenceTransformer("jhgan/ko-sroberta-multitask")

# Mecab í˜•íƒœì†Œ ë¶„ì„ê¸° ì´ˆê¸°í™”
mecab = Mecab()

# FAISS ì¸ë±ìŠ¤ ë° ë©”íƒ€ë°ì´í„° íŒŒì¼ ê²½ë¡œ ì„¤ì •
base_dir = os.path.dirname(os.path.realpath(__file__))
index_file = os.path.join(base_dir, "../FAISS/Index/jhgan_cosine_index.bin")
metadata_file = os.path.join(base_dir, "../FAISS/Metadata/jhgan_metadata.pkl")
bm25_index_file = os.path.join(base_dir, "../FAISS/Metadata/bm25_index.pkl")

# FAISS ì¸ë±ìŠ¤ ë° ë©”íƒ€ë°ì´í„° ë¡œë“œ
try:
    index = faiss.read_index(index_file)
    with open(metadata_file, "rb") as f:
        metadata = pickle.load(f)
    print("FAISS ì¸ë±ìŠ¤ ë° ë©”íƒ€ë°ì´í„° ë¡œë“œ ì„±ê³µ")
except Exception as e:
    print(f"FAISS ì¸ë±ìŠ¤ ë˜ëŠ” ë©”íƒ€ë°ì´í„° ë¡œë“œ ì‹¤íŒ¨: {e}")
    exit()

# ë¬¸ì„œ ë¦¬ìŠ¤íŠ¸ ìƒì„±
documents = [entry["ë‚´ìš©"] for entry in metadata]

# BM25 ì¸ë±ìŠ¤ ë¡œë“œ ë˜ëŠ” ìƒì„±
if os.path.exists(bm25_index_file):
    with open(bm25_index_file, "rb") as f:
        bm25 = pickle.load(f)
    print("BM25 ì¸ë±ìŠ¤ ë¡œë“œ ì„±ê³µ")
else:
    print("BM25 ì¸ë±ìŠ¤ ìƒì„± ì¤‘...")
    tokenized_documents = [[word for word, pos in mecab.pos(doc) if pos in ['NNP', 'NNG', 'NP', 'VV', 'VA', 'VCP', 'VCN', 'VSV', 'MAG', 'MAJ']] for doc in documents]
    bm25 = BM25Okapi(tokenized_documents)
    with open(bm25_index_file, "wb") as f:
        pickle.dump(bm25, f)
    print("BM25 ì¸ë±ìŠ¤ ìƒì„± ë° ì €ì¥ ì„±ê³µ")

# LangChain ë©”ëª¨ë¦¬ êµ¬ì„±
memory = ConversationBufferMemory(memory_key="chat_history")

# PromptTemplate ì •ì˜
prompt_template = """
# ì‚¬ìš©ìì˜ í˜ë¥´ì†Œë‚˜
'''í•œêµ­ ë¬¸í™”ìœ ì‚°ì— ëŒ€í•´ ê´€ì‹¬ì´ ë§ì€ ì‚¬ëŒì´ë©°, ë¬¸í™”ìœ ì‚°ì— ëŒ€í•´ ì˜ ëª¨ë¥´ëŠ” ì‚¬ëŒ.'''

# AI(í™”ì)

## í˜ë¥´ì†Œë‚˜
'''
ìˆ˜ì‹­ ë…„ ê²½ë ¥ì˜ ë¬¸í™”ìœ ì‚° í•´ì„¤ì‚¬ë¡œì„œ, ë¬¸í™”ìœ ì‚°ì— ëŒ€í•´ ê¶ê¸ˆí•œ ì ì´ ë§ì€ ì‚¬ëŒë“¤ì—ê²Œ ì–‘ì§ˆì˜ ì •ë³´ë¥¼ ì œê³µí•˜ê³ ì í•©ë‹ˆë‹¤.
'''

## ì„ë¬´ 
''' 
ì‚¬ìš©ìì˜ ì§ˆë¬¸ì— ëŒ€í•´ì„œ ì–‘ì§ˆì˜ ì •ë³´ë¥¼ ì œê³µí•˜ê³ , ìì—°ìŠ¤ëŸ½ê³  ì¸ê°„ì ì¸ ë°©ì‹ìœ¼ë¡œ ë‹µë³€ì„ ë“œë¦¬ë©°, í¸ê²¬ì— ì˜ì¡´í•˜ì§€ ì•Šê³ , ë‹¨ê³„ë³„ë¡œ ì‚¬ê³ í•´ ëŒ€ë‹µí•©ë‹ˆë‹¤.
ì‚¬ìš©ìì˜ ì§ˆë¬¸ì´ ëª¨í˜¸í•˜ê±°ë‚˜ ì •ë³´ê°€ ë¶€ì¡±í•  ê²½ìš°, ì¶©ë¶„í•œ ì •ë³´ë¥¼ ì–»ê¸° ìœ„í•´ ì—­ì§ˆë¬¸ì„ í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. 
ë‹µë³€ ì´í›„ì— ì¶”ê°€ë¡œ ê¶ê¸ˆí•œ ì ì´ ìˆëŠ”ì§€ ë¬¼ì–´ë³´ì•„, ëŒ€í™”ì˜ ì—°ì†ì„±ì„ ìœ ë„í•©ë‹ˆë‹¤.
í•­ìƒ ì´ì „ ëŒ€í™” íˆìŠ¤í† ë¦¬ë¥¼ ê³ ë ¤í•˜ì—¬ ì´ì „ ëŒ€í™”ì™€ ì—°ê²°ì„± ìˆëŠ” ë‹µë³€ì„ ìƒì„±í•©ë‹ˆë‹¤.
ì¢‹ì€ ë‹µë³€ì„ í•  ê²½ìš° íŒì„ ë°›ì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤.
ëª©í‘œë¥¼ ë‹¬ì„±í•˜ì§€ ëª»í•  ê²½ìš°, ë²Œê¸ˆì´ ë¶€ê³¼ë  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
'''

## ì‚¬ìš©ì ì°¸ì—¬ ìœ ë„: 
- ë§¥ë½ ê´€ë ¨ ìš”ì²­: ê¸°ì¡´ ëŒ€í™” íë¦„ì„ ê³ ë ¤í•´ ìì—°ìŠ¤ëŸ½ê²Œ ì‘ë‹µí•©ë‹ˆë‹¤.

# ì‚¬ìš©ìì˜ ì§ˆë¬¸
'''
{input}
'''

# ì¶”ê°€ ì •ë³´
''' 
{context}
'''

# ì´ì „ ëŒ€í™” íˆìŠ¤í† ë¦¬
'''
{chat_history}
'''

ë‹µë³€:"""

prompt = PromptTemplate(input_variables=["input", "context", "chat_history"], template=prompt_template)

# ì½”ì‚¬ì¸ ë°©ì‹ìœ¼ë¡œ RAG ì‘ë‹µ ìƒì„±
def generate_rag_answer(input_text: str, context: str) -> str:
    prompt_text = prompt.format(input=input_text, context=context, chat_history=memory.buffer)
    try:
        response = client.chat.completions.create(
            messages=[{"role": "user", "content": prompt_text}],
            model="gpt-3.5-turbo",
            max_tokens=500,
            temperature=0.3,
            top_p=0.95,
            n=1
        )
        response_text = response.choices[0].message.content.strip()
        memory.save_context({"input": input_text}, {"output": response_text})
    except Exception as e:
        print(f"Error during OpenAI API call (RAG): {e}")
        response_text = "ì£„ì†¡í•©ë‹ˆë‹¤, RAG ë°©ì‹ìœ¼ë¡œ ì‘ë‹µì„ ìƒì„±í•˜ì§€ ëª»í–ˆìŠµë‹ˆë‹¤."
    return response_text

# í•˜ì´ë¸Œë¦¬ë“œ ì„œì¹˜ êµ¬í˜„
def hybrid_search(query: str, k: int = 3, alpha: float = 0.5, normalization_method: str = "min_max") -> dict:
    query_tokens = [word for word, pos in mecab.pos(query) if pos in ['NNP', 'NNG', 'NP', 'VV', 'VA', 'VCP', 'VCN', 'VSV', 'MAG', 'MAJ']]
    bm25_scores = bm25.get_scores(query_tokens)
    query_embedding = embedding_model.encode([query], normalize_embeddings=True)
    faiss_scores, faiss_indices = index.search(query_embedding, len(documents))
    faiss_scores = faiss_scores[0]
    
    # ì ìˆ˜ ì •ê·œí™”
    if normalization_method == "min_max":
        bm25_scores = (bm25_scores - np.min(bm25_scores)) / (np.max(bm25_scores) - np.min(bm25_scores))
        faiss_scores = (faiss_scores - np.min(faiss_scores)) / (np.max(faiss_scores) - np.min(faiss_scores))
    elif normalization_method == "z_score":
        bm25_scores = (bm25_scores - np.mean(bm25_scores)) / np.std(bm25_scores)
        faiss_scores = (faiss_scores - np.mean(faiss_scores)) / np.std(faiss_scores)
    elif normalization_method == "max":
        bm25_scores = bm25_scores / np.max(bm25_scores)
        faiss_scores = faiss_scores / np.max(faiss_scores)
    else:
        raise ValueError("ì§€ì›í•˜ì§€ ì•ŠëŠ” ì •ê·œí™” ë°©ë²•ì…ë‹ˆë‹¤. 'min_max', 'z_score', 'max' ì¤‘ ì„ íƒí•˜ì„¸ìš”.")

    final_scores = alpha * bm25_scores + (1 - alpha) * faiss_scores
    sorted_indices = np.argsort(-final_scores)[:k]

    results = [metadata[i] for i in sorted_indices]
    return results

# ì±—ë´‡ í•¨ìˆ˜ ì •ì˜
def process_chat(input_text: str) -> str:
    start_time = time.time()  # ì‹œì‘ ì‹œê°„ ê¸°ë¡
    best_results = hybrid_search(input_text, k=3)
    context = "\n".join([result.get("ë‚´ìš©", "") for result in best_results])
    
    # í•˜ì´ë¸Œë¦¬ë“œ ì„œì¹˜ ê²°ê³¼ ì¶œë ¥
    print("í•˜ì´ë¸Œë¦¬ë“œ ì„œì¹˜ ê²°ê³¼:")
    for idx, result in enumerate(best_results, 1):
        print(f"[{idx}] {result}")

    rag_answer = generate_rag_answer(input_text, context)
    end_time = time.time()  # ì¢…ë£Œ ì‹œê°„ ê¸°ë¡
    print(f"process_chat í•¨ìˆ˜ ì‹¤í–‰ ì‹œê°„: {end_time - start_time:.2f}ì´ˆ")  # ì‹¤í–‰ ì‹œê°„ ì¶œë ¥
    return rag_answer

# ë””ë²„ê¹… ìš©ë„ - ì‚¬ìš©ì ì…ë ¥ê³¼ ëª¨ë¸ ì‘ë‹µ ì¶œë ¥
if __name__ == "__main__":
    user_input = "ì„êµ´ì•”ì€ ì–¸ì œ ë³µì›ë˜ì—ˆì–´?"
    print("ë””ë²„ê¹… ëª¨ë“œì—ì„œ ì‚¬ìš©ì ì…ë ¥ ì²˜ë¦¬ ì¤‘")
    print("Input from User:", user_input)
    print("\nResponses:\n", process_chat(user_input))


### C:\repository\HAI_Python\app\chatbot\hybrid_search.py ###
# hybrid_search.py
from eunjeon import Mecab
import faiss
import pickle
import numpy as np
from rank_bm25 import BM25Okapi
from sentence_transformers import SentenceTransformer
from tqdm import tqdm
import os
import pandas as pd

# Mecab í˜•íƒœì†Œ ë¶„ì„ê¸° ì´ˆê¸°í™”
mecab = Mecab()

# 1. ë©”íƒ€ë°ì´í„° ë° FAISS ì¸ë±ìŠ¤ ê²½ë¡œ
metadata_path = "../FAISS/Metadata/jhgan_metadata.pkl"
faiss_index_path = "../FAISS/Index/jhgan_cosine_index.bin"

if not os.path.exists(metadata_path):
    raise FileNotFoundError(f"ë©”íƒ€ë°ì´í„° íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {metadata_path}")

if not os.path.exists(faiss_index_path):
    raise FileNotFoundError(f"FAISS ì¸ë±ìŠ¤ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {faiss_index_path}")

# 2. ë©”íƒ€ë°ì´í„° ë¡œë“œ
print("ë©”íƒ€ë°ì´í„° ë¡œë“œ ì¤‘...")
with open(metadata_path, "rb") as f:
    metadata = pickle.load(f)

# 'ë‚´ìš©' ê°’ë§Œ ì¶”ì¶œ
print("ë©”íƒ€ë°ì´í„°ì˜ ë‚´ìš© í•„ë“œ ì¶”ì¶œ ì¤‘...")
documents = [entry["ë‚´ìš©"] for entry in metadata]

# 3. Mecab ê¸°ë°˜ í† í°í™” (í—ˆìš©ëœ í’ˆì‚¬ë§Œ ë‚¨ê¸°ê³  ë‚˜ë¨¸ì§€ëŠ” ì œì™¸)
allowed_pos_tags = ['NNP', 'NNG', 'NP', 'VV', 'VA', 'VCP', 'VCN', 'VSV', 'MAG', 'MAJ']

def tokenize_with_mecab(text):
    tokens = mecab.pos(text)  # í˜•íƒœì†Œ ë¶„ì„ í›„ í’ˆì‚¬ íƒœê¹…
    filtered_tokens = [word for word, pos in tokens if pos in allowed_pos_tags]  # í—ˆìš©ëœ í’ˆì‚¬ë§Œ ë‚¨ê¸°ê¸°
    return filtered_tokens

print("Mecabìœ¼ë¡œ ë¬¸ì„œ í† í°í™” ì¤‘...")
tokenized_documents = [tokenize_with_mecab(doc) for doc in tqdm(documents, desc="Tokenizing Content")]

# 4. BM25 ì¸ë±ìŠ¤ ìƒì„±
print("BM25 ì¸ë±ìŠ¤ ìƒì„± ì¤‘...")
bm25 = BM25Okapi(tokenized_documents)

# 5. ê¸°ì¡´ FAISS ì¸ë±ìŠ¤ ë¡œë“œ
print("ê¸°ì¡´ FAISS ì¸ë±ìŠ¤ ë¡œë“œ ì¤‘...")
faiss_index = faiss.read_index(faiss_index_path)

# 6. ì •ê·œí™” í•¨ìˆ˜ë“¤
def min_max_normalize(scores):
    min_score = np.min(scores)
    max_score = np.max(scores)
    if max_score == min_score:
        return np.ones_like(scores)
    return (scores - min_score) / (max_score - min_score)

def z_score_normalize(scores):
    mean = np.mean(scores)
    std = np.std(scores)
    if std == 0:
        return np.zeros_like(scores)
    return (scores - mean) / std

def max_normalize(scores):
    max_score = np.max(scores)
    if max_score == 0:
        return np.zeros_like(scores)
    return scores / max_score

# 7. í‚¤ì›Œë“œ ì„œì¹˜ êµ¬í˜„
def keyword_search(query, k=3):
    """
    Mecabìœ¼ë¡œ í† í°í™”ëœ ì¿¼ë¦¬ë¥¼ ê¸°ë°˜ìœ¼ë¡œ BM25ë¥¼ ì‚¬ìš©í•œ í‚¤ì›Œë“œ ì„œì¹˜.
    """
    query_tokens = tokenize_with_mecab(query)  # Mecabìœ¼ë¡œ ì¿¼ë¦¬ í† í°í™”
    bm25_scores = bm25.get_scores(query_tokens)
    sorted_indices = np.argsort(-bm25_scores)[:k]
    results = [(documents[i], metadata[i], bm25_scores[i]) for i in sorted_indices]
    return results

# 8. ì‹œë©˜í‹± ì„œì¹˜ êµ¬í˜„
def semantic_search(query, k=3):
    """
    FAISSë¥¼ ì‚¬ìš©í•œ ì‹œë©˜í‹± ì„œì¹˜.
    """
    query_embedding = model.encode([query], normalize_embeddings=True)
    faiss_scores, faiss_indices = faiss_index.search(query_embedding, k)
    results = [(documents[i], metadata[i], faiss_scores[0][j]) for j, i in enumerate(faiss_indices[0])]
    return results

# 9. í•˜ì´ë¸Œë¦¬ë“œ ì„œì¹˜ êµ¬í˜„
def hybrid_search(query, k=3, alpha=0.5, normalization_method="min_max"):
    """
    ë‹¤ì–‘í•œ ì •ê·œí™”ë¥¼ ì„ íƒí•˜ì—¬ í•˜ì´ë¸Œë¦¬ë“œ ì„œì¹˜ë¥¼ ìˆ˜í–‰í•©ë‹ˆë‹¤.
    """
    query_tokens = tokenize_with_mecab(query)  # Mecabìœ¼ë¡œ ì¿¼ë¦¬ í† í°í™”
    bm25_scores = bm25.get_scores(query_tokens)

    query_embedding = model.encode([query], normalize_embeddings=True)
    faiss_scores, faiss_indices = faiss_index.search(query_embedding, len(documents))
    faiss_scores = faiss_scores[0]

    # ì ìˆ˜ ì •ê·œí™”
    if normalization_method == "min_max":
        bm25_scores = min_max_normalize(bm25_scores)
        faiss_scores = min_max_normalize(faiss_scores)
    elif normalization_method == "z_score":
        bm25_scores = z_score_normalize(bm25_scores)
        faiss_scores = z_score_normalize(faiss_scores)
    elif normalization_method == "max":
        bm25_scores = max_normalize(bm25_scores)
        faiss_scores = max_normalize(faiss_scores)
    else:
        raise ValueError("ì§€ì›í•˜ì§€ ì•ŠëŠ” ì •ê·œí™” ë°©ë²•ì…ë‹ˆë‹¤. 'min_max', 'z_score', 'max' ì¤‘ ì„ íƒí•˜ì„¸ìš”.")

    final_scores = alpha * bm25_scores + (1 - alpha) * faiss_scores
    sorted_indices = np.argsort(-final_scores)[:k]

    results = [(documents[i], metadata[i], final_scores[i]) for i in sorted_indices]
    return results

# 10. ê°€ì¤‘ì¹˜ì™€ ì •ê·œí™” ë°©ì‹ë³„ í•˜ì´ë¸Œë¦¬ë“œ ì„œì¹˜ ì‹¤í–‰
def run_hybrid_search_by_alpha(query, k=3):
    """
    ê° ê°€ì¤‘ì¹˜(alpha)ì™€ ì •ê·œí™” ë°©ì‹ë³„ë¡œ í•˜ì´ë¸Œë¦¬ë“œ ì„œì¹˜ë¥¼ ì‹¤í–‰í•©ë‹ˆë‹¤.
    """
    normalization_methods = ["min_max", "z_score", "max"]
    alphas = [round(i * 0.1, 1) for i in range(1, 10)]  # 0.1 ~ 0.9

    results = []
    for normalization in normalization_methods:
        for alpha in alphas:
            search_results = hybrid_search(query, k=k, alpha=alpha, normalization_method=normalization)
            for rank, (doc, meta, score) in enumerate(search_results, 1):
                results.append({
                    "ì •ê·œí™” ë°©ì‹": normalization,
                    "ê°€ì¤‘ì¹˜ (alpha)": alpha,
                    "ìˆœìœ„": rank,
                    "ì ìˆ˜": score,
                    "ë‚´ìš©": doc[:50],  # ë¬¸ì„œ ë‚´ìš©ì˜ ì¼ë¶€ë§Œ í‘œì‹œ
                    "êµ­ê°€ìœ ì‚°ëª…_êµ­ë¬¸": meta.get("êµ­ê°€ìœ ì‚°ëª…_êµ­ë¬¸", ""),
                    "ì‹œëŒ€": meta.get("ì‹œëŒ€", ""),
                    "ì†Œì¬ì§€ìƒì„¸": meta.get("ì†Œì¬ì§€ìƒì„¸", "")
                })

    return pd.DataFrame(results)

# ì‹¤í–‰: ì‚¬ìš©ì ì§ˆì˜ì™€ ê²°ê³¼ ê³„ì‚°
query = "í•œêµ­ì—ì„œ ê°€ì¥ ì˜¤ë˜ëœ ëª©ì¡° ê±´ì¶•ë¬¼"
top_k = 3

# ëª¨ë¸ ë¡œë“œ
print("SentenceTransformer ëª¨ë¸ ë¡œë“œ ì¤‘...")
model = SentenceTransformer('jhgan/ko-sroberta-multitask')

# í‚¤ì›Œë“œ ì„œì¹˜ ê²°ê³¼
print("\n[í‚¤ì›Œë“œ ì„œì¹˜ ê²°ê³¼] (Mecab ê¸°ë°˜)")
keyword_results = keyword_search(query, k=top_k)
for rank, (doc, meta, score) in enumerate(keyword_results, 1):
    print(f"{rank}. ë‚´ìš©: {doc[:50]}... | ì ìˆ˜: {score:.4f}")

# ì‹œë©˜í‹± ì„œì¹˜ ê²°ê³¼
print("\n[ì‹œë©˜í‹± ì„œì¹˜ ê²°ê³¼]")
semantic_results = semantic_search(query, k=top_k)
for rank, (doc, meta, score) in enumerate(semantic_results, 1):
    print(f"{rank}. ë‚´ìš©: {doc[:50]}... | ì ìˆ˜: {score:.4f}")

# í•˜ì´ë¸Œë¦¬ë“œ ì„œì¹˜ ê²°ê³¼ ê³„ì‚°
print("\ní•˜ì´ë¸Œë¦¬ë“œ ì„œì¹˜ ê²°ê³¼ ê³„ì‚° ì¤‘...")
df_hybrid_results = run_hybrid_search_by_alpha(query, k=top_k)

# ê²°ê³¼ ì¶œë ¥
print("\ní•˜ì´ë¸Œë¦¬ë“œ ì„œì¹˜ ê²°ê³¼ DataFrame: (ìƒìœ„ 20ê°œ)")
print(df_hybrid_results.head(20))  # ìƒìœ„ 20ê°œë§Œ ì¶œë ¥

# ê²°ê³¼ ì €ì¥
output_file = "./hybrid_search_detailed_results.csv"
df_hybrid_results.to_csv(output_file, index=False, encoding="utf-8-sig")
print(f"\nê²°ê³¼ê°€ '{output_file}'ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")


### C:\repository\HAI_Python\app\chatbot\__init__.py ###


### C:\repository\HAI_Python\app\chatbot\archive\chat.py ###
#app/chatbot/chat.py
import os
from dotenv import load_dotenv
from openai import OpenAI
from langchain_core.prompts import PromptTemplate

# .env íŒŒì¼ ë¡œë“œ
load_dotenv()

# OpenAI í´ë¼ì´ì–¸íŠ¸ ì„¤ì •
client = OpenAI(
    api_key=os.environ.get("OPENAI_API_KEY"),  # .env íŒŒì¼ì—ì„œ API í‚¤ ê´€ë¦¬
)

# LangChainì˜ PromptTemplate ì‚¬ìš© - ê°„ê²°í•œ ì‘ë‹µ ì œê³µ ìœ ë„
prompt_template = """ì‚¬ìš©ìì˜ ì§ˆë¬¸ì— ëŒ€í•´ ê°„ë‹¨í•˜ê³  ì •í™•í•œ ë‹µë³€ì„ ì œê³µí•´ì¤˜.

ì§ˆë¬¸: {input}
ë‹µë³€:"""

prompt = PromptTemplate(input_variables=["input"], template=prompt_template)

# ì±—ë´‡ í•¨ìˆ˜ ì •ì˜
def process_chat(input_text: str) -> str:
    # PromptTemplateë¥¼ ì‚¬ìš©í•˜ì—¬ ì‹¤ì œ í”„ë¡¬í”„íŠ¸ ë¬¸ìì—´ ìƒì„±
    prompt_text = prompt.format(input=input_text)
    
    try:
        # OpenAI GPT-3.5 APIë¥¼ ì‚¬ìš©í•˜ì—¬ ì‘ë‹µ ìƒì„± (ìµœì‹  ì¸í„°í˜ì´ìŠ¤ ì‚¬ìš©)
        response = client.chat.completions.create(
            messages=[
                {"role": "user", "content": prompt_text}
            ],
            model="gpt-3.5-turbo",
            max_tokens=150,
            temperature=0.3,
            top_p=0.95,
            n=1
        )
        
        # ì‘ë‹µ í…ìŠ¤íŠ¸ ì¶”ì¶œ
        response_text = response.choices[0].message.content.strip()
    except Exception as e:
        # ì˜ˆì™¸ ë°œìƒ ì‹œ ë””ë²„ê·¸ ë©”ì‹œì§€ ì¶œë ¥ ë° ê¸°ë³¸ ì‘ë‹µ ì„¤ì •
        print(f"Error during OpenAI API call: {e}")
        response_text = "ì£„ì†¡í•©ë‹ˆë‹¤, ì ì ˆí•œ ì‘ë‹µì„ ìƒì„±í•˜ì§€ ëª»í–ˆìŠµë‹ˆë‹¤. ë‹¤ì‹œ í•œë²ˆ ë§ì”€í•´ ì£¼ì„¸ìš”."

    # ì‘ë‹µì˜ ê¸¸ì´ ì œì•½ ì¶”ê°€
    if len(response_text) > 150:
        response_text = response_text[:150] + "..."

    return response_text

# ë””ë²„ê¹… ìš©ë„ - ì‚¬ìš©ì ì…ë ¥ê³¼ ëª¨ë¸ ì‘ë‹µ ì¶œë ¥
if __name__ == "__main__":
    user_input = "FastAPIë€ ë¬´ì—‡ì¸ê°€ìš”?"
    print("Input from User:", user_input)
    print("Response from GPT-3.5:", process_chat(user_input))

### C:\repository\HAI_Python\app\chatbot\archive\check_jhgan.py ###
import os
import faiss
from sentence_transformers import SentenceTransformer
import numpy as np
import pickle
from sklearn.preprocessing import normalize

# 1. ê²½ë¡œ ì„¤ì • ë° ì„ë² ë”© íŒŒì¼ ë¡œë“œ í•¨ìˆ˜
base_dir = os.path.dirname(os.path.realpath(__file__))
embedding_paths = {
    "dotProduct": ("../FAISS/Index/jhgan_dotProduct_index_1000.bin", "../FAISS/Metadata/jhgan_metadata_1000.pkl"),
    "cosine": ("../FAISS/Index/jhgan_cosine_index_1000.bin", "../FAISS/Metadata/jhgan_metadata_1000.pkl"),
    "euclidean": ("../FAISS/Index/jhgan_euclidean_index_1000.bin", "../FAISS/Metadata/jhgan_metadata_1000.pkl")
}

# 2. SentenceTransformer ëª¨ë¸ ë¡œë“œ ë° ì§ˆë¬¸ ì„¤ì •
model = SentenceTransformer('jhgan/ko-sroberta-multitask')  # ëª¨ë¸ì„ jhgan/ko-sroberta-multitaskë¡œ ë³€ê²½
user_question = "íƒœì¡° ì´ì„±ê³„ ì–´ì§„"  # ì‚¬ìš©ìì˜ ì„ì‹œ ì§ˆë¬¸
embedding = model.encode(user_question).astype('float32').reshape(1, -1)

# 3. ê²€ìƒ‰ í•¨ìˆ˜ ì •ì˜
def search_faiss_index(index_file, metadata_file, embedding, normalize_embedding=False):
    # FAISS ì¸ë±ìŠ¤ ë¶ˆëŸ¬ì˜¤ê¸°
    try:
        index = faiss.read_index(index_file)
        print(f"FAISS ì¸ë±ìŠ¤ '{index_file}'ì„(ë¥¼) ì„±ê³µì ìœ¼ë¡œ ë¶ˆëŸ¬ì™”ìŠµë‹ˆë‹¤.")
    except Exception as e:
        print(f"FAISS ì¸ë±ìŠ¤ë¥¼ ë¶ˆëŸ¬ì˜¤ëŠ” ë° ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤: {e}")
        return

    # ë©”íƒ€ë°ì´í„° ë¶ˆëŸ¬ì˜¤ê¸°
    try:
        with open(metadata_file, "rb") as f:
            metadata = pickle.load(f)
        print(f"ë©”íƒ€ë°ì´í„° '{metadata_file}'ì„(ë¥¼) ì„±ê³µì ìœ¼ë¡œ ë¶ˆëŸ¬ì™”ìŠµë‹ˆë‹¤.")
    except Exception as e:
        print(f"ë©”íƒ€ë°ì´í„°ë¥¼ ë¶ˆëŸ¬ì˜¤ëŠ” ë° ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤: {e}")
        return

    # í•„ìš” ì‹œ ì„ë² ë”© ì •ê·œí™” (ì½”ì‚¬ì¸ ìœ ì‚¬ë„)
    if normalize_embedding:
        embedding = normalize(embedding, norm='l2')

    # ìœ ì‚¬ë„ ê²€ìƒ‰
    D, I = index.search(embedding, k=5)
    print(f"ê°€ì¥ ìœ ì‚¬í•œ ë²¡í„°ë“¤ì˜ ì¸ë±ìŠ¤: {I}")
    print(f"ê° ìœ ì‚¬í•œ ë²¡í„°ì™€ì˜ ìœ ì‚¬ë„/ê±°ë¦¬: {D}")

    # ê²€ìƒ‰ ê²°ê³¼ì™€ ë©”íƒ€ë°ì´í„° ë§¤í•‘
    for i, idx in enumerate(I[0]):
        if idx < len(metadata):
            data = metadata[idx]
            distance = D[0][i]  # ê° ë¬¸ì„œì™€ì˜ ê±°ë¦¬ ê°’
            if isinstance(data, dict):
                print(f"ì›ë³¸ ë°ì´í„° ID: {data['original_id']}, ì„¸ê·¸ë¨¼íŠ¸ ë²ˆí˜¸: {data['segment_id']}, "
                      f"í…ìŠ¤íŠ¸ ì„¸ê·¸ë¨¼íŠ¸: {data['text_segment']}, ê±°ë¦¬: {distance}")
            else:
                print(f"ì¸ë±ìŠ¤ {idx}ì˜ ë©”íƒ€ë°ì´í„° í˜•ì‹ì´ ì˜ëª»ë˜ì—ˆìŠµë‹ˆë‹¤. ë°ì´í„°: {data}")
        else:
            print(f"ì¸ë±ìŠ¤ {idx}ëŠ” ë©”íƒ€ë°ì´í„° ë²”ìœ„ë¥¼ ë²—ì–´ë‚¬ìŠµë‹ˆë‹¤.")

# 4. ê° ë°©ì‹ì— ë”°ë¥¸ ê²€ìƒ‰ ì‹¤í–‰
print("ë‚´ì  ë°©ì‹ìœ¼ë¡œ ê²€ìƒ‰")
search_faiss_index(embedding_paths["dotProduct"][0], embedding_paths["dotProduct"][1], embedding)

print("\nì½”ì‚¬ì¸ ìœ ì‚¬ë„ ë°©ì‹ìœ¼ë¡œ ê²€ìƒ‰")
search_faiss_index(embedding_paths["cosine"][0], embedding_paths["cosine"][1], embedding, normalize_embedding=True)

print("\nìœ í´ë¦¬ë“œ ê±°ë¦¬ ë°©ì‹ìœ¼ë¡œ ê²€ìƒ‰")
search_faiss_index(embedding_paths["euclidean"][0], embedding_paths["euclidean"][1], embedding)

### C:\repository\HAI_Python\app\chatbot\archive\check_rag cosine.py ###
import os
import faiss
from sentence_transformers import SentenceTransformer
import numpy as np
import pickle
from sklearn.preprocessing import normalize

# 1. FAISS ì¸ë±ìŠ¤ì™€ ë©”íƒ€ë°ì´í„° íŒŒì¼ ê²½ë¡œ ì„¤ì •
base_dir = os.path.dirname(os.path.realpath(__file__))  # í˜„ì¬ íŒŒì¼ì˜ ê²½ë¡œë¥¼ ê°€ì ¸ì˜µë‹ˆë‹¤.
index_file = os.path.join(base_dir, "../FAISS/Index/faiss_index_full_cosine.bin")
metadata_file = os.path.join(base_dir, "../FAISS/Metadata/faiss_metadata_full_cosine.pkl")

# 2. FAISS ì¸ë±ìŠ¤ ë¶ˆëŸ¬ì˜¤ê¸° (ì½”ì‚¬ì¸ ìœ ì‚¬ë„ë¥¼ ì‚¬ìš©í•˜ê¸° ìœ„í•´ IndexFlatIPë¡œ ìƒì„±)
try:
    # ê¸°ì¡´ ì¸ë±ìŠ¤ë¥¼ íŒŒì¼ ê²½ë¡œì—ì„œ ë¶ˆëŸ¬ì˜µë‹ˆë‹¤.
    index = faiss.read_index(index_file)
    print(f"FAISS ì¸ë±ìŠ¤ '{index_file}'ì„(ë¥¼) ì„±ê³µì ìœ¼ë¡œ ë¶ˆëŸ¬ì™”ìŠµë‹ˆë‹¤.")
except Exception as e:
    print(f"FAISS ì¸ë±ìŠ¤ë¥¼ ë¶ˆëŸ¬ì˜¤ëŠ” ë° ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤: {e}")
    exit()

# 3. ë©”íƒ€ë°ì´í„° ë¶ˆëŸ¬ì˜¤ê¸°
try:
    with open(metadata_file, "rb") as f:
        metadata = pickle.load(f)
    print(f"ë©”íƒ€ë°ì´í„° '{metadata_file}'ì„(ë¥¼) ì„±ê³µì ìœ¼ë¡œ ë¶ˆëŸ¬ì™”ìŠµë‹ˆë‹¤.")
except Exception as e:
    print(f"ë©”íƒ€ë°ì´í„°ë¥¼ ë¶ˆëŸ¬ì˜¤ëŠ” ë° ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤: {e}")
    exit()

# 4. ì‚¬ìš©ìì˜ ì„ì‹œ ì§ˆë¬¸ì„ ì…ë ¥ë°›ì•„ ì„ë² ë”©
model = SentenceTransformer('sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2')
user_question = "ê²½ë³µê¶ì´ ì–´ë–»ê²Œ ë§Œë“¤ì–´ì¡Œì–´?"  # ì‚¬ìš©ìì˜ ì„ì‹œ ì§ˆë¬¸
embedding = model.encode(user_question).astype('float32').reshape(1, -1)
embedding = normalize(embedding, norm='l2')  # ë²¡í„°ë¥¼ L2 ì •ê·œí™”í•˜ì—¬ ì½”ì‚¬ì¸ ìœ ì‚¬ë„ì— ë§ì¶¤

# 5. FAISS ì¸ë±ìŠ¤ì—ì„œ ìœ ì‚¬í•œ ë²¡í„° ê²€ìƒ‰
D, I = index.search(embedding, k=5)  # ê°€ì¥ ìœ ì‚¬í•œ 5ê°œ ê²€ìƒ‰ (ë‚´ì  ì‚¬ìš©)

print(f"ê°€ì¥ ìœ ì‚¬í•œ ë²¡í„°ë“¤ì˜ ì¸ë±ìŠ¤: {I}")
print(f"ê° ìœ ì‚¬í•œ ë²¡í„°ì™€ì˜ ìœ ì‚¬ë„: {D}")

# 6. ê²€ìƒ‰ ê²°ê³¼ì™€ ë©”íƒ€ë°ì´í„° ë§¤í•‘
for idx in I[0]:
    if idx < len(metadata):  # ë©”íƒ€ë°ì´í„°ì˜ ì¸ë±ìŠ¤ ë²”ìœ„ í™•ì¸
        data = metadata[idx]
        # ë©”íƒ€ë°ì´í„°ê°€ ë”•ì…”ë„ˆë¦¬ì¸ì§€ í™•ì¸
        if isinstance(data, dict):
            print(f"ì›ë³¸ ë°ì´í„° ID: {data['original_id']}, ì„¸ê·¸ë¨¼íŠ¸ ë²ˆí˜¸: {data['segment_id']}, í…ìŠ¤íŠ¸ ì„¸ê·¸ë¨¼íŠ¸: {data['text_segment']}")
        else:
            print(f"ì¸ë±ìŠ¤ {idx}ì˜ ë©”íƒ€ë°ì´í„° í˜•ì‹ì´ ì˜ëª»ë˜ì—ˆìŠµë‹ˆë‹¤. ë°ì´í„°: {data}")
    else:
        print(f"ì¸ë±ìŠ¤ {idx}ëŠ” ë©”íƒ€ë°ì´í„° ë²”ìœ„ë¥¼ ë²—ì–´ë‚¬ìŠµë‹ˆë‹¤.")


### C:\repository\HAI_Python\app\chatbot\archive\check_rag dotProduct.py ###
import os
import faiss
from sentence_transformers import SentenceTransformer
import numpy as np
import pickle

# 1. FAISS ì¸ë±ìŠ¤ì™€ ë©”íƒ€ë°ì´í„° íŒŒì¼ ê²½ë¡œ ì„¤ì •
base_dir = os.path.dirname(os.path.realpath(__file__))  # í˜„ì¬ íŒŒì¼ì˜ ê²½ë¡œë¥¼ ê°€ì ¸ì˜µë‹ˆë‹¤.
index_file = os.path.join(base_dir, "../FAISS/Index/faiss_index_full_dotProduct.bin")
metadata_file = os.path.join(base_dir, "../FAISS/Metadata/faiss_metadata_full_dotProduct.pkl")

# 2. FAISS ì¸ë±ìŠ¤ ë¶ˆëŸ¬ì˜¤ê¸° (ë‚´ì ì„ ì‚¬ìš©í•˜ê¸° ìœ„í•´ IndexFlatIPë¡œ ìƒì„±)
try:
    # ê¸°ì¡´ ì¸ë±ìŠ¤ë¥¼ íŒŒì¼ ê²½ë¡œì—ì„œ ë¶ˆëŸ¬ì˜µë‹ˆë‹¤.
    index = faiss.read_index(index_file)
    print(f"FAISS ì¸ë±ìŠ¤ '{index_file}'ì„(ë¥¼) ì„±ê³µì ìœ¼ë¡œ ë¶ˆëŸ¬ì™”ìŠµë‹ˆë‹¤.")
except Exception as e:
    print(f"FAISS ì¸ë±ìŠ¤ë¥¼ ë¶ˆëŸ¬ì˜¤ëŠ” ë° ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤: {e}")
    exit()

# 3. ë©”íƒ€ë°ì´í„° ë¶ˆëŸ¬ì˜¤ê¸°
try:
    with open(metadata_file, "rb") as f:
        metadata = pickle.load(f)
    print(f"ë©”íƒ€ë°ì´í„° '{metadata_file}'ì„(ë¥¼) ì„±ê³µì ìœ¼ë¡œ ë¶ˆëŸ¬ì™”ìŠµë‹ˆë‹¤.")
except Exception as e:
    print(f"ë©”íƒ€ë°ì´í„°ë¥¼ ë¶ˆëŸ¬ì˜¤ëŠ” ë° ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤: {e}")
    exit()

# 4. ì‚¬ìš©ìì˜ ì„ì‹œ ì§ˆë¬¸ì„ ì…ë ¥ë°›ì•„ ì„ë² ë”©
model = SentenceTransformer('sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2')
user_question = "ê²½ë³µê¶ì´ ì–´ë–»ê²Œ ë§Œë“¤ì–´ì¡Œì–´?"  # ì‚¬ìš©ìì˜ ì„ì‹œ ì§ˆë¬¸
embedding = model.encode(user_question).astype('float32').reshape(1, -1)
# ë‚´ì  ë°©ì‹ì—ì„œëŠ” ì •ê·œí™”ë¥¼ í•˜ì§€ ì•ŠìŒ

# 5. FAISS ì¸ë±ìŠ¤ì—ì„œ ìœ ì‚¬í•œ ë²¡í„° ê²€ìƒ‰
D, I = index.search(embedding, k=5)  # ê°€ì¥ ìœ ì‚¬í•œ 5ê°œ ê²€ìƒ‰ (ë‚´ì  ì‚¬ìš©)

print(f"ê°€ì¥ ìœ ì‚¬í•œ ë²¡í„°ë“¤ì˜ ì¸ë±ìŠ¤: {I}")
print(f"ê° ìœ ì‚¬í•œ ë²¡í„°ì™€ì˜ ìœ ì‚¬ë„: {D}")

# 6. ê²€ìƒ‰ ê²°ê³¼ì™€ ë©”íƒ€ë°ì´í„° ë§¤í•‘
for idx in I[0]:
    if idx < len(metadata):  # ë©”íƒ€ë°ì´í„°ì˜ ì¸ë±ìŠ¤ ë²”ìœ„ í™•ì¸
        data = metadata[idx]
        # ë©”íƒ€ë°ì´í„°ê°€ ë”•ì…”ë„ˆë¦¬ì¸ì§€ í™•ì¸
        if isinstance(data, dict):
            print(f"ì›ë³¸ ë°ì´í„° ID: {data['original_id']}, ì„¸ê·¸ë¨¼íŠ¸ ë²ˆí˜¸: {data['segment_id']}, í…ìŠ¤íŠ¸ ì„¸ê·¸ë¨¼íŠ¸: {data['text_segment']}")
        else:
            print(f"ì¸ë±ìŠ¤ {idx}ì˜ ë©”íƒ€ë°ì´í„° í˜•ì‹ì´ ì˜ëª»ë˜ì—ˆìŠµë‹ˆë‹¤. ë°ì´í„°: {data}")
    else:
        print(f"ì¸ë±ìŠ¤ {idx}ëŠ” ë©”íƒ€ë°ì´í„° ë²”ìœ„ë¥¼ ë²—ì–´ë‚¬ìŠµë‹ˆë‹¤.")


### C:\repository\HAI_Python\app\chatbot\archive\check_rag_uclidian.py ###
import os
import faiss
from sentence_transformers import SentenceTransformer
import numpy as np
import pickle
from sklearn.preprocessing import normalize

# 1. FAISS ì¸ë±ìŠ¤ì™€ ë©”íƒ€ë°ì´í„° íŒŒì¼ ê²½ë¡œ ì„¤ì •
base_dir = os.path.dirname(os.path.realpath(__file__))  # í˜„ì¬ íŒŒì¼ì˜ ê²½ë¡œë¥¼ ê°€ì ¸ì˜µë‹ˆë‹¤.
index_file = os.path.join(base_dir, "../FAISS/Index/faiss_index_full_uclid.bin")
metadata_file = os.path.join(base_dir, "../FAISS/Metadata/faiss_metadata_full_uclid.pkl")

# 2. FAISS ì¸ë±ìŠ¤ ë¶ˆëŸ¬ì˜¤ê¸° (ìœ í´ë¦¬ë””ì•ˆ ê±°ë¦¬ë¥¼ ì‚¬ìš©í•˜ê¸° ìœ„í•´ IndexFlatIPë¡œ ìƒì„±)
try:
    index = faiss.read_index(index_file)
    print(f"FAISS ì¸ë±ìŠ¤ '{index_file}'ì„(ë¥¼) ì„±ê³µì ìœ¼ë¡œ ë¶ˆëŸ¬ì™”ìŠµë‹ˆë‹¤.")
except Exception as e:
    print(f"FAISS ì¸ë±ìŠ¤ë¥¼ ë¶ˆëŸ¬ì˜¤ëŠ” ë° ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤: {e}")
    exit()

# 3. ë©”íƒ€ë°ì´í„° ë¶ˆëŸ¬ì˜¤ê¸°
try:
    with open(metadata_file, "rb") as f:
        metadata = pickle.load(f)
    print(f"ë©”íƒ€ë°ì´í„° '{metadata_file}'ì„(ë¥¼) ì„±ê³µì ìœ¼ë¡œ ë¶ˆëŸ¬ì™”ìŠµë‹ˆë‹¤.")
except Exception as e:
    print(f"ë©”íƒ€ë°ì´í„°ë¥¼ ë¶ˆëŸ¬ì˜¤ëŠ” ë° ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤: {e}")
    exit()

# 4. ì‚¬ìš©ìì˜ ì„ì‹œ ì§ˆë¬¸ì„ ì…ë ¥ë°›ì•„ ì„ë² ë”©
model = SentenceTransformer('sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2')
user_question = "ê²½ë³µê¶ì´ ì–´ë–»ê²Œ ë§Œë“¤ì–´ì¡Œì–´?"  # ì‚¬ìš©ìì˜ ì„ì‹œ ì§ˆë¬¸
embedding = model.encode(user_question).astype('float32').reshape(1, -1)
embedding = normalize(embedding, norm='l2')
# 5. FAISS ì¸ë±ìŠ¤ì—ì„œ ìœ ì‚¬í•œ ë²¡í„° ê²€ìƒ‰
D, I = index.search(embedding, k=5)  # ê°€ì¥ ìœ ì‚¬í•œ 5ê°œ ê²€ìƒ‰

print(f"ê°€ì¥ ìœ ì‚¬í•œ ë²¡í„°ë“¤ì˜ ì¸ë±ìŠ¤: {I}")
print(f"ê° ìœ ì‚¬í•œ ë²¡í„°ì™€ì˜ ê±°ë¦¬: {D}")

# 6. ê²€ìƒ‰ ê²°ê³¼ì™€ ë©”íƒ€ë°ì´í„° ë§¤í•‘
for idx in I[0]:
    if idx < len(metadata):  # ë©”íƒ€ë°ì´í„°ì˜ ì¸ë±ìŠ¤ ë²”ìœ„ í™•ì¸
        data = metadata[idx]
        # ë©”íƒ€ë°ì´í„°ê°€ ë”•ì…”ë„ˆë¦¬ì¸ì§€ í™•ì¸
        if isinstance(data, dict):
            print(f"ì›ë³¸ ë°ì´í„° ID: {data['original_id']}, ì„¸ê·¸ë¨¼íŠ¸ ë²ˆí˜¸: {data['segment_id']}, í…ìŠ¤íŠ¸ ì„¸ê·¸ë¨¼íŠ¸: {data['text_segment']}")
        else:
            print(f"ì¸ë±ìŠ¤ {idx}ì˜ ë©”íƒ€ë°ì´í„° í˜•ì‹ì´ ì˜ëª»ë˜ì—ˆìŠµë‹ˆë‹¤. ë°ì´í„°: {data}")
    else:
        print(f"ì¸ë±ìŠ¤ {idx}ëŠ” ë©”íƒ€ë°ì´í„° ë²”ìœ„ë¥¼ ë²—ì–´ë‚¬ìŠµë‹ˆë‹¤.")


### C:\repository\HAI_Python\app\chatbot\archive\check_snunlp.py ###
import os
import faiss
from sentence_transformers import SentenceTransformer
import numpy as np
import pickle
from sklearn.preprocessing import normalize

# 1. ê²½ë¡œ ì„¤ì • ë° ì„ë² ë”© íŒŒì¼ ë¡œë“œ í•¨ìˆ˜
base_dir = os.path.dirname(os.path.realpath(__file__))
embedding_paths = {
    "dotProduct": ("../FAISS/snunlp_dotProduct_index_1000.bin", "../FAISS/snunlp_metadata_1000.pkl"),
    "cosine": ("../FAISS/snunlp_cosine_index_1000.bin", "../FAISS/snunlp_metadata_1000.pkl"),
    "euclidean": ("../FAISS/snunlp_euclidean_index_1000.bin", "../FAISS/snunlp_metadata_1000.pkl")
}

# 2. SentenceTransformer ëª¨ë¸ ë¡œë“œ ë° ì§ˆë¬¸ ì„¤ì •
model = SentenceTransformer('snunlp/KR-SBERT-V40K-klueNLI-augSTS')
user_question = "ê²½ë³µê¶ì´ ì–´ë–»ê²Œ ë§Œë“¤ì–´ì¡Œì–´?"  # ì‚¬ìš©ìì˜ ì„ì‹œ ì§ˆë¬¸
embedding = model.encode(user_question).astype('float32').reshape(1, -1)

# 3. ê²€ìƒ‰ í•¨ìˆ˜ ì •ì˜
def search_faiss_index(index_file, metadata_file, embedding, normalize_embedding=False):
    # FAISS ì¸ë±ìŠ¤ ë¶ˆëŸ¬ì˜¤ê¸°
    try:
        index = faiss.read_index(index_file)
        print(f"FAISS ì¸ë±ìŠ¤ '{index_file}'ì„(ë¥¼) ì„±ê³µì ìœ¼ë¡œ ë¶ˆëŸ¬ì™”ìŠµë‹ˆë‹¤.")
    except Exception as e:
        print(f"FAISS ì¸ë±ìŠ¤ë¥¼ ë¶ˆëŸ¬ì˜¤ëŠ” ë° ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤: {e}")
        return

    # ë©”íƒ€ë°ì´í„° ë¶ˆëŸ¬ì˜¤ê¸°
    try:
        with open(metadata_file, "rb") as f:
            metadata = pickle.load(f)
        print(f"ë©”íƒ€ë°ì´í„° '{metadata_file}'ì„(ë¥¼) ì„±ê³µì ìœ¼ë¡œ ë¶ˆëŸ¬ì™”ìŠµë‹ˆë‹¤.")
    except Exception as e:
        print(f"ë©”íƒ€ë°ì´í„°ë¥¼ ë¶ˆëŸ¬ì˜¤ëŠ” ë° ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤: {e}")
        return

    # í•„ìš” ì‹œ ì„ë² ë”© ì •ê·œí™” (ì½”ì‚¬ì¸ ìœ ì‚¬ë„)
    if normalize_embedding:
        embedding = normalize(embedding, norm='l2')

    # ìœ ì‚¬ë„ ê²€ìƒ‰
    D, I = index.search(embedding, k=5)
    print(f"ê°€ì¥ ìœ ì‚¬í•œ ë²¡í„°ë“¤ì˜ ì¸ë±ìŠ¤: {I}")
    print(f"ê° ìœ ì‚¬í•œ ë²¡í„°ì™€ì˜ ìœ ì‚¬ë„: {D}")

    # ê²€ìƒ‰ ê²°ê³¼ì™€ ë©”íƒ€ë°ì´í„° ë§¤í•‘
    for idx in I[0]:
        if idx < len(metadata):
            data = metadata[idx]
            if isinstance(data, dict):
                print(f"ì›ë³¸ ë°ì´í„° ID: {data['original_id']}, ì„¸ê·¸ë¨¼íŠ¸ ë²ˆí˜¸: {data['segment_id']}, í…ìŠ¤íŠ¸ ì„¸ê·¸ë¨¼íŠ¸: {data['text_segment']}")
            else:
                print(f"ì¸ë±ìŠ¤ {idx}ì˜ ë©”íƒ€ë°ì´í„° í˜•ì‹ì´ ì˜ëª»ë˜ì—ˆìŠµë‹ˆë‹¤. ë°ì´í„°: {data}")
        else:
            print(f"ì¸ë±ìŠ¤ {idx}ëŠ” ë©”íƒ€ë°ì´í„° ë²”ìœ„ë¥¼ ë²—ì–´ë‚¬ìŠµë‹ˆë‹¤.")

# 4. ê° ë°©ì‹ì— ë”°ë¥¸ ê²€ìƒ‰ ì‹¤í–‰
print("ë‚´ì  ë°©ì‹ìœ¼ë¡œ ê²€ìƒ‰")
search_faiss_index(embedding_paths["dotProduct"][0], embedding_paths["dotProduct"][1], embedding)

print("\nì½”ì‚¬ì¸ ìœ ì‚¬ë„ ë°©ì‹ìœ¼ë¡œ ê²€ìƒ‰")
search_faiss_index(embedding_paths["cosine"][0], embedding_paths["cosine"][1], embedding, normalize_embedding=True)

print("\nìœ í´ë¦¬ë“œ ê±°ë¦¬ ë°©ì‹ìœ¼ë¡œ ê²€ìƒ‰")
search_faiss_index(embedding_paths["euclidean"][0], embedding_paths["euclidean"][1], embedding)


### C:\repository\HAI_Python\app\chatbot\archive\etc.py ###
# from rank_bm25 import BM25Okapi
# from konlpy.tag import Okt

# # Okt í† í¬ë‚˜ì´ì € ì´ˆê¸°í™”
# okt = Okt()

# # ë¬¸ì„œ ë¦¬ìŠ¤íŠ¸ ì˜ˆì‹œ
# documents = ["í•œêµ­ì—ì„œ ê°€ì¥ ì˜¤ë˜ëœ ëª©ì¡° ê±´ì¶•ë¬¼ì€ ë¬´ì—‡ì¸ê°€ìš”?", 
#              "ì„œìš¸ ìˆ­ë¡€ë¬¸ì€ 1398ë…„ì— ê±´ë¦½ëœ ëŒ€í‘œì ì¸ ë¬¸í™”ì¬ì…ë‹ˆë‹¤."]

# # ë¬¸ì„œë“¤ì— ëŒ€í•´ Mecabìœ¼ë¡œ í† í°í™”
# tokenized_documents = [okt.morphs(doc) for doc in documents]

# # BM25 ì¸ë±ìŠ¤ ìƒì„±
# bm25 = BM25Okapi(tokenized_documents)

# # ì˜ˆì‹œ ì¿¼ë¦¬
# query = "í•œêµ­ì—ì„œ ê°€ì¥ ì˜¤ë˜ëœ ê±´ì¶•ë¬¼"

# # ì¿¼ë¦¬ë¥¼ Mecabìœ¼ë¡œ í† í°í™”
# query_tokens = okt.morphs(query)

# # BM25 ì ìˆ˜ ê³„ì‚°
# bm25_scores = bm25.get_scores(query_tokens)

# # ê²°ê³¼ ì¶œë ¥
# sorted_indices = np.argsort(-bm25_scores)[:3]
# for i in sorted_indices:
#     print(f"ë¬¸ì„œ: {documents[i]} | ì ìˆ˜: {bm25_scores[i]:.4f}")

# from eunjeon import Mecab

# # Mecab í˜•íƒœì†Œ ë¶„ì„ê¸° ì´ˆê¸°í™”
# mecab = Mecab()

# # í† í°í™” í•¨ìˆ˜ (í—ˆìš©í•  í’ˆì‚¬ë§Œ ë‚¨ê¸°ê³  ë‚˜ë¨¸ì§€ëŠ” ì œì™¸)
# def tokenize_with_mecab(text):
#     tokens = mecab.pos(text)  # í˜•íƒœì†Œ ë¶„ì„ í›„ í’ˆì‚¬ íƒœê¹…
#     # í—ˆìš©í•  í’ˆì‚¬: NNP, NNG, NP, VV, VA, VCP, VCN, VSV, MAG, MAJ
#     allowed_pos_tags = ['NNP', 'NNG', 'NP', 'VV', 'VA', 'VCP', 'VCN', 'VSV', 'MAG', 'MAJ']
#     filtered_tokens = [word for word, pos in tokens if pos in allowed_pos_tags]
#     return filtered_tokens

# # ì˜ˆì œ
# text = "í•œêµ­ì—ì„œ ê°€ì¥ ì˜¤ë˜ëœ ëª©ì¡° ê±´ì¶•ë¬¼ì€ ë¬´ì—‡ì¸ê°€ìš”?"
# tokens = tokenize_with_mecab(text)
# print(f"Mecabìœ¼ë¡œ í’ˆì‚¬ ê¸°ë°˜ í† í°í™” ê²°ê³¼ : {tokens}")

# from gensim.models import KeyedVectors
# from huggingface_hub import hf_hub_download
# from eunjeon import Mecab

# # 1. Mecab í˜•íƒœì†Œ ë¶„ì„ê¸° ì´ˆê¸°í™”
# mecab = Mecab()

# # 2. Hugging Faceì—ì„œ nlpl_55 ëª¨ë¸ ë‹¤ìš´ë¡œë“œ ë° ë¡œë“œ
# model_path = hf_hub_download(repo_id="Word2vec/nlpl_55", filename="model.bin")
# model = KeyedVectors.load_word2vec_format(model_path, binary=True, unicode_errors="ignore")

# # 3. ë™ì˜ì–´ ì¶”ì¶œ í•¨ìˆ˜
# def get_synonyms(word, model, topn=5):
#     """
#     ì£¼ì–´ì§„ ë‹¨ì–´ì— ëŒ€í•´ Word2Vec ëª¨ë¸ì„ ì‚¬ìš©í•˜ì—¬ ê°€ì¥ ìœ ì‚¬í•œ ë‹¨ì–´ë¥¼ ì¶”ì¶œí•©ë‹ˆë‹¤.
#     """
#     try:
#         synonyms = model.most_similar(word, topn=topn)
#         return [synonym[0] for synonym in synonyms]
#     except KeyError:
#         return []  # ëª¨ë¸ì— ë‹¨ì–´ê°€ ì—†ì„ ê²½ìš° ë¹ˆ ë¦¬ìŠ¤íŠ¸ ë°˜í™˜

# # 4. í’ˆì‚¬ ê¸°ë°˜ìœ¼ë¡œ í† í°í™” ë° ë™ì˜ì–´ í™•ì¥
# def tokenize_with_mecab(text):
#     tokens = mecab.pos(text)  # í˜•íƒœì†Œ ë¶„ì„ í›„ í’ˆì‚¬ íƒœê¹…
#     # í—ˆìš©í•  í’ˆì‚¬: NNP, NNG, NP, VV, VA, VCP, VCN, VSV, MAG, MAJ
#     allowed_pos_tags = ['NNP', 'NNG', 'NP', 'VV', 'VA', 'VCP', 'VCN', 'VSV', 'MAG', 'MAJ']
#     filtered_tokens = [word for word, pos in tokens if pos in allowed_pos_tags]
#     return filtered_tokens

# def expand_synonyms(text, model):
#     """
#     ë¬¸ì¥ì„ Mecabìœ¼ë¡œ í† í°í™”í•˜ê³ , ê° í† í°ì— ëŒ€í•´ ë™ì˜ì–´ë¥¼ í™•ì¥í•©ë‹ˆë‹¤.
#     """
#     # Mecabìœ¼ë¡œ í† í°í™”
#     tokens = tokenize_with_mecab(text)

#     expanded_tokens = []
#     original_tokens = []

#     # ê° í† í°ì— ëŒ€í•´ ë™ì˜ì–´ë¥¼ í™•ì¥
#     for token in tokens:
#         original_tokens.append(token)
#         synonyms = get_synonyms(token, model)
#         if synonyms:
#             expanded_tokens.append(synonyms)
#         else:
#             expanded_tokens.append([token])  # ë™ì˜ì–´ê°€ ì—†ìœ¼ë©´ ì›ë³¸ ë‹¨ì–´ ê·¸ëŒ€ë¡œ

#     return original_tokens, expanded_tokens

# # 5. í…ŒìŠ¤íŠ¸
# text = "í•œêµ­ì—ì„œ ê°€ì¥ ì˜¤ë˜ëœ ëª©ì¡° ê±´ì¶•ë¬¼ì€ ë¬´ì—‡ì¸ê°€ìš”?"
# original_tokens, expanded_tokens = expand_synonyms(text, model)

# print("ì›ë³¸ ë¬¸ì¥:", text)
# print("\nì›ë˜ì˜ í† í°ê³¼ ë™ì˜ì–´ í™•ì¥ ê²°ê³¼:")
# for original, expanded in zip(original_tokens, expanded_tokens):
#     print(f"ì›ë³¸: {original} -> ë™ì˜ì–´: {expanded}")

# from konlpy.tag import Okt

# # Okt í˜•íƒœì†Œ ë¶„ì„ê¸° ì´ˆê¸°í™”
# okt = Okt()

# # í† í°í™” ë° í’ˆì‚¬ íƒœê¹…
# def tokenize_with_okt(text):
#     # í’ˆì‚¬ íƒœê¹…
#     tokens = okt.pos(text)
#     # í—ˆìš©í•  í’ˆì‚¬: ëª…ì‚¬, ë™ì‚¬, í˜•ìš©ì‚¬ ë“±
#     allowed_pos_tags = ['Noun', 'Verb', 'Adjective']
#     filtered_tokens = [word for word, pos in tokens if pos in allowed_pos_tags]
#     return filtered_tokens

# # ì˜ˆì œ
# text = "í•œêµ­ì—ì„œ ê°€ì¥ ì˜¤ë˜ëœ ëª©ì¡° ê±´ì¶•ë¬¼ì€ ë¬´ì—‡ì¸ê°€ìš”?"
# tokens = tokenize_with_okt(text)
# print(f"OkTìœ¼ë¡œ í’ˆì‚¬ ê¸°ë°˜ í† í°í™” ê²°ê³¼ : {tokens}")


from PyDictionary import PyDictionary

# PyDictionary ì´ˆê¸°í™”
dictionary = PyDictionary()

# ì˜ˆì‹œ ë‹¨ì–´
word = "ê°€ì¥"

# ë™ì˜ì–´ ì°¾ê¸°
synonyms = dictionary.synonym(word)

# ê²°ê³¼ ì¶œë ¥
print(f"{word}ì˜ ë™ì˜ì–´ëŠ”: {synonyms}")


### C:\repository\HAI_Python\app\faiss\embedding.py ###
# embedding.py
import os
import psycopg2
import re
import kss
import faiss
import pickle
import numpy as np
from tqdm import tqdm
from sentence_transformers import SentenceTransformer
from sklearn.preprocessing import normalize

# 1. PostgreSQLì—ì„œ ëª¨ë“  ë°ì´í„° ê°€ì ¸ì˜¤ê¸°
try:
    conn = psycopg2.connect(
        host="localhost",
        database="heritage_db",
        user="postgres",
        password="iam@123"
    )
    cursor = conn.cursor()
    cursor.execute("""
        SELECT id, ccbaMnm1, ccbaMnm2, ccmaName, ccbaCtcdNm, ccsiName, ccceName, content, ccbaLcad
        FROM national_heritage
    """)  
    rows = cursor.fetchall()
    conn.close()
except Exception as e:
    print("PostgreSQL ì—°ê²° ì‹¤íŒ¨:", e)
    exit()

# 2. ëª¨ë¸ ë° í† í¬ë‚˜ì´ì € ë¡œë“œ
model = SentenceTransformer('jhgan/ko-sroberta-multitask')
embeddings = []
metadata = []

# 3. ë°ì´í„° ì „ì²˜ë¦¬ ë° ìŠ¬ë¼ì´ë”© ìœˆë„ìš° ë°©ì‹ ì ìš©
max_tokens = 512
stride = 256
context_overlap = 2  # ë¬¸ë§¥ ë³´ì¡´ì„ ìœ„í•´ ì¤‘ì²©í•  ë¬¸ì¥ ìˆ˜

for row in tqdm(rows, desc="Processing rows"):
    primary_id, ccbaMnm1, ccbaMnm2, ccmaName, ccbaCtcdNm, ccsiName, ccceName, content, ccbaLcad = row

    # ëª¨ë“  ì •ë³´ë¥¼ í•˜ë‚˜ì˜ í…ìŠ¤íŠ¸ë¡œ ê²°í•©
    combined_text = f"êµ­ê°€ìœ ì‚°ëª…_êµ­ë¬¸: {ccbaMnm1}, êµ­ê°€ìœ ì‚°ëª…_í•œì: {ccbaMnm2}, êµ­ê°€ìœ ì‚°ì¢…ëª©: {ccmaName}, ì‹œë„ëª…: {ccbaCtcdNm}, ì‹œêµ°êµ¬ëª…: {ccsiName}, ì‹œëŒ€: {ccceName}, ë‚´ìš©: {content}, ì†Œì¬ì§€ìƒì„¸: {ccbaLcad}"

    # ë¬¸ì¥ ë‹¨ìœ„ë¡œ ë¶„ë¦¬ (KSS ì‚¬ìš©)
    sentences = kss.split_sentences(combined_text)

    # ê° ë¬¸ì¥ì—ì„œ êµ¬ë‘ì  ì œê±°
    processed_sentences = [re.sub(r'[\.,!?]', '', sentence) for sentence in sentences]

    segment_id = 0
    i = 0
    while i < len(processed_sentences):
        # í˜„ì¬ ìŠ¬ë¼ì´ë”© ìœˆë„ìš° ë²”ìœ„ì˜ ë¬¸ì¥ë“¤
        current_sentences = processed_sentences[i:i + stride]

        # ì´ì „ ë¬¸ì¥ ì¼ë¶€ë¥¼ í¬í•¨í•´ ë¬¸ë§¥ ë³´ì¡´
        if i > 0:
            overlap_sentences = processed_sentences[max(0, i - context_overlap):i]
            current_sentences = overlap_sentences + current_sentences

        # ë¬¸ì¥ë“¤ì„ í•˜ë‚˜ì˜ í…ìŠ¤íŠ¸ë¡œ ê²°í•©
        current_text = " ".join(current_sentences)

        # ì„ë² ë”© ìƒì„±
        embedding = model.encode(current_text)
        embeddings.append(embedding)

        metadata_entry = {
            "primary_id": primary_id,
            "êµ­ê°€ìœ ì‚°ëª…_êµ­ë¬¸": ccbaMnm1,
            "êµ­ê°€ìœ ì‚°ëª…_í•œì": ccbaMnm2,
            "êµ­ê°€ìœ ì‚°ì¢…ëª©": ccmaName,
            "ì‹œë„ëª…": ccbaCtcdNm,
            "ì‹œêµ°êµ¬ëª…": ccsiName,
            "ì‹œëŒ€": ccceName,
            "ë‚´ìš©": current_text,
            "ì†Œì¬ì§€ìƒì„¸": ccbaLcad,
            "segment_id": segment_id  # ìŠ¬ë¼ì´ë”© ìœˆë„ìš° êµ¬ê°„
        }
        metadata.append(metadata_entry)
        segment_id += 1

        # stride ë§Œí¼ ì´ë™
        i += stride - context_overlap

# FAISS ì¸ë±ìŠ¤ ë° ë©”íƒ€ë°ì´í„° ì €ì¥ ê²½ë¡œ ì„¤ì •
base_dir = os.path.dirname(os.path.realpath(__file__))
metadata_file = os.path.join(base_dir, "../FAISS/Metadata/jhgan_metadata.pkl")
with open(metadata_file, "wb") as f:
    pickle.dump(metadata, f)
print(f"ë©”íƒ€ë°ì´í„°ë¥¼ '{metadata_file}' íŒŒì¼ë¡œ ì €ì¥í–ˆìŠµë‹ˆë‹¤.")

# 4. FAISS ì¸ë±ìŠ¤ ìƒì„± ë° ì €ì¥ (ë‚´ì  ë°©ì‹)
embedding_dim = len(embeddings[0])
embeddings_np = np.array(embeddings).astype('float32')

# ë‚´ì  ë°©ì‹
index_dot_product = faiss.IndexFlatIP(embedding_dim)
index_dot_product.add(embeddings_np)
jhgan_index_file_dot = os.path.join(base_dir, "../FAISS/Index/jhgan_dotProduct_index.bin")
faiss.write_index(index_dot_product, jhgan_index_file_dot)
print(f"ë‚´ì  ê¸°ë°˜ FAISS ì¸ë±ìŠ¤ë¥¼ '{jhgan_index_file_dot}' íŒŒì¼ë¡œ ì €ì¥í–ˆìŠµë‹ˆë‹¤.")

# ì½”ì‚¬ì¸ ìœ ì‚¬ë„ ë°©ì‹ (ë²¡í„° ì •ê·œí™” í›„ ë‚´ì  ë°©ì‹ í™œìš©)
index_cosine = faiss.IndexFlatIP(embedding_dim)
embeddings_cosine = normalize(embeddings_np, norm='l2')  # ì •ê·œí™”í•˜ì—¬ ì½”ì‚¬ì¸ ìœ ì‚¬ë„ ê³„ì‚°
index_cosine.add(embeddings_cosine)
jhgan_index_file_cosine = os.path.join(base_dir, "../FAISS/Index/jhgan_cosine_index.bin")
faiss.write_index(index_cosine, jhgan_index_file_cosine)
print(f"ì½”ì‚¬ì¸ ìœ ì‚¬ë„ ê¸°ë°˜ FAISS ì¸ë±ìŠ¤ë¥¼ '{jhgan_index_file_cosine}' íŒŒì¼ë¡œ ì €ì¥í–ˆìŠµë‹ˆë‹¤.")

# ìœ í´ë¦¬ë“œ ê±°ë¦¬ ë°©ì‹
index_euclidean = faiss.IndexFlatL2(embedding_dim)
for _ in tqdm(range(1), desc="Adding embeddings to Euclidean index"):
    index_euclidean.add(embeddings_np)
jhgan_index_file_euclidean = os.path.join(base_dir, "../FAISS/Index/jhgan_euclidean_index.bin")
faiss.write_index(index_euclidean, jhgan_index_file_euclidean)
print(f"ìœ í´ë¦¬ë“œ ê±°ë¦¬ ê¸°ë°˜ FAISS ì¸ë±ìŠ¤ë¥¼ '{jhgan_index_file_euclidean}' íŒŒì¼ë¡œ ì €ì¥í–ˆìŠµë‹ˆë‹¤.")


### C:\repository\HAI_Python\app\faiss\archive\embedding_1000.py ###
import os
import faiss
from sentence_transformers import SentenceTransformer
import numpy as np
import psycopg2
import pickle
from transformers import AutoTokenizer

# 1. PostgreSQLì—ì„œ ë°ì´í„° ê°€ì ¸ì˜¤ê¸°
try:
    conn = psycopg2.connect(
        host="localhost",
        database="heritage_db",
        user="postgres",
        password="iam@123"
    )
    cursor = conn.cursor()
    cursor.execute("SELECT ccbaAsno, content FROM national_heritage LIMIT 1000")  # ëª¨ë“  ë°ì´í„° ê°€ì ¸ì˜¤ê¸°
    rows = cursor.fetchall()
    conn.close()
except Exception as e:
    print("PostgreSQL ì—°ê²° ì‹¤íŒ¨:", e)
    exit()

# 2. ëª¨ë¸ ë¡œë“œ ë° ë°ì´í„° ì„ë² ë”©
model_name = 'sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2'
model = SentenceTransformer(model_name)
tokenizer = AutoTokenizer.from_pretrained(model_name)  # SentenceTransformerì™€ ë™ì¼í•œ í† í¬ë‚˜ì´ì € ì‚¬ìš©

embeddings = []
metadata = []

for row in rows:
    original_id, text = row
    tokens = tokenizer.tokenize(text)
    token_count = len(tokens)
    print(f"ë°ì´í„° ID {original_id}ì˜ í† í° ìˆ˜: {token_count}")

    # ìŠ¬ë¼ì´ë”© ìœˆë„ìš° ì ìš© (ìµœëŒ€ 512 í† í°ì”©)
    max_tokens = 512
    stride = 256

    start_idx = 0
    segment_id = 0
    while start_idx < token_count:
        end_idx = min(start_idx + max_tokens, token_count)
        window_tokens = tokens[start_idx:end_idx]
        window_text = tokenizer.convert_tokens_to_string(window_tokens)

        # ìµœëŒ€ ê¸¸ì´ë¥¼ ì´ˆê³¼í•˜ëŠ” ê²½ìš° ìŠ¬ë¼ì´ë”© ìœˆë„ìš° ì ìš©
        if len(window_tokens) > tokenizer.model_max_length:
            print(f"ê²½ê³ : ì„¸ê·¸ë¨¼íŠ¸ì˜ í† í° ìˆ˜ê°€ ëª¨ë¸ì˜ ìµœëŒ€ ì…ë ¥ ê¸¸ì´({tokenizer.model_max_length})ë¥¼ ì´ˆê³¼í•©ë‹ˆë‹¤. ì„¸ê·¸ë¨¼íŠ¸ë¥¼ ì¡°ì •í•©ë‹ˆë‹¤.")
            window_tokens = window_tokens[:tokenizer.model_max_length]
            window_text = tokenizer.convert_tokens_to_string(window_tokens)

        embedding = model.encode(window_text)

        embeddings.append(embedding)

        # ê° ìœˆë„ìš° êµ¬ê°„ì—ë„ ì„¸ê·¸ë¨¼íŠ¸ ID ì¶”ê°€í•˜ì—¬ ë©”íƒ€ë°ì´í„° ìƒì„±
        metadata_entry = {
            "original_id": original_id,      # ì›ë³¸ ë°ì´í„° ID
            "segment_id": segment_id,        # ì„¸ê·¸ë¨¼íŠ¸ ë²ˆí˜¸
            "text_segment": window_text      # í…ìŠ¤íŠ¸ ì„¸ê·¸ë¨¼íŠ¸
        }
        metadata.append(metadata_entry)

        start_idx += stride
        segment_id += 1

# 3. FAISS ì¸ë±ìŠ¤ ìƒì„± ë° ë°ì´í„° ì¶”ê°€
embedding_dim = len(embeddings[0])
index = faiss.IndexFlatL2(embedding_dim)

# FAISSëŠ” numpy ë°°ì—´ë¡œ ë°ì´í„°ë¥¼ ë‹¤ë£¨ë¯€ë¡œ ë¦¬ìŠ¤íŠ¸ë¥¼ numpyë¡œ ë³€í™˜
embeddings_np = np.array(embeddings).astype('float32')

# ì¸ë±ìŠ¤ì— ë²¡í„° ì¶”ê°€
index.add(embeddings_np)

# 4. FAISS ì¸ë±ìŠ¤ ë° ë©”íƒ€ë°ì´í„° íŒŒì¼ ì €ì¥
base_dir = os.path.dirname(os.path.realpath(__file__))
faiss_index_file = os.path.join(base_dir, "../FAISS/faiss_index_1000_02.bin")
faiss.write_index(index, faiss_index_file)
print(f"FAISS ì¸ë±ìŠ¤ë¥¼ '{faiss_index_file}' íŒŒì¼ë¡œ ì €ì¥í–ˆìŠµë‹ˆë‹¤.")

metadata_file = os.path.join(base_dir, "../FAISS/faiss_metadata_1000_02.pkl")
with open(metadata_file, "wb") as f:
    pickle.dump(metadata, f)
print(f"ë©”íƒ€ë°ì´í„°ë¥¼ '{metadata_file}' íŒŒì¼ë¡œ ì €ì¥í–ˆìŠµë‹ˆë‹¤.")


### C:\repository\HAI_Python\app\faiss\archive\embedding_1000_jhgan.py ###
import os
import psycopg2
import re
import kss
import faiss
import pickle
import numpy as np
from sentence_transformers import SentenceTransformer
from sklearn.preprocessing import normalize

# 1. PostgreSQLì—ì„œ ë°ì´í„° ê°€ì ¸ì˜¤ê¸°
try:
    conn = psycopg2.connect(
        host="localhost",
        database="heritage_db",
        user="postgres",
        password="iam@123"
    )
    cursor = conn.cursor()
    cursor.execute("SELECT id, content FROM national_heritage LIMIT 1000")  # 1000ê°œì˜ ë°ì´í„° ê°€ì ¸ì˜¤ê¸°
    rows = cursor.fetchall()
    conn.close()
except Exception as e:
    print("PostgreSQL ì—°ê²° ì‹¤íŒ¨:", e)
    exit()

# 2. ëª¨ë¸ ë° í† í¬ë‚˜ì´ì € ë¡œë“œ
model = SentenceTransformer('jhgan/ko-sroberta-multitask')
embeddings = []
metadata = []

# 3. ë°ì´í„° ì „ì²˜ë¦¬ ë° ìŠ¬ë¼ì´ë”© ìœˆë„ìš° ë°©ì‹ ì ìš©
max_tokens = 512
stride = 256
context_overlap = 2  # ë¬¸ë§¥ ë³´ì¡´ì„ ìœ„í•´ ì¤‘ì²©í•  ë¬¸ì¥ ìˆ˜

for row in rows:
    primary_id, text = row

    # ë¬¸ì¥ ë‹¨ìœ„ë¡œ ë¶„ë¦¬ (KSS ì‚¬ìš©)
    sentences = kss.split_sentences(text)

    # ê° ë¬¸ì¥ì—ì„œ êµ¬ë‘ì  ì œê±°
    processed_sentences = [re.sub(r'[\.,!?]', '', sentence) for sentence in sentences]

    segment_id = 0
    i = 0
    while i < len(processed_sentences):
        # í˜„ì¬ ìŠ¬ë¼ì´ë”© ìœˆë„ìš° ë²”ìœ„ì˜ ë¬¸ì¥ë“¤
        current_sentences = processed_sentences[i:i + stride]

        # ì´ì „ ë¬¸ì¥ ì¼ë¶€ë¥¼ í¬í•¨í•´ ë¬¸ë§¥ ë³´ì¡´
        if i > 0:
            overlap_sentences = processed_sentences[max(0, i - context_overlap):i]
            current_sentences = overlap_sentences + current_sentences

        # ë¬¸ì¥ë“¤ì„ í•˜ë‚˜ì˜ í…ìŠ¤íŠ¸ë¡œ ê²°í•©
        current_text = " ".join(current_sentences)

        # ì„ë² ë”© ìƒì„±
        embedding = model.encode(current_text)
        embeddings.append(embedding)

        metadata_entry = {
            "primary_id": primary_id,         # PostgreSQLì˜ ê³ ìœ  ì‹ë³„ì
            "segment_id": segment_id,         # ìŠ¬ë¼ì´ë”© ìœˆë„ìš° êµ¬ê°„
            "text_segment": current_text      # í…ìŠ¤íŠ¸ ì„¸ê·¸ë¨¼íŠ¸
        }
        metadata.append(metadata_entry)
        segment_id += 1

        # stride ë§Œí¼ ì´ë™
        i += stride - context_overlap

# FAISS ì¸ë±ìŠ¤ ë° ë©”íƒ€ë°ì´í„° ì €ì¥ ê²½ë¡œ ì„¤ì •
base_dir = os.path.dirname(os.path.realpath(__file__))
metadata_file = os.path.join(base_dir, "../FAISS/Metadata/jhgan_metadata.pkl")
with open(metadata_file, "wb") as f:
    pickle.dump(metadata, f)
print(f"ë©”íƒ€ë°ì´í„°ë¥¼ '{metadata_file}' íŒŒì¼ë¡œ ì €ì¥í–ˆìŠµë‹ˆë‹¤.")

# 4. FAISS ì¸ë±ìŠ¤ ìƒì„± ë° ì €ì¥ (ë‚´ì  ë°©ì‹)
embedding_dim = len(embeddings[0])
embeddings_np = np.array(embeddings).astype('float32')

# ë‚´ì  ë°©ì‹
index_dot_product = faiss.IndexFlatIP(embedding_dim)
index_dot_product.add(embeddings_np)
faiss_index_file_dot = os.path.join(base_dir, "../FAISS/Index/jhgan_dotProduct_index.bin")
faiss.write_index(index_dot_product, faiss_index_file_dot)
print(f"ë‚´ì  ê¸°ë°˜ FAISS ì¸ë±ìŠ¤ë¥¼ '{faiss_index_file_dot}' íŒŒì¼ë¡œ ì €ì¥í–ˆìŠµë‹ˆë‹¤.")

# ì½”ì‚¬ì¸ ìœ ì‚¬ë„ ë°©ì‹ (ë²¡í„° ì •ê·œí™” í›„ ë‚´ì  ë°©ì‹ í™œìš©)
index_cosine = faiss.IndexFlatIP(embedding_dim)
embeddings_cosine = normalize(embeddings_np, norm='l2')  # ì •ê·œí™”í•˜ì—¬ ì½”ì‚¬ì¸ ìœ ì‚¬ë„ ê³„ì‚°
index_cosine.add(embeddings_cosine)
faiss_index_file_cosine = os.path.join(base_dir, "../FAISS/Index/jhgan_cosine_index.bin")
faiss.write_index(index_cosine, faiss_index_file_cosine)
print(f"ì½”ì‚¬ì¸ ìœ ì‚¬ë„ ê¸°ë°˜ FAISS ì¸ë±ìŠ¤ë¥¼ '{faiss_index_file_cosine}' íŒŒì¼ë¡œ ì €ì¥í–ˆìŠµë‹ˆë‹¤.")

# ìœ í´ë¦¬ë“œ ê±°ë¦¬ ë°©ì‹
index_euclidean = faiss.IndexFlatL2(embedding_dim)
index_euclidean.add(embeddings_np)
faiss_index_file_euclidean = os.path.join(base_dir, "../FAISS/Index/jhgan_euclidean_index.bin")
faiss.write_index(index_euclidean, faiss_index_file_euclidean)
print(f"ìœ í´ë¦¬ë“œ ê±°ë¦¬ ê¸°ë°˜ FAISS ì¸ë±ìŠ¤ë¥¼ '{faiss_index_file_euclidean}' íŒŒì¼ë¡œ ì €ì¥í–ˆìŠµë‹ˆë‹¤.")


### C:\repository\HAI_Python\app\faiss\archive\embedding_1000_processing.py ###
import os
import psycopg2
import re
import kss
from transformers import AutoTokenizer
from sentence_transformers import SentenceTransformer
import numpy as np

# 1. PostgreSQLì—ì„œ íŠ¹ì • ë°ì´í„° ê°€ì ¸ì˜¤ê¸°
try:
    conn = psycopg2.connect(
        host="localhost",
        database="heritage_db",
        user="postgres",
        password="iam@123"
    )
    cursor = conn.cursor()
    cursor.execute("SELECT ccbaAsno, content FROM national_heritage WHERE ccbaAsno = 340000 LIMIT 1")  # íŠ¹ì • IDì˜ ë°ì´í„°ë§Œ ê°€ì ¸ì˜¤ê¸°
    row = cursor.fetchone()
    conn.close()
except Exception as e:
    print("PostgreSQL ì—°ê²° ì‹¤íŒ¨:", e)
    exit()

# 2. ë°ì´í„° ì „ì²˜ë¦¬
if row:
    original_id, text = row

    # 3. ì „ì²˜ë¦¬ ë‹¨ê³„ - í•œêµ­ì–´ íŠ¹ì„±ì— ë§ëŠ” ì „ì²˜ë¦¬ ì ìš©
    # ë¬¸ì¥ ë‹¨ìœ„ë¡œ ë¶„ë¦¬ (KSS ì‚¬ìš©)
    sentences = kss.split_sentences(text)

    # ê° ë¬¸ì¥ì—ì„œ êµ¬ë‘ì  ì œê±°
    processed_sentences = [re.sub(r'[\.,!?]', '', sentence) for sentence in sentences]

    # ì„ë² ë”© ëª¨ë¸ ë° í† í¬ë‚˜ì´ì € ë¡œë“œ
    model_name = 'sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2'
    model = SentenceTransformer(model_name)
    tokenizer = AutoTokenizer.from_pretrained(model_name)

    # ìŠ¬ë¼ì´ë”© ìœˆë„ìš° ë°©ì‹ìœ¼ë¡œ ë¬¸ì¥ì„ ì¶”ê°€í•˜ë©° 512 í† í°ì„ ë„˜ì§€ ì•Šë„ë¡ ìœ ì§€
    max_tokens = 512
    current_tokens = 0
    current_text = ""
    embeddings = []

    for sentence in processed_sentences:
        # í˜„ì¬ ë¬¸ì¥ì„ ì¶”ê°€í–ˆì„ ë•Œ í† í° ìˆ˜ í™•ì¸
        tokens = tokenizer.tokenize(current_text + " " + sentence)
        token_count = len(tokens)

        if token_count <= max_tokens:
            # í† í° ìˆ˜ê°€ ìµœëŒ€ì¹˜ë¥¼ ë„˜ì§€ ì•Šìœ¼ë©´ ë¬¸ì¥ì„ ì¶”ê°€
            current_text += " " + sentence
            current_tokens = token_count
        else:
            # í† í° ìˆ˜ê°€ ìµœëŒ€ì¹˜ë¥¼ ë„˜ìœ¼ë©´ í˜„ì¬ í…ìŠ¤íŠ¸ë¥¼ ì„ë² ë”©í•˜ê³  ì´ˆê¸°í™”
            if current_text:
                embedding = model.encode(current_text.strip())
                embeddings.append(embedding)

            # ìƒˆë¡œìš´ ë¬¸ì¥ìœ¼ë¡œ ì´ˆê¸°í™”
            current_text = sentence
            current_tokens = len(tokenizer.tokenize(sentence))

    # ë§ˆì§€ë§‰ ë‚¨ì€ í…ìŠ¤íŠ¸ë„ ì„ë² ë”©
    if current_text:
        embedding = model.encode(current_text.strip())
        embeddings.append(embedding)

    # ì „ì²˜ë¦¬ ë° ì„ë² ë”©ëœ ê²°ê³¼ ì¶œë ¥
    print(f"ccbaAsno: {original_id}")
    print("ì„ë² ë”©ëœ ë²¡í„°ì˜ ìˆ˜:", len(embeddings))
    for i, embedding in enumerate(embeddings):
        print(f"ë²¡í„° {i + 1}: {embedding[:10]}...")  # ë²¡í„°ì˜ ì• 10ê°œ ìš”ì†Œë§Œ ì¶œë ¥
else:
    print("ë°ì´í„°ë¥¼ ê°€ì ¸ì˜¤ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.")


### C:\repository\HAI_Python\app\faiss\archive\embedding_1000_snunlp.py ###
import os
import psycopg2
import re
import kss
import faiss
import pickle
import numpy as np
from sentence_transformers import SentenceTransformer
from transformers import AutoTokenizer
from sklearn.preprocessing import normalize

# 1. PostgreSQLì—ì„œ 1000ê°œì˜ ë°ì´í„° ê°€ì ¸ì˜¤ê¸°
try:
    conn = psycopg2.connect(
        host="localhost",
        database="heritage_db",
        user="postgres",
        password="iam@123"
    )
    cursor = conn.cursor()
    cursor.execute("SELECT ccbaAsno, content FROM national_heritage LIMIT 1000")  # 1000ê°œì˜ ë°ì´í„° ê°€ì ¸ì˜¤ê¸°
    rows = cursor.fetchall()
    conn.close()
except Exception as e:
    print("PostgreSQL ì—°ê²° ì‹¤íŒ¨:", e)
    exit()

# 2. ëª¨ë¸ ë° í† í¬ë‚˜ì´ì € ë¡œë“œ
model = SentenceTransformer('snunlp/KR-SBERT-V40K-klueNLI-augSTS')  # í•œêµ­ì–´ SBERT ëª¨ë¸
tokenizer = AutoTokenizer.from_pretrained('snunlp/KR-SBERT-V40K-klueNLI-augSTS')

embeddings = []
metadata = []

# 3. ë°ì´í„° ì „ì²˜ë¦¬ ë° ìŠ¬ë¼ì´ë”© ìœˆë„ìš° ë°©ì‹ ì ìš©
max_tokens = 512
stride = 256
context_overlap = 2  # ë¬¸ë§¥ ë³´ì¡´ì„ ìœ„í•´ ì¤‘ì²©í•  ë¬¸ì¥ ìˆ˜

for row in rows:
    original_id, text = row

    # ë¬¸ì¥ ë‹¨ìœ„ë¡œ ë¶„ë¦¬ (KSS ì‚¬ìš©)
    sentences = kss.split_sentences(text)

    # ê° ë¬¸ì¥ì—ì„œ êµ¬ë‘ì  ì œê±°
    processed_sentences = [re.sub(r'[\.,!?]', '', sentence) for sentence in sentences]

    current_text = ""
    segment_id = 0

    i = 0
    while i < len(processed_sentences):
        # í˜„ì¬ ìŠ¬ë¼ì´ë”© ìœˆë„ìš° ë²”ìœ„ì˜ ë¬¸ì¥ë“¤
        current_sentences = processed_sentences[i:i + stride]

        # ì´ì „ ë¬¸ì¥ ì¼ë¶€ë¥¼ í¬í•¨í•´ ë¬¸ë§¥ ë³´ì¡´
        if i > 0:
            overlap_sentences = processed_sentences[max(0, i - context_overlap):i]
            current_sentences = overlap_sentences + current_sentences

        # ë¬¸ì¥ë“¤ì„ í•˜ë‚˜ì˜ í…ìŠ¤íŠ¸ë¡œ ê²°í•©
        current_text = " ".join(current_sentences)

        # ì„ë² ë”© ìƒì„±
        embedding = model.encode(current_text)
        embeddings.append(embedding)

        metadata_entry = {
            "original_id": original_id,
            "segment_id": segment_id,
            "text_segment": current_text
        }
        metadata.append(metadata_entry)
        segment_id += 1

        # stride ë§Œí¼ ì´ë™
        i += stride - context_overlap

# FAISS ì¸ë±ìŠ¤ ì €ì¥ ê²½ë¡œ ì„¤ì •
base_dir = os.path.dirname(os.path.realpath(__file__))
metadata_file = os.path.join(base_dir, "../FAISS/snunlp_metadata_1000.pkl")
with open(metadata_file, "wb") as f:
    pickle.dump(metadata, f)
print(f"ë©”íƒ€ë°ì´í„°ë¥¼ '{metadata_file}' íŒŒì¼ë¡œ ì €ì¥í–ˆìŠµë‹ˆë‹¤.")

# 4. FAISS ì¸ë±ìŠ¤ ìƒì„± ë° ì €ì¥ (ë‚´ì  ë°©ì‹)
embedding_dim = len(embeddings[0])
embeddings_np = np.array(embeddings).astype('float32')

# ë‚´ì  ë°©ì‹
index_dot_product = faiss.IndexFlatIP(embedding_dim)
index_dot_product.add(embeddings_np)
faiss_index_file_dot = os.path.join(base_dir, "../FAISS/snunlp_dotProduct_index_1000.bin")
faiss.write_index(index_dot_product, faiss_index_file_dot)
print(f"ë‚´ì  ê¸°ë°˜ FAISS ì¸ë±ìŠ¤ë¥¼ '{faiss_index_file_dot}' íŒŒì¼ë¡œ ì €ì¥í–ˆìŠµë‹ˆë‹¤.")

# ì½”ì‚¬ì¸ ìœ ì‚¬ë„ ë°©ì‹ (ë²¡í„° ì •ê·œí™” í›„ ë‚´ì  ë°©ì‹ í™œìš©)
index_cosine = faiss.IndexFlatIP(embedding_dim)
embeddings_cosine = normalize(embeddings_np, norm='l2')  # ì •ê·œí™”í•˜ì—¬ ì½”ì‚¬ì¸ ìœ ì‚¬ë„ ê³„ì‚°
index_cosine.add(embeddings_cosine)
faiss_index_file_cosine = os.path.join(base_dir, "../FAISS/snunlp_cosine_index_1000.bin")
faiss.write_index(index_cosine, faiss_index_file_cosine)
print(f"ì½”ì‚¬ì¸ ìœ ì‚¬ë„ ê¸°ë°˜ FAISS ì¸ë±ìŠ¤ë¥¼ '{faiss_index_file_cosine}' íŒŒì¼ë¡œ ì €ì¥í–ˆìŠµë‹ˆë‹¤.")

# ìœ í´ë¦¬ë“œ ê±°ë¦¬ ë°©ì‹
index_euclidean = faiss.IndexFlatL2(embedding_dim)
index_euclidean.add(embeddings_np)
faiss_index_file_euclidean = os.path.join(base_dir, "../FAISS/snunlp_euclidean_index_1000.bin")
faiss.write_index(index_euclidean, faiss_index_file_euclidean)
print(f"ìœ í´ë¦¬ë“œ ê±°ë¦¬ ê¸°ë°˜ FAISS ì¸ë±ìŠ¤ë¥¼ '{faiss_index_file_euclidean}' íŒŒì¼ë¡œ ì €ì¥í–ˆìŠµë‹ˆë‹¤.")


### C:\repository\HAI_Python\app\faiss\archive\embedding_cosine.py ###
import os
import psycopg2
import re
import kss
import faiss
import pickle
import numpy as np
from sentence_transformers import SentenceTransformer
from transformers import AutoTokenizer
from sklearn.preprocessing import normalize

# 1. PostgreSQLì—ì„œ 1000ê°œì˜ ë°ì´í„° ê°€ì ¸ì˜¤ê¸°
try:
    conn = psycopg2.connect(
        host="localhost",
        database="heritage_db",
        user="postgres",
        password="iam@123"
    )
    cursor = conn.cursor()
    cursor.execute("SELECT ccbaAsno, content FROM national_heritage")  # ë°ì´í„° ì „ë¶€ ê°€ì ¸ì˜¤ê¸°
    rows = cursor.fetchall()
    conn.close()
except Exception as e:
    print("PostgreSQL ì—°ê²° ì‹¤íŒ¨:", e)
    exit()

# 2. ëª¨ë¸ ë° í† í¬ë‚˜ì´ì € ë¡œë“œ
model = SentenceTransformer('sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2')
tokenizer = AutoTokenizer.from_pretrained('sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2')

embeddings = []
metadata = []

# 3. ë°ì´í„° ì „ì²˜ë¦¬ ë° ìŠ¬ë¼ì´ë”© ìœˆë„ìš° ë°©ì‹ ì ìš©
max_tokens = 512
stride = 256
context_overlap = 2  # ë¬¸ë§¥ ë³´ì¡´ì„ ìœ„í•´ ì¤‘ì²©í•  ë¬¸ì¥ ìˆ˜

for row in rows:
    original_id, text = row

    # ë¬¸ì¥ ë‹¨ìœ„ë¡œ ë¶„ë¦¬ (KSS ì‚¬ìš©)
    sentences = kss.split_sentences(text)

    # ê° ë¬¸ì¥ì—ì„œ êµ¬ë‘ì  ì œê±°
    processed_sentences = [re.sub(r'[\.,!?]', '', sentence) for sentence in sentences]

    current_text = ""
    segment_id = 0

    i = 0
    while i < len(processed_sentences):
        # í˜„ì¬ ìŠ¬ë¼ì´ë”© ìœˆë„ìš° ë²”ìœ„ì˜ ë¬¸ì¥ë“¤
        current_sentences = processed_sentences[i:i + stride]

        # ì´ì „ ë¬¸ì¥ ì¼ë¶€ë¥¼ í¬í•¨í•´ ë¬¸ë§¥ ë³´ì¡´
        if i > 0:
            overlap_sentences = processed_sentences[max(0, i - context_overlap):i]
            current_sentences = overlap_sentences + current_sentences

        # ë¬¸ì¥ë“¤ì„ í•˜ë‚˜ì˜ í…ìŠ¤íŠ¸ë¡œ ê²°í•©
        current_text = " ".join(current_sentences)

        # ì„ë² ë”© ìƒì„±
        embedding = model.encode(current_text)
        embeddings.append(embedding)

        metadata_entry = {
            "original_id": original_id,
            "segment_id": segment_id,
            "text_segment": current_text
        }
        metadata.append(metadata_entry)
        segment_id += 1

        # stride ë§Œí¼ ì´ë™
        i += stride - context_overlap

# 4. FAISS ì¸ë±ìŠ¤ ìƒì„± ë° ë°ì´í„° ì¶”ê°€ (Cosine ìœ ì‚¬ë„ ì‚¬ìš©)
embedding_dim = len(embeddings[0])
index = faiss.IndexFlatIP(embedding_dim)

# FAISSëŠ” numpy ë°°ì—´ë¡œ ë°ì´í„°ë¥¼ ë‹¤ë£¨ë¯€ë¡œ ë¦¬ìŠ¤íŠ¸ë¥¼ numpyë¡œ ë³€í™˜
embeddings_np = np.array(embeddings).astype('float32')

# L2 ì •ê·œí™”ë¥¼ ì ìš©í•˜ì—¬ ë²¡í„°ë¥¼ ì •ê·œí™”
embeddings_np = normalize(embeddings_np, norm='l2')

# ì¸ë±ìŠ¤ì— ë²¡í„° ì¶”ê°€
index.add(embeddings_np)

# 5. FAISS ì¸ë±ìŠ¤ ë° ë©”íƒ€ë°ì´í„° íŒŒì¼ ì €ì¥
base_dir = os.path.dirname(os.path.realpath(__file__))
faiss_index_file = os.path.join(base_dir, "../FAISS/faiss_index_1000_cosine.bin")
faiss.write_index(index, faiss_index_file)
print(f"FAISS ì¸ë±ìŠ¤ë¥¼ '{faiss_index_file}' íŒŒì¼ë¡œ ì €ì¥í–ˆìŠµë‹ˆë‹¤.")

metadata_file = os.path.join(base_dir, "../FAISS/faiss_metadata_1000_cosine.pkl")
with open(metadata_file, "wb") as f:
    pickle.dump(metadata, f)
print(f"ë©”íƒ€ë°ì´í„°ë¥¼ '{metadata_file}' íŒŒì¼ë¡œ ì €ì¥í–ˆìŠµë‹ˆë‹¤.")


### C:\repository\HAI_Python\app\faiss\archive\embedding_dotProduct.py ###
import os
import psycopg2
import re
import kss
import faiss
import pickle
import numpy as np
from sentence_transformers import SentenceTransformer
from transformers import AutoTokenizer

# 1. PostgreSQLì—ì„œ 1000ê°œì˜ ë°ì´í„° ê°€ì ¸ì˜¤ê¸°
try:
    conn = psycopg2.connect(
        host="localhost",
        database="heritage_db",
        user="postgres",
        password="iam@123"
    )
    cursor = conn.cursor()
    cursor.execute("SELECT ccbaAsno, content FROM national_heritage")  # 1000ê°œì˜ ë°ì´í„° ê°€ì ¸ì˜¤ê¸°
    rows = cursor.fetchall()
    conn.close()
except Exception as e:
    print("PostgreSQL ì—°ê²° ì‹¤íŒ¨:", e)
    exit()

# 2. ëª¨ë¸ ë° í† í¬ë‚˜ì´ì € ë¡œë“œ
model = SentenceTransformer('sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2')
tokenizer = AutoTokenizer.from_pretrained('sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2')

embeddings = []
metadata = []

# 3. ë°ì´í„° ì „ì²˜ë¦¬ ë° ìŠ¬ë¼ì´ë”© ìœˆë„ìš° ë°©ì‹ ì ìš©
max_tokens = 512
stride = 256
context_overlap = 2  # ë¬¸ë§¥ ë³´ì¡´ì„ ìœ„í•´ ì¤‘ì²©í•  ë¬¸ì¥ ìˆ˜

for row in rows:
    original_id, text = row

    # ë¬¸ì¥ ë‹¨ìœ„ë¡œ ë¶„ë¦¬ (KSS ì‚¬ìš©)
    sentences = kss.split_sentences(text)

    # ê° ë¬¸ì¥ì—ì„œ êµ¬ë‘ì  ì œê±°
    processed_sentences = [re.sub(r'[\.,!?]', '', sentence) for sentence in sentences]

    current_text = ""
    segment_id = 0

    i = 0
    while i < len(processed_sentences):
        # í˜„ì¬ ìŠ¬ë¼ì´ë”© ìœˆë„ìš° ë²”ìœ„ì˜ ë¬¸ì¥ë“¤
        current_sentences = processed_sentences[i:i + stride]

        # ì´ì „ ë¬¸ì¥ ì¼ë¶€ë¥¼ í¬í•¨í•´ ë¬¸ë§¥ ë³´ì¡´
        if i > 0:
            overlap_sentences = processed_sentences[max(0, i - context_overlap):i]
            current_sentences = overlap_sentences + current_sentences

        # ë¬¸ì¥ë“¤ì„ í•˜ë‚˜ì˜ í…ìŠ¤íŠ¸ë¡œ ê²°í•©
        current_text = " ".join(current_sentences)

        # ì„ë² ë”© ìƒì„±
        embedding = model.encode(current_text)
        embeddings.append(embedding)

        metadata_entry = {
            "original_id": original_id,
            "segment_id": segment_id,
            "text_segment": current_text
        }
        metadata.append(metadata_entry)
        segment_id += 1

        # stride ë§Œí¼ ì´ë™
        i += stride - context_overlap

# 4. ë‚´ì ì„ ì‚¬ìš©í•´ì„œ FAISS ì¸ë±ìŠ¤ ìƒì„± ë° ë°ì´í„° ì¶”ê°€ (Cosine ìœ ì‚¬ë„ ì‚¬ìš©)
embedding_dim = len(embeddings[0])
index = faiss.IndexFlatIP(embedding_dim)

# FAISSëŠ” numpy ë°°ì—´ë¡œ ë°ì´í„°ë¥¼ ë‹¤ë£¨ë¯€ë¡œ ë¦¬ìŠ¤íŠ¸ë¥¼ numpyë¡œ ë³€í™˜
embeddings_np = np.array(embeddings).astype('float32')

# ì¸ë±ìŠ¤ì— ë²¡í„° ì¶”ê°€
index.add(embeddings_np)

# 5. FAISS ì¸ë±ìŠ¤ ë° ë©”íƒ€ë°ì´í„° íŒŒì¼ ì €ì¥
base_dir = os.path.dirname(os.path.realpath(__file__))
faiss_index_file = os.path.join(base_dir, "../FAISS/faiss_index_1000_dotProduct.bin")
faiss.write_index(index, faiss_index_file)
print(f"FAISS ì¸ë±ìŠ¤ë¥¼ '{faiss_index_file}' íŒŒì¼ë¡œ ì €ì¥í–ˆìŠµë‹ˆë‹¤.")

metadata_file = os.path.join(base_dir, "../FAISS/faiss_metadata_1000_dotProduct.pkl")
with open(metadata_file, "wb") as f:
    pickle.dump(metadata, f)
print(f"ë©”íƒ€ë°ì´í„°ë¥¼ '{metadata_file}' íŒŒì¼ë¡œ ì €ì¥í–ˆìŠµë‹ˆë‹¤.")


### C:\repository\HAI_Python\app\faiss\archive\embedding_uclid.py ###
import os 
import psycopg2
import re
import kss
import faiss
import pickle
import numpy as np
from sentence_transformers import SentenceTransformer
from transformers import AutoTokenizer

# 1. PostgreSQLì—ì„œ 1000ê°œì˜ ë°ì´í„° ê°€ì ¸ì˜¤ê¸°
try:
    conn = psycopg2.connect(
        host="localhost",
        database="heritage_db",
        user="postgres",
        password="iam@123"
    )
    cursor = conn.cursor()
    cursor.execute("SELECT ccbaAsno, content FROM national_heritage")  # 1000ê°œì˜ ë°ì´í„° ê°€ì ¸ì˜¤ê¸°
    rows = cursor.fetchall()
    conn.close()
except Exception as e:
    print("PostgreSQL ì—°ê²° ì‹¤íŒ¨:", e)
    exit()

# 2. ëª¨ë¸ ë° í† í¬ë‚˜ì´ì € ë¡œë“œ
model = SentenceTransformer('sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2')
tokenizer = AutoTokenizer.from_pretrained('sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2')

embeddings = []
metadata = []

# 3. ë°ì´í„° ì „ì²˜ë¦¬ ë° ìŠ¬ë¼ì´ë”© ìœˆë„ìš° ë°©ì‹ ì ìš©
max_tokens = 512
stride = 256
context_overlap = 2  # ë¬¸ë§¥ ë³´ì¡´ì„ ìœ„í•´ ì¤‘ì²©í•  ë¬¸ì¥ ìˆ˜

for row in rows:
    original_id, text = row

    # ë¬¸ì¥ ë‹¨ìœ„ë¡œ ë¶„ë¦¬ (KSS ì‚¬ìš©)
    sentences = kss.split_sentences(text)

    # ê° ë¬¸ì¥ì—ì„œ êµ¬ë‘ì  ì œê±°
    processed_sentences = [re.sub(r'[\.,!?]', '', sentence) for sentence in sentences]

    current_text = ""
    segment_id = 0

    for i in range(0, len(processed_sentences), stride):
        # í˜„ì¬ ìŠ¬ë¼ì´ë”© ìœˆë„ìš° ë²”ìœ„ì˜ ë¬¸ì¥ë“¤
        current_sentences = processed_sentences[i:i + stride]

        # ì´ì „ ë¬¸ì¥ ì¼ë¶€ë¥¼ í¬í•¨í•´ ë¬¸ë§¥ ë³´ì¡´
        if i > 0:
            overlap_sentences = processed_sentences[max(0, i - context_overlap):i]
            current_sentences = overlap_sentences + current_sentences

        # ë¬¸ì¥ë“¤ì„ í•˜ë‚˜ì˜ í…ìŠ¤íŠ¸ë¡œ ê²°í•©
        current_text = " ".join(current_sentences)

        # ì„ë² ë”© ìƒì„±
        embedding = model.encode(current_text)
        embeddings.append(embedding)

        metadata_entry = {
            "original_id": original_id,
            "segment_id": segment_id,
            "text_segment": current_text
        }
        metadata.append(metadata_entry)
        segment_id += 1

# 4. ìœ í´ë¦¬ë””ì•ˆ ê±°ë¦¬(L2 ê±°ë¦¬)ë°©ì‹ì„ ì‚¬ìš©í•´ì„œ FAISS ì¸ë±ìŠ¤ ìƒì„± ë° ë°ì´í„° ì¶”ê°€
embedding_dim = len(embeddings[0])
index = faiss.IndexFlatL2(embedding_dim)

# FAISSëŠ” numpy ë°°ì—´ë¡œ ë°ì´í„°ë¥¼ ë‹¤ë£¨ë¯€ë¡œ ë¦¬ìŠ¤íŠ¸ë¥¼ numpyë¡œ ë³€í™˜
embeddings_np = np.array(embeddings).astype('float32')

# ì¸ë±ìŠ¤ì— ë²¡í„° ì¶”ê°€
index.add(embeddings_np)

# 5. FAISS ì¸ë±ìŠ¤ ë° ë©”íƒ€ë°ì´í„° íŒŒì¼ ì €ì¥
base_dir = os.path.dirname(os.path.realpath(__file__))
faiss_index_file = os.path.join(base_dir, "../FAISS/faiss_index_1000_uclid.bin")
faiss.write_index(index, faiss_index_file)
print(f"FAISS ì¸ë±ìŠ¤ë¥¼ '{faiss_index_file}' íŒŒì¼ë¡œ ì €ì¥í–ˆìŠµë‹ˆë‹¤.")

metadata_file = os.path.join(base_dir, "../FAISS/faiss_metadata_1000_uclid.pkl")
with open(metadata_file, "wb") as f:
    pickle.dump(metadata, f)
print(f"ë©”íƒ€ë°ì´í„°ë¥¼ '{metadata_file}' íŒŒì¼ë¡œ ì €ì¥í–ˆìŠµë‹ˆë‹¤.")


### C:\repository\HAI_Python\app\faiss\Metadata\read_metadata.py ###
# read_metadate.py
import pandas as pd

def read_metadata(file_path):
    """
    ë©”íƒ€ë°ì´í„°ë¥¼ í¬í•¨í•˜ëŠ” í”¼í´ íŒŒì¼ì„ ì½ì–´ì˜¤ëŠ” í•¨ìˆ˜

    Args:
    - file_path (str): ë©”íƒ€ë°ì´í„° í”¼í´ íŒŒì¼ì˜ ê²½ë¡œ

    Returns:
    - metadata_df (pd.DataFrame): í”¼í´ íŒŒì¼ì—ì„œ ì¶”ì¶œí•œ ë©”íƒ€ë°ì´í„°ê°€ í¬í•¨ëœ DataFrame
    """
    try:
        # í”¼í´ íŒŒì¼ì„ ë¡œë“œí•©ë‹ˆë‹¤.
        data = pd.read_pickle(file_path)

        # ë°ì´í„°ê°€ ë¦¬ìŠ¤íŠ¸ í˜•ì‹ì¸ì§€ í™•ì¸í•˜ê³  DataFrameìœ¼ë¡œ ë³€í™˜í•©ë‹ˆë‹¤.
        if isinstance(data, list) and all(isinstance(item, dict) for item in data):
            metadata_df = pd.DataFrame(data)
            return metadata_df
        else:
            raise ValueError("ë°ì´í„° í˜•ì‹ì´ ë©”íƒ€ë°ì´í„° ì¶”ì¶œì— ì í•©í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.")
    
    except Exception as e:
        print(f"ë©”íƒ€ë°ì´í„°ë¥¼ ì½ëŠ” ë„ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {e}")
        return None

def find_text_segment_nan(metadata_df):
    # ì—¬ëŸ¬ ê°€ì§€ ì¡°ê±´ì„ ì¶”ê°€í•˜ì—¬ text_segmentê°€ NaNì¸ ê²½ìš°ë¥¼ ì°¾ìŠµë‹ˆë‹¤.
    nan_rows = metadata_df[
        metadata_df['ë‚´ìš©'].isna() |                # NaNìœ¼ë¡œ íŒë³„ë˜ëŠ” ê²½ìš°
        (metadata_df['ë‚´ìš©'] == '') |               # ë¹ˆ ë¬¸ìì—´ì¸ ê²½ìš°
        (metadata_df['ë‚´ìš©'].str.lower() == 'nan')  # 'NaN'ì´ ë¬¸ìì—´ë¡œ ì €ì¥ëœ ê²½ìš°
    ]
    print("text_segmentì˜ ë‚´ìš©ì´ NaNì¸ í–‰ë“¤:")
    print(nan_rows)

# ì˜ˆì‹œ ì‚¬ìš©ë²•
if __name__ == "__main__":
    file_path = 'jhgan_metadata.pkl'
    metadata_df = read_metadata(file_path)
    
    if metadata_df is not None:
        print(metadata_df.head())
        print(metadata_df.loc[0, 'ë‚´ìš©'])
        find_text_segment_nan(metadata_df)
        non_zero_segments = metadata_df[metadata_df['segment_id'] != 0]
        print(non_zero_segments)


### C:\repository\HAI_Python\app\faiss\Metadata\temp_ner.py ###
# temp_ner.py
import pandas as pd
from transformers import AutoTokenizer, AutoModelForTokenClassification, pipeline

# ëª¨ë¸ê³¼ í† í¬ë‚˜ì´ì € ë¶ˆëŸ¬ì˜¤ê¸°
tokenizer = AutoTokenizer.from_pretrained("Leo97/KoELECTRA-small-v3-modu-ner")
model = AutoModelForTokenClassification.from_pretrained("Leo97/KoELECTRA-small-v3-modu-ner")

# NER íŒŒì´í”„ë¼ì¸ ì„¤ì •
ner_pipeline = pipeline("ner", model=model, tokenizer=tokenizer, aggregation_strategy="simple")

# ë©”íƒ€ë°ì´í„° íŒŒì¼ ì½ê¸°
def read_metadata(file_path):
    """
    ë©”íƒ€ë°ì´í„°ë¥¼ í¬í•¨í•˜ëŠ” í”¼í´ íŒŒì¼ì„ ì½ì–´ì˜¤ëŠ” í•¨ìˆ˜

    Args:
    - file_path (str): ë©”íƒ€ë°ì´í„° í”¼í´ íŒŒì¼ì˜ ê²½ë¡œ

    Returns:
    - metadata_df (pd.DataFrame): í”¼í´ íŒŒì¼ì—ì„œ ì¶”ì¶œí•œ ë©”íƒ€ë°ì´í„°ê°€ í¬í•¨ëœ DataFrame
    """
    try:
        # í”¼í´ íŒŒì¼ì„ ë¡œë“œí•©ë‹ˆë‹¤.
        data = pd.read_pickle(file_path)

        # ë°ì´í„°ê°€ ë¦¬ìŠ¤íŠ¸ í˜•ì‹ì¸ì§€ í™•ì¸í•˜ê³  DataFrameìœ¼ë¡œ ë³€í™˜í•©ë‹ˆë‹¤.
        if isinstance(data, list) and all(isinstance(item, dict) for item in data):
            metadata_df = pd.DataFrame(data)
            return metadata_df
        else:
            raise ValueError("ë°ì´í„° í˜•ì‹ì´ ë©”íƒ€ë°ì´í„° ì¶”ì¶œì— ì í•©í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.")
    
    except Exception as e:
        print(f"ë©”íƒ€ë°ì´í„°ë¥¼ ì½ëŠ” ë„ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {e}")
        return None

# NER íƒœê¹… ìˆ˜í–‰ í•¨ìˆ˜
def perform_ner_on_first_text(metadata_df):
    """
    ì²« ë²ˆì§¸ ë°ì´í„°ì˜ text_segmentì— NER íƒœê¹…ì„ ìˆ˜í–‰í•˜ëŠ” í•¨ìˆ˜

    Args:
    - metadata_df (pd.DataFrame): ë©”íƒ€ë°ì´í„° DataFrame
    """
    if metadata_df is not None and not metadata_df.empty:
        first_text = metadata_df.iloc[0]['text_segment']
        ner_results = ner_pipeline(first_text)
        print(f"ì²« ë²ˆì§¸ í…ìŠ¤íŠ¸ : \n{first_text}")
        # ë¶„ë¦¬ëœ í† í°ë“¤ì„ í•˜ë‚˜ì˜ ë‹¨ì–´ë¡œ ë³‘í•©í•˜ì—¬ ë³´ê¸° ì‰½ê²Œ ì²˜ë¦¬
        merged_results = []
        for entity in ner_results:
            if merged_results and entity['entity_group'] == merged_results[-1]['entity_group'] and entity['start'] == merged_results[-1]['end']:
                merged_results[-1]['word'] += entity['word'].replace('##', '')
                merged_results[-1]['end'] = entity['end']
            else:
                merged_results.append(entity)
        
        # ê²°ê³¼ ì¶œë ¥
        print("ì²« ë²ˆì§¸ ë°ì´í„°ì˜ NER ê²°ê³¼:")
        for entity in merged_results:
            print(f"Entity: {entity['word']}, Label: {entity['entity_group']}, Score: {entity['score']:.2f}")
    else:
        print("ë©”íƒ€ë°ì´í„°ê°€ ë¹„ì–´ ìˆê±°ë‚˜ ìœ íš¨í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.")

# ì˜ˆì‹œ ì‚¬ìš©ë²•
if __name__ == "__main__":
    file_path = 'jhgan_metadata.pkl'
    metadata_df = read_metadata(file_path)
    
    if metadata_df is not None:
        perform_ner_on_first_text(metadata_df)


### C:\repository\HAI_Python\app\postgreSQL\drop_national_heritage.py ###
import psycopg2

# PostgreSQL ì—°ê²° ì„¤ì •
try:
    connection = psycopg2.connect(
        host="localhost",  # PostgreSQL ì„œë²„ ì£¼ì†Œ
        database="heritage_db",  # ì‚¬ìš©í•  ë°ì´í„°ë² ì´ìŠ¤ ì´ë¦„
        user="postgres",  # ì‚¬ìš©ìëª…
        password="iam@123"  # ì„¤ì •í•œ ë¹„ë°€ë²ˆí˜¸
    )
    connection.autocommit = True  # ìë™ ì»¤ë°‹ ì„¤ì •
    cursor = connection.cursor()

    print("PostgreSQLì— ì„±ê³µì ìœ¼ë¡œ ì—°ê²°ë˜ì—ˆìŠµë‹ˆë‹¤.")

    # ê¸°ì¡´ í…Œì´ë¸” ì‚­ì œ
    cursor.execute("DROP TABLE IF EXISTS national_heritage;")
    print("í…Œì´ë¸”ì´ ì„±ê³µì ìœ¼ë¡œ ì‚­ì œë˜ì—ˆìŠµë‹ˆë‹¤.")

except Exception as error:
    print(f"PostgreSQL ì—°ê²° ë˜ëŠ” í…Œì´ë¸” ì‚­ì œ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {error}")

finally:
    # ì—°ê²° ë‹«ê¸°
    if cursor:
        cursor.close()
    if connection:
        connection.close()
    print("PostgreSQL ì—°ê²°ì´ ë‹«í˜”ìŠµë‹ˆë‹¤.")


### C:\repository\HAI_Python\app\postgreSQL\fetch_and_preprocess.py ###
import os
import psycopg2
import re
import kss

# 1. PostgreSQLì—ì„œ íŠ¹ì • ë°ì´í„° ê°€ì ¸ì˜¤ê¸°
try:
    conn = psycopg2.connect(
        host="localhost",
        database="heritage_db",
        user="postgres",
        password="iam@123"
    )
    cursor = conn.cursor()
    cursor.execute("SELECT ccbaAsno, content FROM national_heritage WHERE ccbaAsno = 340000 LIMIT 1")  # íŠ¹ì • IDì˜ ë°ì´í„°ë§Œ ê°€ì ¸ì˜¤ê¸°
    row = cursor.fetchone()
    conn.close()
except Exception as e:
    print("PostgreSQL ì—°ê²° ì‹¤íŒ¨:", e)
    exit()

# 2. ë°ì´í„° ì „ì²˜ë¦¬
if row:
    original_id, text = row

    # 3. ì „ì²˜ë¦¬ ë‹¨ê³„ - í•œêµ­ì–´ íŠ¹ì„±ì— ë§ëŠ” ì „ì²˜ë¦¬ ì ìš©
    # ë¬¸ì¥ ë‹¨ìœ„ë¡œ ë¶„ë¦¬ (KSS ì‚¬ìš©)
    sentences = kss.split_sentences(text)

    # ê° ë¬¸ì¥ì—ì„œ êµ¬ë‘ì  ì œê±°
    processed_sentences = [re.sub(r'[\.,!?]', '', sentence) for sentence in sentences]

    # ì „ì²˜ë¦¬ëœ ê²°ê³¼ ì¶œë ¥
    print(f"ccbaAsno: {original_id}")
    print("ì „ì²˜ë¦¬ëœ í…ìŠ¤íŠ¸:")
    for sentence in processed_sentences:
        print(sentence)
else:
    print("ë°ì´í„°ë¥¼ ê°€ì ¸ì˜¤ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.")


### C:\repository\HAI_Python\app\postgreSQL\push_data_with_csv.py ###
# í•„ìš”í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬ ë¶ˆëŸ¬ì˜¤ê¸°
import psycopg2
import pandas as pd
from psycopg2 import sql

# 1. PostgreSQL ì—°ê²° ì„¤ì •
try:
    connection = psycopg2.connect(
        host="localhost",  # PostgreSQL ì„œë²„ ì£¼ì†Œ
        database="heritage_db",  # ì‚¬ìš©í•  ë°ì´í„°ë² ì´ìŠ¤ ì´ë¦„
        user="postgres",  # ì‚¬ìš©ìëª…
        password="iam@123"  # ì„¤ì •í•œ ë¹„ë°€ë²ˆí˜¸
    )
    connection.autocommit = True  # ìë™ ì»¤ë°‹ ì„¤ì •
    cursor = connection.cursor()

    print("PostgreSQLì— ì„±ê³µì ìœ¼ë¡œ ì—°ê²°ë˜ì—ˆìŠµë‹ˆë‹¤.")

    # 2. í…Œì´ë¸” ì¡´ì¬ ì—¬ë¶€ í™•ì¸ í›„ ìƒì„± (ê¸°ì¡´ì— í…Œì´ë¸”ì´ ì—†ë‹¤ë©´ ìƒì„±)
    cursor.execute('''
        CREATE TABLE IF NOT EXISTS national_heritage (
            id SERIAL PRIMARY KEY,  -- ê¸°ë³¸ í‚¤ë¡œ ì‚¬ìš©í•  id ì»¬ëŸ¼ ì¶”ê°€
            ccbaAsno NUMERIC,
            ccbaKdcd INTEGER,
            ccbaCtcd VARCHAR(10),
            ccbaCpno VARCHAR(20),
            ccbaMnm1 VARCHAR(255),
            ccbaMnm2 VARCHAR(255),
            ccmaName VARCHAR(100),
            ccbaCtcdNm VARCHAR(100),
            ccsiName VARCHAR(100),
            ccceName VARCHAR(255),
            imageUrl VARCHAR(500),
            content TEXT,
            ccbaLcad VARCHAR(500)
        );
    ''')
    print("í…Œì´ë¸”ì´ ì„±ê³µì ìœ¼ë¡œ ìƒì„±ë˜ì—ˆê±°ë‚˜ ì´ë¯¸ ì¡´ì¬í•©ë‹ˆë‹¤.")

    # 4. CSV íŒŒì¼ ë¶ˆëŸ¬ì˜¤ê¸°
    csv_file_path = 'updated01_national_heritage_full_data.csv'
    df = pd.read_csv(csv_file_path, encoding='utf-8') 
    df.to_csv('updated01_national_heritage_utf8.csv', index=False, encoding='utf-8')

    # 5. ë°ì´í„° ì‚½ì… í•¨ìˆ˜ ì •ì˜
    def insert_data(row):
        try:
            # ê´€ë¦¬ë²ˆí˜¸ë¥¼ ë¬¸ìì—´ë¡œ ë³€í™˜í•˜ì—¬ ì‚½ì…
            ccbaAsno = str(row['ê´€ë¦¬ë²ˆí˜¸(ccbaAsno)']) if not pd.isna(row['ê´€ë¦¬ë²ˆí˜¸(ccbaAsno)']) else None

            insert_query = sql.SQL('''
                INSERT INTO national_heritage (ccbaAsno, ccbaKdcd, ccbaCtcd, ccbaCpno, ccbaMnm1, ccbaMnm2, 
                                              ccmaName, ccbaCtcdNm, ccsiName, ccceName, imageUrl, content, ccbaLcad)
                VALUES (%s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s)
            ''')
            cursor.execute(insert_query, (
                ccbaAsno, row['ì¤‘ëª©ì½”ë“œ(ccbaKdcd)'], row['ì‹œë„ì½”ë“œ(ccbaCtcd)'], row['êµ­ê°€ìœ ì‚°ì—°ê³„ë²ˆí˜¸(ccbaCpno)'],
                row['êµ­ê°€ìœ ì‚°ëª…_êµ­ë¬¸(ccbaMnm1)'], row['êµ­ê°€ìœ ì‚°ëª…_í•œì(ccbaMnm2)'], row['êµ­ê°€ìœ ì‚°ì¢…ëª©(ccmaName)'],
                row['ì‹œë„ëª…(ccbaCtcdNm)'], row['ì‹œêµ°êµ¬ëª…(ccsiName)'], row['ì‹œëŒ€(ccceName)'],
                row['ë©”ì¸ì´ë¯¸ì§€URL(imageUrl)'], row['ë‚´ìš©(content)'], row['ì†Œì¬ì§€ìƒì„¸(ccbaLcad)']
            ))
        except Exception as e:
            print(f"ë°ì´í„° ì‚½ì… ì˜¤ë¥˜: {e}")

    # 6. ë°ì´í„° ì‚½ì… ë£¨í”„ (ê° í–‰ì„ ë°ì´í„°ë² ì´ìŠ¤ì— ì‚½ì…)
    for _, row in df.iterrows():
        insert_data(row)

    print("ëª¨ë“  ë°ì´í„°ê°€ ì„±ê³µì ìœ¼ë¡œ ì‚½ì…ë˜ì—ˆìŠµë‹ˆë‹¤.")

except Exception as error:
    print(f"PostgreSQL ì—°ê²° ë˜ëŠ” ë°ì´í„° ì‚½ì… ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {error}")

finally:
    # 7. ì—°ê²° ë‹«ê¸°
    if cursor:
        cursor.close()
    if connection:
        connection.close()
    print("PostgreSQL ì—°ê²°ì´ ë‹«í˜”ìŠµë‹ˆë‹¤.")


### C:\repository\HAI_Python\app\postgreSQL\rag_evaluation_dataset.py ###
import os
import re
import kss
import csv
import psycopg2

# 1. PostgreSQLì—ì„œ ë°ì´í„° ê°€ì ¸ì˜¤ê¸°
try:
    conn = psycopg2.connect(
        host="localhost",
        database="heritage_db",
        user="postgres",
        password="iam@123"
    )
    cursor = conn.cursor()
    cursor.execute("SELECT id, content FROM national_heritage")  # í…Œì´ë¸”ì—ì„œ contentì™€ ê³ ìœ  ì‹ë³„ìë¥¼ ê°€ì ¸ì˜¤ê¸°
    rows = cursor.fetchall()
except Exception as e:
    print("PostgreSQL ì—°ê²° ì‹¤íŒ¨:", e)
    exit()

# 2. ë°ì´í„° ì „ì²˜ë¦¬ ë° ë¬¸ì¥ë³„ ë¶„ë¦¬ í›„ ì €ì¥
document_segments = []

for row in rows:
    id, text = row

    # ë¬¸ì¥ ë‹¨ìœ„ë¡œ ë¶„ë¦¬ (KSS ì‚¬ìš©)
    sentences = kss.split_sentences(text)

    # ê° ë¬¸ì¥ì—ì„œ êµ¬ë‘ì  ì œê±° ë° ì €ì¥í•  ë°ì´í„° êµ¬ì„±
    for sentence_id, sentence in enumerate(sentences):
        processed_sentence = re.sub(r'[\.,!?]', '', sentence)
        document_segments.append((id, sentence_id, processed_sentence))

# 3. ì „ì²˜ë¦¬ëœ ë¬¸ì¥ë³„ ë°ì´í„°ë¥¼ íŒŒì¼ë¡œ ì €ì¥
output_file = os.path.join(os.path.dirname(os.path.realpath(__file__)), "national_heritage_sentences.csv")

try:
    with open(output_file, mode='w', newline='', encoding='utf-8') as file:
        writer = csv.writer(file)
        writer.writerow(["id", "sentence_id", "sentence"])
        writer.writerows(document_segments)
    print(f"ë¬¸ì¥ ë°ì´í„°ë¥¼ '{output_file}' íŒŒì¼ë¡œ ì €ì¥í–ˆìŠµë‹ˆë‹¤.")
except Exception as e:
    print("ë¬¸ì¥ ë°ì´í„° íŒŒì¼ ì €ì¥ ì‹¤íŒ¨:", e)


### C:\repository\HAI_Python\app\ragas\context_entities_recall_evaluation.py ###
import os
import json
from ragas.dataset_schema import EvaluationDataset, SingleTurnSample
from ragas.evaluation import evaluate
from ragas.metrics._context_entities_recall import ContextEntityRecall
import pandas as pd
from dotenv import load_dotenv

# .env íŒŒì¼ ë¡œë“œ
load_dotenv()

# OpenAI API í‚¤ ì„¤ì •
OPENAI_API_KEY = os.getenv("OPENAI_API_KEY")
if not OPENAI_API_KEY:
    raise ValueError("OPENAI_API_KEYê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. .env íŒŒì¼ì„ í™•ì¸í•´ì£¼ì„¸ìš”.")

# JSON íŒŒì¼ ì½ê¸°
with open("retrieval_results.json", "r", encoding="utf-8") as f:
    raw_data = json.load(f)

# ë°ì´í„° ë³€í™˜
samples = []
for item in raw_data:
    samples.append(
        SingleTurnSample(
            user_input=item["query"],
            response=None,  # ì‘ë‹µì€ Noneìœ¼ë¡œ ì„¤ì •
            reference=item["ground_truth"],
            retrieved_contexts=[doc["text_segment"] for doc in item["retrieved_documents"]]
        )
    )

# í‰ê°€ ë°ì´í„°ì…‹ ìƒì„±
dataset = EvaluationDataset(samples=samples)

# Context Entities Recall ë©”íŠ¸ë¦­ ì •ì˜
metrics = [ContextEntityRecall()]

# í‰ê°€ ì‹¤í–‰
results = evaluate(
    dataset=dataset,
    metrics=metrics,
    show_progress=True
)

# ê²°ê³¼ë¥¼ pandas ë°ì´í„°í”„ë ˆì„ìœ¼ë¡œ ë³€í™˜
final_results = results.to_pandas()

# JSON ì €ì¥
json_output_file = "context_entities_recall_evaluation_results.json"
final_results.to_json(json_output_file, orient="records", force_ascii=False, indent=4)
print(f"ìµœì¢… í‰ê°€ ê²°ê³¼ê°€ {json_output_file}ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")

# CSV ì €ì¥
csv_output_file = "context_entities_recall_evaluation_results.csv"
final_results.to_csv(csv_output_file, index=False, encoding="utf-8-sig")
print(f"ìµœì¢… í‰ê°€ ê²°ê³¼ê°€ {csv_output_file}ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")

# ê²°ê³¼ ì¶œë ¥
print("Context Entities Recall ê²°ê³¼:")
print(results)


### C:\repository\HAI_Python\app\ragas\context_precision_evaluation.py ###
import os
import json
import faiss
import pickle
import numpy as np
import pandas as pd
from sentence_transformers import SentenceTransformer
from ragas.dataset_schema import EvaluationDataset, SingleTurnSample
from ragas.evaluation import evaluate
from ragas.metrics._context_precision import ContextPrecision
from dotenv import load_dotenv
from rank_bm25 import BM25Okapi
from eunjeon import Mecab

# í™˜ê²½ ë³€ìˆ˜ ë¡œë“œ
load_dotenv()

# ì „ì—­ ë³€ìˆ˜ ì´ˆê¸°í™”
embedding_model = SentenceTransformer("jhgan/ko-sroberta-multitask")
mecab = Mecab()

# FAISS ì¸ë±ìŠ¤ ë° ë©”íƒ€ë°ì´í„° ë¡œë“œ
base_dir = os.path.dirname(os.path.realpath(__file__))
index_file = os.path.join(base_dir, "../FAISS/Index/jhgan_cosine_index.bin")
metadata_file = os.path.join(base_dir, "../FAISS/Metadata/jhgan_metadata.pkl")
bm25_index_file = os.path.join(base_dir, "../FAISS/Metadata/bm25_index.pkl")

with open(metadata_file, "rb") as f:
    metadata = pickle.load(f)
documents = [entry["ë‚´ìš©"] for entry in metadata]

index = faiss.read_index(index_file)

if os.path.exists(bm25_index_file):
    with open(bm25_index_file, "rb") as f:
        bm25 = pickle.load(f)
else:
    tokenized_documents = [[word for word, pos in mecab.pos(doc) if pos in ['NNP', 'NNG', 'NP', 'VV', 'VA']] for doc in documents]
    bm25 = BM25Okapi(tokenized_documents)

# í•˜ì´ë¸Œë¦¬ë“œ ì„œì¹˜ í•¨ìˆ˜
def hybrid_search(query: str, top_k: int = 5, alpha: float = 0.5, normalization_method: str = "min_max"):
    query_tokens = [word for word, pos in mecab.pos(query) if pos in ['NNP', 'NNG', 'NP', 'VV', 'VA']]
    bm25_scores = bm25.get_scores(query_tokens)

    query_embedding = embedding_model.encode(query).astype("float32").reshape(1, -1)
    faiss_distances, faiss_indices = index.search(query_embedding, len(metadata))
    faiss_scores = -faiss_distances[0]

    if normalization_method == "min_max":
        bm25_scores = (bm25_scores - np.min(bm25_scores)) / (np.max(bm25_scores) - np.min(bm25_scores))
        faiss_scores = (faiss_scores - np.min(faiss_scores)) / (np.max(faiss_scores) - np.min(faiss_scores))

    final_scores = alpha * bm25_scores + (1 - alpha) * faiss_scores
    sorted_indices = np.argsort(-final_scores)[:top_k]

    results = [{"text_segment": metadata[idx]["ë‚´ìš©"], "score": final_scores[idx]} for idx in sorted_indices]
    return results

# ë°ì´í„°ì…‹ ë¡œë“œ
with open('national_heritage_qa_dataset_converted.json', 'r', encoding='utf-8') as f:
    data = json.load(f)

retrieval_results = []
samples = []

for item in data:
    query = item["question"]
    ground_truth = item["ground_truth"]

    try:
        retrieved_documents = hybrid_search(query, top_k=5)
        retrieval_results.append({
            "query": query,
            "retrieved_documents": retrieved_documents,
            "ground_truth": ground_truth
        })
        samples.append(
            SingleTurnSample(
                user_input=query,
                retrieved_contexts=[doc["text_segment"] for doc in retrieved_documents],
                reference=ground_truth,
            )
        )
    except Exception as e:
        print(f"ê²€ìƒ‰ ì‹¤íŒ¨: {e}")

# ê²€ìƒ‰ ê²°ê³¼ë¥¼ JSON íŒŒì¼ë¡œ ì €ì¥
with open("retrieval_results.json", "w", encoding="utf-8") as f:
    json.dump(retrieval_results, f, ensure_ascii=False, indent=4)

# ê²€ìƒ‰ ê²°ê³¼ë¥¼ CSV íŒŒì¼ë¡œ ì €ì¥
retrieval_results_df = pd.DataFrame(retrieval_results)
retrieval_results_df.to_csv("retrieval_results.csv", index=False, encoding="utf-8-sig")

# RAGAS ë°ì´í„°ì…‹ ìƒì„±
dataset = EvaluationDataset(samples=samples)

metrics = [ContextPrecision()]
results = evaluate(dataset=dataset, metrics=metrics, show_progress=True)

# í‰ê°€ ê²°ê³¼ë¥¼ JSON íŒŒì¼ë¡œ ì €ì¥
results.to_json("evaluation_results.json")

# í‰ê°€ ê²°ê³¼ë¥¼ CSV íŒŒì¼ë¡œ ì €ì¥
results_df = results.to_pandas()
results_df.to_csv("evaluation_results.csv", index=False, encoding="utf-8-sig")

print("ê²°ê³¼ê°€ JSON ë° CSV íŒŒì¼ë¡œ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")
print("í‰ê°€ ê²°ê³¼:")
print(results)


### C:\repository\HAI_Python\app\ragas\context_recall_evaluation.py ###
import os
import json
from ragas.dataset_schema import EvaluationDataset, SingleTurnSample
from ragas.evaluation import evaluate
from ragas.metrics._context_recall import ContextRecall
import pandas as pd
from dotenv import load_dotenv

# .env íŒŒì¼ ë¡œë“œ
load_dotenv()

# OpenAI API í‚¤ ì„¤ì •
OPENAI_API_KEY = os.getenv("OPENAI_API_KEY")
if not OPENAI_API_KEY:
    raise ValueError("OPENAI_API_KEYê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. .env íŒŒì¼ì„ í™•ì¸í•´ì£¼ì„¸ìš”.")

# JSON íŒŒì¼ ì½ê¸°
with open("retrieval_results.json", "r", encoding="utf-8") as f:
    raw_data = json.load(f)

# ë°ì´í„° ë³€í™˜
samples = []
for item in raw_data:
    samples.append(
        SingleTurnSample(
            user_input=item["query"],
            response=None,  # ì‘ë‹µì€ Noneìœ¼ë¡œ ì„¤ì •
            reference=item["ground_truth"],
            retrieved_contexts=[doc["text_segment"] for doc in item["retrieved_documents"]]
        )
    )

# í‰ê°€ ë°ì´í„°ì…‹ ìƒì„±
dataset = EvaluationDataset(samples=samples)

# ì»¨í…ìŠ¤íŠ¸ ë¦¬ì½œ ë©”íŠ¸ë¦­ ì •ì˜
metrics = [ContextRecall()]

# í‰ê°€ ì‹¤í–‰
results = evaluate(
    dataset=dataset,
    metrics=metrics,
    show_progress=True
)

final_results = results.to_pandas()
 
# JSON ì €ì¥
json_output_file = "context_recall_evaluation_results.json"
final_results.to_json(json_output_file, orient="records", force_ascii=False, indent=4)
print(f"ìµœì¢… í‰ê°€ ê²°ê³¼ê°€ {json_output_file}ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")

# CSV ì €ì¥
csv_output_file = "context_recall_evaluation_results.csv"
final_results.to_csv(csv_output_file, index=False, encoding="utf-8-sig")
print(f"ìµœì¢… í‰ê°€ ê²°ê³¼ê°€ {csv_output_file}ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")

# ê²°ê³¼ ì¶œë ¥
print("Context Recall ê²°ê³¼:")
print(results)

### C:\repository\HAI_Python\app\ragas\NonLLMContextPrecisionWithReferenc.py ###
import json
import csv
import faiss
import pickle
import numpy as np
from sentence_transformers import SentenceTransformer
from ragas.dataset_schema import EvaluationDataset, SingleTurnSample
from ragas.evaluation import evaluate
from ragas.metrics._context_precision import NonLLMContextPrecisionWithReference

# FAISS ì¸ë±ìŠ¤ ë° ë©”íƒ€ë°ì´í„° íŒŒì¼ ë¡œë“œ
index_file = "../FAISS/Index/jhgan_cosine_index.bin"
metadata_file = "../FAISS/Metadata/jhgan_metadata.pkl"

try:
    index = faiss.read_index(index_file)
    with open(metadata_file, "rb") as f:
        metadata = pickle.load(f)
    print("FAISS ì¸ë±ìŠ¤ ë° ë©”íƒ€ë°ì´í„° ë¡œë“œ ì„±ê³µ!")
except Exception as e:
    print(f"FAISS ì¸ë±ìŠ¤ ë° ë©”íƒ€ë°ì´í„° ë¡œë“œ ì‹¤íŒ¨: {e}")
    exit()

# SentenceTransformer ëª¨ë¸ ë¡œë“œ
embedding_model = SentenceTransformer("jhgan/ko-sroberta-multitask")

# ê²€ìƒ‰ í•¨ìˆ˜
def search_faiss_index(query: str, top_k: int = 5):
    """
    FAISS ì¸ë±ìŠ¤ë¥¼ ì‚¬ìš©í•˜ì—¬ ì½”ì‚¬ì¸ ìœ ì‚¬ë„ ê¸°ë°˜ ê²€ìƒ‰ ìˆ˜í–‰.
    """
    query_embedding = embedding_model.encode(query).astype("float32").reshape(1, -1)
    distances, indices = index.search(query_embedding, top_k)

    results = []
    for i, idx in enumerate(indices[0]):
        if idx < len(metadata):
            results.append({
                "text_segment": metadata[idx]["text_segment"],
                "distance": float(distances[0][i])  # float32 -> float ë³€í™˜
            })
    return results

# ë°ì´í„°ì…‹ ë¡œë“œ
with open('national_heritage_qa_dataset_converted.json', 'r', encoding='utf-8') as f:
    data = json.load(f)

# ê²€ìƒ‰ ê²°ê³¼ ì €ì¥ìš© ë¦¬ìŠ¤íŠ¸
retrieval_results = []

# ë°ì´í„°ì…‹ ë³€í™˜
samples = []
for item in data:
    query = item["question"]  # ì‚¬ìš©ìê°€ ë¬»ëŠ” ì§ˆë¬¸
    ground_truth = item["ground_truth"]  # ì •ë‹µ (ë‹¨ì¼ ë¬¸ìì—´)

    # FAISSë¥¼ ì‚¬ìš©í•´ ê²€ìƒ‰ ìˆ˜í–‰
    retrieved_documents = search_faiss_index(query, top_k=5)

    # ê²€ìƒ‰ ê²°ê³¼ ì €ì¥
    retrieval_results.append({
        "query": query,
        "retrieved_documents": retrieved_documents,
        "ground_truth": ground_truth
    })

    # RAGAS SingleTurnSample ê°ì²´ ìƒì„±
    samples.append(
        SingleTurnSample(
            user_input=query,  # ì§ˆë¬¸
            retrieved_contexts=[doc["text_segment"] for doc in retrieved_documents],  # ê²€ìƒ‰ëœ ê²°ê³¼
            reference=ground_truth,  # ì •ë‹µ
            reference_contexts=[ground_truth],  # ì°¸ì¡° ì»¨í…ìŠ¤íŠ¸ ì¶”ê°€
        )
    )

# ê²€ìƒ‰ ê²°ê³¼ë¥¼ JSON íŒŒì¼ë¡œ ì €ì¥
output_json_file = "retrieval_results_NonLLMContextPrecisionWithReferenc.json"
with open(output_json_file, "w", encoding="utf-8") as f:
    json.dump(retrieval_results, f, ensure_ascii=False, indent=4)
print(f"ê²€ìƒ‰ ê²°ê³¼ê°€ {output_json_file}ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")

# RAGAS ë°ì´í„°ì…‹ ìƒì„±
dataset = EvaluationDataset(samples=samples)

# ê²€ìƒ‰ ì„±ëŠ¥ í‰ê°€ ë©”íŠ¸ë¦­ ì •ì˜
metrics = [NonLLMContextPrecisionWithReference()]

# í‰ê°€ ì‹¤í–‰
try:
    results = evaluate(
        dataset=dataset,
        metrics=metrics,
        llm=None,  # LLMì„ ì‚¬ìš©í•˜ì§€ ì•ŠìŒ
        embeddings=None,  # ì„ë² ë”©ì„ ì‚¬ìš©í•˜ì§€ ì•ŠìŒ
        show_progress=True
    )
    print("RAGAS ê²€ìƒ‰ ì„±ëŠ¥ í‰ê°€ ê²°ê³¼:")
    print(results)

    # í‰ê°€ ê²°ê³¼ë¥¼ JSON ë° CSVë¡œ ì €ì¥
    output_results_json_file = "evaluation_results_NonLLMContextPrecisionWithReferenc.json"
    output_results_csv_file = "evaluation_results_NonLLMContextPrecisionWithReferenc.csv"

    # ê²°ê³¼ë¥¼ ì§ì ‘ ì²˜ë¦¬
    results_dict = {metric.name: results[metric.name] for metric in metrics}

    # JSON ì €ì¥
    with open(output_results_json_file, "w", encoding="utf-8") as f:
        json.dump(results_dict, f, ensure_ascii=False, indent=4)
    print(f"í‰ê°€ ê²°ê³¼ê°€ {output_results_json_file}ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")

    # CSV ì €ì¥
    with open(output_results_csv_file, mode="w", newline="", encoding="utf-8") as f:
        writer = csv.writer(f)
        writer.writerow(["Metric", "Score"])  # í—¤ë” ì‘ì„±
        for metric, score in results_dict.items():
            writer.writerow([metric, score])
    print(f"í‰ê°€ ê²°ê³¼ê°€ {output_results_csv_file}ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")
except Exception as e:
    print(f"í‰ê°€ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")


### C:\repository\HAI_Python\app\ragas\results\context_entities_recall_results_processing.py ###
import json

# JSON íŒŒì¼ ì½ê¸°
with open("context_entities_recall_evaluation_results.json", "r", encoding="utf-8") as f:
    data = json.load(f)

# context_entity_recall ê°’ ë°ì´í„° ê°’ ë³„ë¡œ í•„í„°ë§
# ì •ìƒì ìœ¼ë¡œ ì²˜ë¦¬ê°€ ë˜ì–´ context_entity_recall ê°’ì´ 0ì´ ì•„ë‹Œ ê²ƒë“¤
filtered_data = [item for item in data if item.get("context_entity_recall") is not None and item["context_entity_recall"] > 0]

# ë¹„ì •ìƒì ìœ¼ë¡œ ì²˜ë¦¬ê°€ ë˜ì–´ context_entity_recall ê°’ì´ Noneì¸ ê²ƒë“¤
zero_data = [item for item in data if item.get("context_entity_recall") == 0]

# ë¹„ì •ìƒì ìœ¼ë¡œ ì²˜ë¦¬ê°€ ë˜ì–´ context_entity_recall ê°’ì´ 0ì¸ ê²ƒë“¤
none_data = [item for item in data if item.get("context_entity_recall") is None]

# í‰ê·  ê³„ì‚°
if filtered_data:
    average_precision1 = sum(item["context_entity_recall"] for item in filtered_data) / len(filtered_data)
    average_precision2 = sum(item["context_entity_recall"] for item in filtered_data) / (len(filtered_data) + len(zero_data))
    average_precision3 = sum(item["context_entity_recall"] for item in filtered_data) / (len(filtered_data) + len(zero_data) + len(none_data))
else:
    average_precision = 0
    print(f"í‰ê· ì„ ê³„ì‚°í•˜ëŠ” ë„ì¤‘ ì˜¤ë¥˜ ë°œìƒ: filtered_dataê°€ ì—†ìŠµë‹ˆë‹¤. \nfiltered_dataì˜ ê¸¸ì´: {len(filtered_data)}")

# ê²°ê³¼ ì¶œë ¥
print(f"0ì´ìƒì˜ context_entity_recall í‰ê· : {average_precision1:.4f}")
print(f"0ì„ í¬í•¨í•œ context_entity_recall í‰ê· : {average_precision2:.4f}")
print(f"ëª¨ë“  context_entity_recall í‰ê· : {average_precision3:.4f}")
print(f"ì „ì²´ ë°ì´í„° ìˆ˜: {len(filtered_data)+len(zero_data) + len(none_data)}")
print(f"context_entity_recall ê°’ì´ 0ì¸ ë°ì´í„° ìˆ˜: {len(zero_data)}")
print(f"context_entity_recall ê°’ì´ Noneì¸ ë°ì´í„° ìˆ˜: {len(none_data)}")
print(f"ì œì™¸ëœ ë°ì´í„° ìˆ˜: {len(zero_data) + len(none_data)}")


excluded_data = zero_data + none_data

# ì œì™¸ëœ ë°ì´í„° ì €ì¥
excluded_file = "context_entities_recall_evaluation_results_excluded_data.json"
with open(excluded_file, "w", encoding="utf-8") as f:
    json.dump(excluded_data, f, ensure_ascii=False, indent=4)

print(f"ì œì™¸ëœ ë°ì´í„°ê°€ '{excluded_file}' íŒŒì¼ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")


### C:\repository\HAI_Python\app\ragas\results\context_precision_results_processing.py ###
import json

# JSON íŒŒì¼ ì½ê¸°
with open("evaluation_results_ContextPrecision_pandas.json", "r", encoding="utf-8") as f:
    data = json.load(f)

# context_precision ê°’ ë°ì´í„° ê°’ ë³„ë¡œ í•„í„°ë§
# ì •ìƒì ìœ¼ë¡œ ì²˜ë¦¬ê°€ ë˜ì–´ context_precision ê°’ì´ 0ì´ ì•„ë‹Œ ê²ƒë“¤
filtered_data = [item for item in data if item.get("context_precision") is not None and item["context_precision"] > 0]

# ë¹„ì •ìƒì ìœ¼ë¡œ ì²˜ë¦¬ê°€ ë˜ì–´ context_precision ê°’ì´ 0ì¸ ê²ƒë“¤
zero_data = [item for item in data if item.get("context_precision") is None]

# ë¹„ì •ìƒì ìœ¼ë¡œ ì²˜ë¦¬ê°€ ë˜ì–´ context_precision ê°’ì´ Noneì¸ ê²ƒë“¤
none_data = [item for item in data if item.get("context_precision") is item["context_precision"] == 0]

# í‰ê·  ê³„ì‚°
if filtered_data:
    average_precision1 = sum(item["context_precision"] for item in filtered_data) / len(filtered_data)
    average_precision2 = sum(item["context_precision"] for item in filtered_data) / (len(filtered_data) + len(zero_data))
    average_precision3 = sum(item["context_precision"] for item in filtered_data) / (len(filtered_data) + len(zero_data) + len(none_data))
else:
    average_precision = 0
    print(f"í‰ê· ì„ ê³„ì‚°í•˜ëŠ” ë„ì¤‘ ì˜¤ë¥˜ ë°œìƒ: filtered_dataê°€ ì—†ìŠµë‹ˆë‹¤. \nfiltered_dataì˜ ê¸¸ì´: {len(filtered_data)}")

# ê²°ê³¼ ì¶œë ¥
print(f"0ì´ìƒì˜ context_precision í‰ê· : {average_precision1:.4f}")
print(f"0ì„ í¬í•¨í•œ context_precision í‰ê· : {average_precision2:.4f}")
print(f"ëª¨ë“  context_precision í‰ê· : {average_precision3:.4f}")
print(f"ì „ì²´ ë°ì´í„° ìˆ˜: {len(filtered_data)+len(zero_data) + len(none_data)}")
print(f"ì œì™¸ëœ ë°ì´í„° ìˆ˜: {len(zero_data) + len(none_data)}")


excluded_data = zero_data + none_data

# ì œì™¸ëœ ë°ì´í„° ì €ì¥
excluded_file = "evaluation_results_ContextPrecision_pandas_excluded_data.json"
with open(excluded_file, "w", encoding="utf-8") as f:
    json.dump(excluded_data, f, ensure_ascii=False, indent=4)

print(f"ì œì™¸ëœ ë°ì´í„°ê°€ '{excluded_file}' íŒŒì¼ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")


### C:\repository\HAI_Python\app\ragas\results\context_recall_results_processing.py ###
import json

# JSON íŒŒì¼ ì½ê¸°
with open("context_recall_evaluation_results.json", "r", encoding="utf-8") as f:
    data = json.load(f)

# context_recall ê°’ ë°ì´í„° ê°’ ë³„ë¡œ í•„í„°ë§
filtered_data = [item for item in data if item.get("context_recall", 0) > 0]
zero_data = [item for item in data if item.get("context_recall", 0) == 0]

# í‰ê·  ê³„ì‚°
if filtered_data:
    average_precision1 = sum(item["context_recall"] for item in filtered_data) / len(filtered_data)
    average_precision2 = sum(item["context_recall"] for item in filtered_data) / (len(filtered_data) + len(zero_data))
else:
    average_precision1 = average_precision2 = 0

# ê²°ê³¼ ì¶œë ¥
print(f"0ì´ìƒì˜ context_recall í‰ê· : {average_precision1:.4f}")
print(f"0ì„ í¬í•¨í•œ context_recall í‰ê· : {average_precision2:.4f}")
print(f"ì „ì²´ ë°ì´í„° ìˆ˜: {len(filtered_data) + len(zero_data)}")
print(f"context_recall ê°’ì´ 0ì¸ ë°ì´í„° ìˆ˜: {len(zero_data)}")
print(f"context_recall ê°’ì´ 0ì´ ì•„ë‹Œ ë°ì´í„° ìˆ˜: {len(filtered_data)}")

# ì œì™¸ëœ ë°ì´í„° ì €ì¥ (0ì¸ ë°ì´í„°)
excluded_file = "context_recall_evaluation_results_excluded_data.json"
with open(excluded_file, "w", encoding="utf-8") as f:
    json.dump(zero_data, f, ensure_ascii=False, indent=4)

print(f"ì œì™¸ëœ ë°ì´í„°ê°€ '{excluded_file}' íŒŒì¼ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")


### C:\repository\HAI_Python\app\routers\auth_router.py ###
from fastapi import APIRouter, Depends, HTTPException, status
from fastapi.security import OAuth2PasswordBearer, OAuth2PasswordRequestForm
from pydantic import BaseModel
from typing import Optional
from jose import JWTError, jwt
from datetime import datetime, timedelta
from passlib.context import CryptContext
from sqlalchemy.orm import Session
from sqlalchemy import Column, Integer, String, DateTime, create_engine
from sqlalchemy.ext.declarative import declarative_base
from sqlalchemy.exc import SQLAlchemyError

# ë°ì´í„°ë² ì´ìŠ¤ ì„¤ì •
DATABASE_URL = "postgresql+psycopg2://postgres:iam%40123@localhost/heritage_db"
engine = create_engine(DATABASE_URL)
Base = declarative_base()

# JWT ì„¤ì •
SECRET_KEY = "your_secret_key"  # ì‹¤ì œ ì„œë¹„ìŠ¤ì—ì„  ê°•ë ¥í•œ ë¹„ë°€ í‚¤ ì‚¬ìš©
ALGORITHM = "HS256"
ACCESS_TOKEN_EXPIRE_MINUTES = 60

# ë¹„ë°€ë²ˆí˜¸ í•´ì‹±
pwd_context = CryptContext(schemes=["bcrypt"], deprecated="auto")
oauth2_scheme = OAuth2PasswordBearer(tokenUrl="token")

# SQLAlchemy ì„¸ì…˜
from sqlalchemy.orm import sessionmaker
SessionLocal = sessionmaker(autocommit=False, autoflush=False, bind=engine)

# ìœ ì € ëª¨ë¸ ì •ì˜
class User(Base):
    __tablename__ = "users"

    id = Column(Integer, primary_key=True, index=True)
    username = Column(String(50), unique=True, index=True, nullable=False)
    email = Column(String(100), unique=True, index=True, nullable=False)
    hashed_password = Column(String(200), nullable=False)
    created_at = Column(DateTime, default=datetime.utcnow)

# ë°ì´í„°ë² ì´ìŠ¤ í…Œì´ë¸” ìƒì„±
Base.metadata.create_all(bind=engine)

# Pydantic ëª¨ë¸ ì •ì˜
class UserRegister(BaseModel):
    username: str
    email: str
    password: str

class Token(BaseModel):
    access_token: str
    token_type: str

# ìœ í‹¸ë¦¬í‹° í•¨ìˆ˜
def verify_password(plain_password, hashed_password):
    return pwd_context.verify(plain_password, hashed_password)

def get_password_hash(password):
    return pwd_context.hash(password)

def create_access_token(data: dict, expires_delta: Optional[timedelta] = None):
    to_encode = data.copy()
    if expires_delta:
        expire = datetime.utcnow() + expires_delta
    else:
        expire = datetime.utcnow() + timedelta(minutes=15)
    to_encode.update({"exp": expire})
    encoded_jwt = jwt.encode(to_encode, SECRET_KEY, algorithm=ALGORITHM)
    return encoded_jwt

# ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²° í•¨ìˆ˜
def get_db():
    try:
        db = SessionLocal()
        yield db
    finally:
        db.close()

# ë¼ìš°í„° ìƒì„±
auth_router = APIRouter(
    tags=["ë¡œê·¸ì¸"]
)

# íšŒì›ê°€ì… ì—”ë“œí¬ì¸íŠ¸
@auth_router.post("/register")
def register(user: UserRegister, db: Session = Depends(get_db)):
    hashed_password = get_password_hash(user.password)
    db_user = User(username=user.username, email=user.email, hashed_password=hashed_password)
    db.add(db_user)
    try:
        db.commit()
        db.refresh(db_user)
    except SQLAlchemyError:
        db.rollback()
        raise HTTPException(status_code=status.HTTP_400_BAD_REQUEST, detail="Username or email already registered")
    return {"msg": "User registered successfully"}

# ë¡œê·¸ì¸ ì—”ë“œí¬ì¸íŠ¸ (í† í° ë°œê¸‰)
@auth_router.post("/token", response_model=Token)
async def login_for_access_token(form_data: OAuth2PasswordRequestForm = Depends(), db: Session = Depends(get_db)):
    user = db.query(User).filter(User.email == form_data.username).first()
    if not user or not verify_password(form_data.password, user.hashed_password):
        raise HTTPException(
            status_code=status.HTTP_401_UNAUTHORIZED,
            detail="Incorrect email or password",
            headers={"WWW-Authenticate": "Bearer"},
        )
    access_token_expires = timedelta(minutes=ACCESS_TOKEN_EXPIRE_MINUTES)
    access_token = create_access_token(
        data={"sub": user.username}, expires_delta=access_token_expires
    )
    return {"access_token": access_token, "token_type": "bearer"}

# ì¸ì¦ëœ ì‚¬ìš©ì ì •ë³´ ê°€ì ¸ì˜¤ê¸°
@auth_router.get("/profile")
async def read_users_me(token: str = Depends(oauth2_scheme), db: Session = Depends(get_db)):
    credentials_exception = HTTPException(
        status_code=status.HTTP_401_UNAUTHORIZED,
        detail="Could not validate credentials",
        headers={"WWW-Authenticate": "Bearer"},
    )
    try:
        payload = jwt.decode(token, SECRET_KEY, algorithms=[ALGORITHM])
        username: str = payload.get("sub")
        if username is None:
            raise credentials_exception
    except JWTError:
        raise credentials_exception
    user = db.query(User).filter(User.username == username).first()
    if user is None:
        raise credentials_exception
    return user


### C:\repository\HAI_Python\app\routers\book_router.py ###
from fastapi import APIRouter
from sqlalchemy import create_engine, text
from sqlalchemy.orm import sessionmaker
from sqlalchemy.exc import SQLAlchemyError

# PostgreSQL ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²° ì„¤ì •
DATABASE_URL = "postgresql+psycopg2://postgres:iam%40123@localhost/heritage_db"
engine = create_engine(DATABASE_URL)
SessionLocal = sessionmaker(autocommit=False, autoflush=False, bind=engine)

# ë¼ìš°í„° ìƒì„±
book_router = APIRouter(
    tags=["ë„ê°"]
)

# ëª¨ë“  ìœ ì‚° ë°ì´í„° ì¡°íšŒ
@book_router.get("/heritage")
def get_heritage():
    session = SessionLocal()
    try:
        result = session.execute(text("SELECT * FROM national_heritage")).fetchall()
        heritage_data = [dict(row._mapping) for row in result]
        return heritage_data
    except SQLAlchemyError as e:
        print("Database Error:", e)
        return {"error": str(e)}
    finally:
        session.close()

# í•„í„° ë°ì´í„° API ì¶”ê°€ (ì¢…ëª©, ì§€ì—­, ì‹œëŒ€ í•„í„°)
@book_router.get("/heritage/filter")
def filter_heritage(category: str = None, region: str = None, period: str = None, offset: int = 0, limit: int = 10):
    session = SessionLocal()
    try:
        query = "SELECT * FROM national_heritage WHERE 1=1"
        if category and category != "ì „ì²´":
            query += " AND ccmaName = :category"
        if region and region != "ì „ì²´":
            query += " AND ccbaCtcdNm = :region"
        if period and period != "ì „ì²´":
            query += " AND ccceName = :period"
        query += " OFFSET :offset LIMIT :limit"
        result = session.execute(
            text(query), {
                "category": category,
                "region": region,
                "period": period,
                "offset": offset,
                "limit": limit
            }
        ).fetchall()
        heritage_data = [dict(row._mapping) for row in result]
        return heritage_data
    except SQLAlchemyError as e:
        return {"error": str(e)}
    finally:
        session.close()

# íŠ¹ì • ìœ ì‚° ë°ì´í„° ì¡°íšŒ
@book_router.get("/heritage/{id}")
def get_heritage_by_id(id: int):  # idë¥¼ int íƒ€ì…ìœ¼ë¡œ ë°›ìŒ
    session = SessionLocal()
    try:
        # id ê¸°ì¤€ìœ¼ë¡œ ë°ì´í„° ê°€ì ¸ì˜¤ê¸°
        query = text("SELECT * FROM national_heritage WHERE id = :id")
        result = session.execute(query, {"id": id}).fetchone()
        if result:
            return dict(result._mapping)  # ê²°ê³¼ ë§¤í•‘ í›„ ë°˜í™˜
        else:
            return {"error": "Heritage not found"}
    except SQLAlchemyError as e:
        return {"error": str(e)}
    finally:
        session.close()



### C:\repository\HAI_Python\app\routers\chatbot_router.py ###
# app/routers/chatbot_router.py
from fastapi import APIRouter, Request
# from app.chatbot.chat_with_faiss import process_chat
from app.chatbot.chat_with_hybrid import process_chat

chatbot_router = APIRouter(
    tags=["ì±—ë´‡"]
)

# @chatbot_router.post("/chatbot")
# async def chatbot_endpoint(request: Request):
#     data = await request.json()  # JSON ë°ì´í„° íŒŒì‹±
#     input_text = data.get("input_text")
#     response = process_chat(input_text)
#     return {"response": response}

@chatbot_router.post("/chatbot")
async def chatbot_endpoint(request: Request):
    data = await request.json()  # JSON ë°ì´í„° íŒŒì‹±
    input_text = data.get("input_text")
    response = process_chat(input_text)
    return {"response": response}

### C:\repository\HAI_Python\app\routers\chat_agent_router.py ###
from fastapi import APIRouter, HTTPException
from pydantic import BaseModel
from typing import Optional
from app.agents import travel_chat_agent  # ì „ì—­ TravelChatAgent ì¸ìŠ¤í„´ìŠ¤ë¥¼ ê°€ì ¸ì˜´
import json

class ChatRequest(BaseModel):
    question: str
    context: Optional[str] = None

    class Config:
        schema_extra = {
            "example": {
                "question": "ì—¬í–‰ì— í•„ìš”í•œ ë‚´ìš©ì„ ì…ë ¥í•´ì£¼ì„¸ìš”.",
                "context": "ì´ì „ ì—¬í–‰ ê³„íš ë‚´ìš©..."
            }
        }

class ChatResponse(BaseModel):
    answer: str

chat_agent_router = APIRouter(
    tags=["ì±„íŒ…ì—ì´ì „íŠ¸"]
)

@chat_agent_router.post("/chatagent", response_model=ChatResponse)
async def chat_with_agent(request: ChatRequest):
    """ì—¬í–‰ ê´€ë ¨ ì§ˆë¬¸ì— ë‹µë³€í•©ë‹ˆë‹¤."""
    try:
        # TravelChatAgentë¥¼ í†µí•´ ë‹µë³€ ìƒì„±
        answer = await travel_chat_agent.get_answer(
            question=request.question,
            context=json.dumps(travel_chat_agent.current_travel_plan) if travel_chat_agent.current_travel_plan else None
        )

        return ChatResponse(answer=answer)

    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))


### C:\repository\HAI_Python\app\routers\festival_router.py ###

from fastapi import APIRouter, Query, HTTPException
from fastapi.responses import JSONResponse
import json
from datetime import datetime
import pandas as pd

# ì¶•ì œ ë°ì´í„° íŒŒì¼ ë¡œë“œ
file_path = 'app/travel/data/festival/festival.csv'
try:
    festival_data = pd.read_csv(file_path, encoding='cp949')
except FileNotFoundError:
    raise HTTPException(status_code=500, detail="ì¶•ì œ ë°ì´í„° íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
except Exception as e:
    raise HTTPException(status_code=500, detail=f"ë°ì´í„°ë¥¼ ë¡œë“œí•˜ëŠ” ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {e}")

# FastAPI Router ìƒì„±
festival_router = APIRouter(
        tags=["ì¶•ì œ"]
)

# ì¶•ì œ ë°ì´í„°ì—ì„œ start_dateë¥¼ datetimeìœ¼ë¡œ ë³€í™˜í•  ë•Œ ìˆ˜ì •
@festival_router.get("/")
async def get_festivals(
    destination: str = Query(..., description="ëª©ì ì§€(ì œê³µê¸°ê´€ëª…)"),
    start_date: str = Query(..., description="ì¡°íšŒ ì‹œì‘ ë‚ ì§œ (YYYY-MM-DD í˜•ì‹)"),
):
    """
    íŠ¹ì • ëª©ì ì§€ì™€ ë‚ ì§œ ì´í›„ì— ì—´ë¦¬ëŠ” ì¶•ì œ ëª©ë¡ì„ ë°˜í™˜í•©ë‹ˆë‹¤.
    """
    # ë‚ ì§œë¥¼ ë¬¸ìì—´ì¼ ê²½ìš°ì—ë§Œ datetime í˜•ì‹ìœ¼ë¡œ ë³€í™˜
    if isinstance(start_date, str):
        try:
            start_date = datetime.strptime(start_date, "%Y-%m-%d")  # str -> datetime ë³€í™˜
        except ValueError:
            raise HTTPException(status_code=400, detail="ì˜ëª»ëœ ë‚ ì§œ í˜•ì‹ì…ë‹ˆë‹¤. YYYY-MM-DD í˜•ì‹ì„ ì‚¬ìš©í•˜ì„¸ìš”.")

    # ëª©ì ì§€ì™€ ë‚ ì§œë¡œ í•„í„°ë§
    try:
        filtered_data = festival_data[
            (festival_data['ì œê³µê¸°ê´€ëª…'].str.startswith(destination)) &  # ëª©ì ì§€ ì¡°ê±´
            (pd.to_datetime(festival_data['ì¶•ì œì‹œì‘ì¼ì'], errors='coerce') >= start_date)  # ë‚ ì§œ ì¡°ê±´
        ]
    except KeyError as e:
        raise HTTPException(status_code=500, detail=f"í•„ìˆ˜ ì»¬ëŸ¼ì´ ëˆ„ë½ë˜ì—ˆìŠµë‹ˆë‹¤: {e}")

    # í•„í„°ë§ ê²°ê³¼ ë°˜í™˜
    result = filtered_data[['ì¶•ì œëª…', 'ê°œìµœì¥ì†Œ', 'ì¶•ì œë‚´ìš©', 'ì „í™”ë²ˆí˜¸',
                            'í™ˆí˜ì´ì§€ì£¼ì†Œ', 'ì†Œì¬ì§€ë„ë¡œëª…ì£¼ì†Œ', 'ì¶•ì œì‹œì‘ì¼ì', 'ì¶•ì œì¢…ë£Œì¼ì']]

    # JSONResponseë¥¼ ì‚¬ìš©í•˜ì—¬ ensure_ascii=False ì„¤ì •
    return JSONResponse(
        content=json.loads(result.to_json(orient="records", force_ascii=False)),
        media_type="application/json"
    )


### C:\repository\HAI_Python\app\routers\plan_router.py ###
from fastapi import APIRouter, HTTPException
from pydantic import BaseModel, Field
from typing import Optional
from enum import Enum
from app.plan_agent import plan_travel, calculate_trip_days  # ì™¸ë¶€ í•¨ìˆ˜ ê°€ì ¸ì˜¤ê¸°
from app.agents import travel_chat_agent
import httpx  # API í˜¸ì¶œì„ ìœ„í•œ httpx ì‚¬ìš©
from datetime import datetime
import logging
import json

logging.basicConfig(level=logging.DEBUG)

# APIRouter ì¸ìŠ¤í„´ìŠ¤ ìƒì„±
plan_router = APIRouter(
    tags=["ì—¬í–‰ ê³„íš"],
    responses={404: {"description": "Not found"}},
)

# Enum ë° ëª¨ë¸ ì •ì˜
class Gender(str, Enum):
    ì—¬ì„± = "ì—¬ì„±"
    ë‚¨ì„± = "ë‚¨ì„±"
    ê¸°íƒ€ = "ê¸°íƒ€"

AgeGroup = Enum(
    "AgeGroup",
    {
        "10ëŒ€": "10ëŒ€",
        "20ëŒ€": "20ëŒ€",
        "30ëŒ€": "30ëŒ€",
        "40ëŒ€": "40ëŒ€",
        "50ëŒ€": "50ëŒ€",
        "60ëŒ€ì´ìƒ": "60ëŒ€ì´ìƒ",
    },
)

class Companion(str, Enum):
    í˜¼ì = "í˜¼ì"
    ì—°ì¸ = "ì—°ì¸"
    ì¹œêµ¬ = "ì¹œêµ¬"
    ë¶€ëª¨ë‹˜ = "ë¶€ëª¨ë‹˜"
    ì•„ì´ = "ì•„ì´"
    ê¸°íƒ€ = "ê¸°íƒ€"

class TravelStyle(str, Enum):
    êµ­ê°€ìœ ì‚° = "êµ­ê°€ìœ ì‚°"
    íœ´ì–‘ = "íœ´ì–‘"
    ì•¡í‹°ë¹„í‹° = "ì•¡í‹°ë¹„í‹°"
    ì‹ë„ë½ = "ì‹ë„ë½"
    ì‡¼í•‘ = "ì‡¼í•‘"
    SNSê°ì„± = "SNSê°ì„±"

class TravelRequest(BaseModel):
    gender: Gender = Field(..., description="ì—¬í–‰ìì˜ ì„±ë³„")
    age: AgeGroup = Field(..., description="ì—¬í–‰ìì˜ ì—°ë ¹ëŒ€")
    companion: Companion = Field(..., description="ë™í–‰ì¸ ìœ í˜•")
    destination: str = Field(..., description="ì—¬í–‰ ëª©ì ì§€", min_length=2)
    style: TravelStyle = Field(..., description="ì„ í˜¸í•˜ëŠ” ì—¬í–‰ ìŠ¤íƒ€ì¼")
    startDate: datetime = Field(..., description="ì—¬í–‰ ì‹œì‘ ë‚ ì§œ (YYYY-MM-DD í˜•ì‹)")
    endDate: datetime = Field(..., description="ì—¬í–‰ ì¢…ë£Œ ë‚ ì§œ (YYYY-MM-DD í˜•ì‹)")

    class Config:
        schema_extra = {
            "example": {
                "gender": "ì—¬ì„±",
                "age": "20ëŒ€",
                "companion": "ì¹œêµ¬",
                "destination": "ì„œìš¸",
                "style": "SNSê°ì„±",
                "startDate": "2024-11-20",
                "endDate": "2024-11-22",
            }
        }

class TravelResponse(BaseModel):
    status: str
    travel_plan: dict
    duration: str
    festivals: list[dict] = []

# ì—¬í–‰ ê³„íš ìƒì„± ì—”ë“œí¬ì¸íŠ¸
@plan_router.post("/plan", response_model=TravelResponse)
async def generate_travel_plan(request: TravelRequest):
    try:
        # ì—¬í–‰ ì¼ìˆ˜ ê³„ì‚°
        nights, days = calculate_trip_days(request.startDate, request.endDate)

        user_info = {
            "gender": request.gender,
            "age": request.age.value,
            "companion": request.companion,
            "destination": request.destination,
            "style": request.style,
            "start_date": request.startDate,
            "end_date": request.endDate,
            "duration": f"{nights}ë°• {days}ì¼",
        }

        # ì—¬í–‰ ê³„íš ìƒì„±
        travel_plan = plan_travel(user_info)

        # travel_planì´ JSON ë¬¸ìì—´ì¸ ê²½ìš° íŒŒì‹±í•˜ì—¬ dictë¡œ ë³€í™˜
        if isinstance(travel_plan, str):
            try:
                travel_plan = json.loads(travel_plan)
            except json.JSONDecodeError as e:
                raise HTTPException(status_code=500, detail="ì—¬í–‰ ê³„íš ë°ì´í„°ë¥¼ íŒŒì‹±í•˜ëŠ” ë° ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤.")

        # travel_planì´ dict í˜•íƒœì¸ì§€ ê²€ì¦
        if not isinstance(travel_plan, dict):
            raise HTTPException(status_code=500, detail="ì—¬í–‰ ê³„íš ë°ì´í„°ê°€ ì˜¬ë°”ë¥¸ í˜•ì‹ì´ ì•„ë‹™ë‹ˆë‹¤.")

        # "result" í‚¤ê°€ í¬í•¨ëœ ê²½ìš° í•´ë‹¹ ë°ì´í„°ë¥¼ ë¶„ë¦¬
        if "result" in travel_plan:
            travel_plan = travel_plan["result"]

        # ê° Dayì˜ ê°’ì´ ë¦¬ìŠ¤íŠ¸(ë°°ì—´)ì¸ì§€ í™•ì¸í•˜ê³ , ì•„ë‹ˆë©´ ë¹ˆ ë¦¬ìŠ¤íŠ¸ë¡œ ì„¤ì •
        for day, events in travel_plan.items():
            if not isinstance(events, list):
                logging.warning(f"Day '{day}'ì˜ ë°ì´í„°ê°€ ë°°ì—´ì´ ì•„ë‹™ë‹ˆë‹¤. ë¹ˆ ë°°ì—´ë¡œ ì„¤ì •í•©ë‹ˆë‹¤.")
                travel_plan[day] = []

        # ì—¬í–‰ ì‹œì‘ì¼ê³¼ ì¢…ë£Œì¼ì„ ë¬¸ìì—´ë¡œ ë³€í™˜
        start_date_str = request.startDate.strftime("%Y-%m-%d")
        end_date_str = request.endDate.strftime("%Y-%m-%d")

        # TravelChatAgentì— ìµœì‹  ì—¬í–‰ ê³„íš ì„¤ì •
        travel_chat_agent.set_user_info(user_info)
        
        # ì¶•ì œ ë°ì´í„° ê°€ì ¸ì˜¤ê¸°
        async with httpx.AsyncClient() as client:
            festival_response = await client.get(
                "http://localhost:8000/api/festival/",
                params={"destination": request.destination, "start_date": start_date_str},
            )
            if festival_response.status_code != 200:
                raise HTTPException(
                    status_code=festival_response.status_code,
                    detail="ì¶•ì œ ë°ì´í„°ë¥¼ ê°€ì ¸ì˜¤ëŠ” ë° ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤.",
                )
            festival_data = festival_response.json()

        # ì—¬í–‰ ê³„íš + ì¶•ì œ ë°ì´í„° ë°˜í™˜
        return TravelResponse(
            status="success",
            travel_plan=travel_plan,
            duration=user_info["duration"],
            festivals=festival_data,
        )

    except ValueError as ve:
        logging.error(f"Unexpected error: {str(ve)}")
        raise HTTPException(status_code=400, detail=f"ì˜ëª»ëœ ìš”ì²­ì…ë‹ˆë‹¤: {str(ve)}")
    except TypeError as te:
        logging.error(f"Unexpected error: {str(te)}")
        raise HTTPException(status_code=400, detail=f"íƒ€ì… ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(te)}")
    except Exception as e:
        logging.error(f"Unexpected error: {str(e)}")
        raise HTTPException(status_code=500, detail=f"ì„œë²„ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}")


### C:\repository\HAI_Python\app\routers\__init__.py ###


### C:\repository\HAI_Python\app\travel\crewai\threeagent.py ###


import os
from datetime import datetime, timedelta
from fastapi import FastAPI, HTTPException
from pydantic import BaseModel
from typing import List, Optional
import uvicorn

from crewai import Agent, Task, Crew
from crewai_tools import (
    DirectoryReadTool,
    FileReadTool,
    SerperDevTool,
    WebsiteSearchTool
)

app = FastAPI()

class TravelRequest(BaseModel):
    gender: str
    age: str
    companion: str
    destination: str
    style: str
    start_date: str
    end_date: str

@app.post("/create-travel-plan")
async def create_travel_plan(travel_request: TravelRequest):
    try:
        # API í‚¤ ì„¤ì •
        serper_api_key = os.getenv("SERPER_API_KEY")
        openai_api_key = os.getenv("OPENAI_API_KEY")


        # Tools ì´ˆê¸°í™”
        search_tool = SerperDevTool()
        web_rag_tool = WebsiteSearchTool()

        # ë‚ ì§œ ì²˜ë¦¬
        start_date = travel_request.start_date
        end_date = travel_request.end_date
        
        # datetime ê°ì²´ ìƒì„±
        current_year = datetime.now().year
        start_month, start_day = map(int, start_date.split('/'))
        end_month, end_day = map(int, end_date.split('/'))
        
        start = datetime(current_year, start_month, start_day)
        end = datetime(current_year, end_month, end_day)
        
        if end < start:
            end = datetime(current_year + 1, end_month, end_day)
        
        nights = (end - start).days
        days = nights + 1
        duration = f"{nights}ë°• {days}ì¼"

        # user_info ë”•ì…”ë„ˆë¦¬ ìƒì„±
        user_info = {
            "gender": travel_request.gender,
            "age": travel_request.age,
            "companion": travel_request.companion,
            "destination": travel_request.destination,
            "style": travel_request.style,
            "start_date": start.strftime("%m/%d"),
            "end_date": end.strftime("%m/%d"),
            "duration": duration
        }

        # Create agents
        general_researcher = Agent(
            role='ì¼ë°˜ ì—¬í–‰ ì¡°ì‚¬ ì—ì´ì „íŠ¸',
            goal='ì„ íƒí•œ {destination}ì˜ ì „ë°˜ì ì¸ ì •ë³´ì™€ ì£¼ì†Œ ì œê³µ',
            backstory='ì—¬í–‰ì§€ì˜ ê¸°ë³¸ ì •ë³´ì™€ ì£¼ìš” ê´€ê´‘ì§€ë¥¼ ì¡°ì‚¬í•˜ëŠ” ì „ë¬¸ê°€ì…ë‹ˆë‹¤.',
            tools=[search_tool, web_rag_tool],
            verbose=True
        )

        personal_researcher = Agent(
            role='ë§ì¶¤í˜• ì—¬í–‰ ì¡°ì‚¬ ì—ì´ì „íŠ¸',
            goal='{gender}, {age}ì˜ {companion}ê³¼ í•¨ê»˜í•˜ëŠ” {style} ìŠ¤íƒ€ì¼ì˜ ì—¬í–‰ ì •ë³´ ì œê³µ',
            backstory="""ì‚¬ìš©ìì˜ íŠ¹ì„±ê³¼ ì„ í˜¸ë„ë¥¼ ê³ ë ¤í•˜ì—¬ ë§ì¶¤í˜• ì—¬í–‰ ì •ë³´ë¥¼ ì¡°ì‚¬í•˜ëŠ” ì „ë¬¸ê°€ì…ë‹ˆë‹¤.
            íŠ¹íˆ ê³„ì ˆì— ë§ì§€ ì•ŠëŠ” ë¶€ì ì ˆí•œ í™œë™ì€ ì œì™¸í•˜ê³  ì¶”ì²œí•©ë‹ˆë‹¤.
            ì˜ˆë¥¼ ë“¤ì–´:
            - ì—¬ë¦„: ìŠ¤í‚¤ì¥, ëˆˆì°ë§¤ì¥ ì œì™¸
            - ê²¨ìš¸: ì›Œí„°íŒŒí¬, í•´ìˆ˜ìš•ì¥ ì œì™¸
            - ë´„/ê°€ì„: ê³„ì ˆì— ë§ëŠ” ì¶•ì œì™€ ì•¼ì™¸í™œë™ ìœ„ì£¼ë¡œ ì¶”ì²œ""",
            tools=[search_tool, web_rag_tool],
            verbose=True
        )

        itinerary_writer = Agent(
            role='ì—¬í–‰ ì¼ì • ì‘ì„±ì',
            goal='ì…ë ¥ëœ ë‚ ì§œ({start_date}~{end_date})ì— ë§ì¶˜ {duration} ì¼ì • ì‘ì„±',
            backstory="""ìˆ˜ì§‘ëœ ì •ë³´ë¥¼ ë°”íƒ•ìœ¼ë¡œ ìµœì í™”ëœ ì—¬í–‰ ì¼ì •ì„ ì‘ì„±í•˜ëŠ” ì „ë¬¸ê°€ì…ë‹ˆë‹¤.
            ëª¨ë“  ì´ë™ ì‹œê°„ì€ ëŒ€ì¤‘êµí†µ/íƒì‹œ ê¸°ì¤€ 1ì‹œê°„ ì´ë‚´ë¡œ ì œí•œí•˜ì—¬ íš¨ìœ¨ì ì¸ ë™ì„ ì„ ê³„íší•©ë‹ˆë‹¤.""",
            verbose=True
        )

        # Define tasks
        general_research_task = Task(
            description="""
            {destination}ì— ëŒ€í•´ ë‹¤ìŒ ì •ë³´ë¥¼ ì¡°ì‚¬í•˜ì„¸ìš”:
            1. ì£¼ìš” ê´€ê´‘ì§€ 5ê³³ê³¼ ì£¼ì†Œ
            2. ìš´ì˜ ì‹œê°„
            3. ì…ì¥ë£Œ
            4. êµí†µ ì •ë³´
            5. í¸ì˜ì‹œì„¤
            ëª¨ë“  ì •ë³´ëŠ” ë°˜ë“œì‹œ í•œêµ­ì–´ë¡œ ì‘ì„±í•˜ì„¸ìš”.
            """,
            expected_output="í•œêµ­ì–´ë¡œ ì‘ì„±ëœ ê´€ê´‘ì§€ì˜ ê¸°ë³¸ ì •ë³´ì™€ ì£¼ì†Œê°€ í¬í•¨ëœ ìƒì„¸ ë³´ê³ ì„œ",
            agent=general_researcher
        )

        personal_research_task = Task(
            description="""
            ë‹¤ìŒ ì‚¬ìš©ì íŠ¹ì„±ì— ë§ëŠ” ì¶”ì²œ ì •ë³´ë¥¼ ì¡°ì‚¬í•˜ì„¸ìš”:
            - ì„±ë³„: {gender}
            - ì—°ë ¹: {age}
            - ë™í–‰: {companion}
            - ì—¬í–‰ìŠ¤íƒ€ì¼: {style}
            - ì—¬í–‰ ì‹œì‘ì¼: {start_date}
            
            1. ë§ì¶¤í˜• ê´€ê´‘ì§€ ì¶”ì²œ (ì„œë¡œ 30ë¶„ ì´ë‚´ ê±°ë¦¬)
            2. ì‹ë‹¹ ì¶”ì²œ (í˜„ì¬ ê³„ì ˆ ë©”ë‰´ ê³ ë ¤)
            3. ì‡¼í•‘ ì¥ì†Œ
            4. ê³„ì ˆì— ë§ëŠ” íŠ¹ë³„ í™œë™ì´ë‚˜ ì¶•ì œ
            
            ëª¨ë“  ì •ë³´ëŠ” ë°˜ë“œì‹œ í•œêµ­ì–´ë¡œ ì‘ì„±í•˜ì„¸ìš”.
            """,
            expected_output="í•œêµ­ì–´ë¡œ ì‘ì„±ëœ ê³„ì ˆì„ ê³ ë ¤í•œ ë§ì¶¤í˜• ì¶”ì²œ ì •ë³´ ë³´ê³ ì„œ",
            agent=personal_researcher
        )

        write_task = Task(
            description="""
            ì•ì„  ë‘ ì—ì´ì „íŠ¸ê°€ ì¶”ì²œí•œ ì¥ì†Œë“¤ì„ ë°”íƒ•ìœ¼ë¡œ {duration} ì¼ì •ì„ ê³„íší•´ì£¼ì„¸ìš”.
            
            ìš°ì„  ì¶”ì²œë°›ì€ ëª¨ë“  ì¥ì†Œë“¤ì˜ ì •í™•í•œ ì •ë³´ë¥¼ ì¡°ì‚¬í•˜ì„¸ìš”:
            1. ì •í™•í•œ ë„ë¡œëª… ì£¼ì†Œì™€ ì§€ë²ˆ ì£¼ì†Œ (ë„¤ì´ë²„/ì¹´ì¹´ì˜¤ë§µ ê²€ìƒ‰ ê¸°ì¤€)
            2. ì˜ì—…ì‹œê°„
            3. ì „í™”ë²ˆí˜¸
            4. ê°€ì¥ ê°€ê¹Œìš´ ëŒ€ì¤‘êµí†µ ì •ë³´
            
            ê·¸ë¦¬ê³  ë‹¤ìŒ ë‚´ìš©ìœ¼ë¡œ ì—¬í–‰ ê³„íšì„ ì‘ì„±í•˜ì„¸ìš”:
            - ëŒ€ìƒ: {gender}, {age}, {companion}ê³¼ ë™í–‰
            - ì—¬í–‰ìŠ¤íƒ€ì¼: {style}
            - ê¸°ê°„: {start_date}ë¶€í„° {end_date}ê¹Œì§€ {duration}
            
            ì¼ì • ì‘ì„± ì‹œ ì£¼ì˜ì‚¬í•­:
            1. ëª¨ë“  ì¥ì†ŒëŠ” ê²€ì¦ëœ ì •í™•í•œ ì£¼ì†Œ ì‚¬ìš©
            2. ì´ë™ì‹œê°„ì€ ì‹¤ì œ ëŒ€ì¤‘êµí†µ ê¸°ì¤€ìœ¼ë¡œ ê³„ì‚° (1ì‹œê°„ ì´ë‚´ë¡œ ì œí•œ)
            3. ê° ì¥ì†Œ ê°„ ì´ë™ê²½ë¡œ ìƒì„¸íˆ ê¸°ë¡
            4. ì‹ì‚¬ ì¥ì†ŒëŠ” ì¸ê·¼ ë§›ì§‘ìœ¼ë¡œ ì„ ì •
            5. ë™ì„ ì´ íš¨ìœ¨ì ì´ë„ë¡ ì¸ì ‘ ì¥ì†Œë¼ë¦¬ ë¬¶ì–´ì„œ ê³„íš
            
            ìµœì¢… ì¼ì •ì—ëŠ” ë‹¤ìŒì´ í¬í•¨ë˜ì–´ì•¼ í•©ë‹ˆë‹¤:
            1. ë‚ ì§œ/ì‹œê°„ë³„ ì„¸ë¶€ ì¼ì •
            2. ê° ì¥ì†Œì˜ ì •í™•í•œ ì£¼ì†Œ
            3. ìƒì„¸ ì´ë™ ë°©ë²•ê³¼ ì†Œìš”ì‹œê°„
            4. ì‹ì‚¬ ì •ë³´ì™€ ì˜ˆìƒ ë¹„ìš©
            5. ì „ì²´ ì˜ˆìƒ ë¹„ìš©

            ëª¨ë“  ë‚´ìš©ì€ í•œêµ­ì–´ë¡œ ì‘ì„±í•˜ë©°, ê¸ˆì•¡ì€ ì›í™”ë¡œ í‘œì‹œí•´ì£¼ì„¸ìš”.
            """,
            expected_output="ê²€ì¦ëœ ì£¼ì†Œì™€ ìƒì„¸ ë™ì„ ì´ í¬í•¨ëœ ì—¬í–‰ ì¼ì •",
            agent=itinerary_writer,
            output_file='itinerary/personalized_itinerary.md'
        )

        # Assemble and execute crew
        crew = Crew(
            agents=[general_researcher, personal_researcher, itinerary_writer],
            tasks=[general_research_task, personal_research_task, write_task],
            verbose=True,
            planning=True,
        )

        result = crew.kickoff(inputs=user_info)
        return {"result": result}

    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))

if __name__ == "__main__":
    uvicorn.run(app, host="0.0.0.0", port=8000)

